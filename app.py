# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
import ee
import geemap.foliumap as geemap
import folium
import json
import datetime
import plotly.express as px
import os
from io import BytesIO
import requests # Needed for getThumbUrl download
import traceback
from streamlit_folium import st_folium
import base64

# --- Custom CSS ---
st.set_page_config(
    page_title="Ø³Ø§Ù…Ø§Ù†Ù‡ Ù¾Ø§ÛŒØ´ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù†ÛŒØ´Ú©Ø±",
    page_icon="ğŸŒ¾",
    layout="wide"
)

# Custom CSS for Persian text alignment and professional styling
st.markdown("""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Vazirmatn:wght@400;700&display=swap');

        /* Main container */
        .main {
            font-family: 'Vazirmatn', sans-serif;
        }

        /* Headers */
        h1, h2, h3 {
            font-family: 'Vazirmatn', sans-serif;
            color: #2c3e50;
            text-align: right;
        }

        /* Standard Metrics */
        .stMetric {
            font-family: 'Vazirmatn', sans-serif;
            background-color: #f8f9fa;
            border-radius: 10px;
            padding: 1rem;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            text-align: right; /* Ensure text aligns right */
            border-left: 5px solid #17a2b8; /* Default border color */
            transition: transform 0.2s ease-in-out, box-shadow 0.2s ease-in-out;
        }
        .stMetric:hover {
            transform: scale(1.03);
            box-shadow: 0 4px 8px rgba(0,0,0,0.15);
        }
        .stMetric > label { /* Metric Label */
            font-weight: 700;
            color: #495057;
        }
        .stMetric > div[data-testid="stMetricValue"] { /* Metric Value */
            font-size: 1.5em;
            font-weight: bold;
            color: #2c3e50;
            direction: ltr; /* Ensure numbers display correctly */
            text-align: right;
        }
        .stMetric > div[data-testid="stMetricDelta"] { /* Metric Delta (optional) */
             font-size: 1em;
             font-weight: normal;
        }


        /* Custom colored metric cards */
        .metric-card-1 { border-left-color: #007bff; } /* Blue */
        .metric-card-2 { border-left-color: #ffc107; } /* Yellow */
        .metric-card-3 { border-left-color: #28a745; } /* Green */
        .metric-card-4 { border-left-color: #dc3545; } /* Red */
        .metric-card-5 { border-left-color: #6f42c1; } /* Purple */


        /* Tabs */
        .stTabs [data-baseweb="tab-list"] {
            gap: 2px;
            direction: rtl;
        }

        .stTabs [data-baseweb="tab"] {
            height: 50px;
            padding: 10px 20px;
            background-color: #f8f9fa;
            border-radius: 5px 5px 0 0;
            font-family: 'Vazirmatn', sans-serif;
            font-weight: 600;
        }

        /* Tables */
        .dataframe {
            font-family: 'Vazirmatn', sans-serif;
            text-align: right;
        }
        th { /* Table header alignment */
            text-align: right !important;
        }
        td { /* Ensure table data cells are also right-aligned */
             text-align: right !important;
             direction: rtl; /* Right-to-left text direction for content */
        }
        td:last-child { /* Align last column (often numbers or status) to center or left if needed */
           /* text-align: center !important; */
           /* direction: ltr; */ /* Keep default for numbers */
        }


        /* Sidebar */
        .css-1d391kg { /* Adjust this selector based on Streamlit version if sidebar styling fails */
            font-family: 'Vazirmatn', sans-serif;
            direction: rtl;
        }
        /* Ensure sidebar content like selectbox options are right-aligned */
         .stSelectbox label, .stTextInput label, .stDateInput label {
            text-align: right !important;
            width: 100%;
         }
         .stSelectbox [data-baseweb="select"] > div {
            text-align: right !important;
         }


        /* Custom status badges */
        .status-badge {
            padding: 4px 8px;
            border-radius: 15px;
            font-size: 0.8em;
            font-weight: bold;
            display: inline-block; /* Make badges inline */
        }
        .status-positive {
            background-color: #d4edda;
            color: #155724;
        }
        .status-neutral {
            background-color: #fff3cd;
            color: #856404;
        }
        .status-negative {
            background-color: #f8d7da;
            color: #721c24;
        }

        /* Folium Legend Styling (if possible within Streamlit) */
        .legend {
             font-family: 'Vazirmatn', sans-serif;
             font-size: 12px;
             direction: rtl;
        }
        .legend-title {
            font-weight: bold;
            margin-bottom: 5px;
            text-align: right;
        }
        .legend-items div {
             text-align: right;
        }
        .legend-items span {
             margin-left: 5px; /* Space between color box and text */
        }


    </style>
""", unsafe_allow_html=True)

# --- Configuration ---
APP_TITLE = "Ø³Ø§Ù…Ø§Ù†Ù‡ Ù¾Ø§ÛŒØ´ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù†ÛŒØ´Ú©Ø±"
APP_SUBTITLE = "Ù…Ø·Ø§Ù„Ø¹Ø§Øª Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ Ø´Ø±Ú©Øª Ú©Ø´Øª Ùˆ ØµÙ†Ø¹Øª Ø¯Ù‡Ø®Ø¯Ø§"
INITIAL_LAT = 31.534442
INITIAL_LON = 48.724416
INITIAL_ZOOM = 11 # Adjusted zoom level

# --- File Paths ---
# ***** UPDATED CSV FILE PATH *****
CSV_FILE_PATH = 'Ø¨Ø±Ù†Ø§Ù…Ù‡_Ø±ÛŒØ²ÛŒ_Ø¨Ø§_Ù…Ø®ØªØµØ§Øª (1).csv'
SERVICE_ACCOUNT_FILE = 'ee-esmaeilkiani13877-cfdea6eaf411 (4).json'

# --- Display Titles ---
st.title(APP_TITLE)
st.caption(APP_SUBTITLE)


# --- Summary Statistics Cards ---
st.markdown("### Ø®Ù„Ø§ØµÙ‡ ÙˆØ¶Ø¹ÛŒØª Ú©Ù„ÛŒ") # Header for the cards

# Data from the image provided
summary_data = {
    "Ø¯Ø§Ø´Øª Û±Û´Û°Û³": "9008.35", # Using dot for decimal
    "Ø¢ÛŒØ´ Û±Û´Û°Û³-Û°Û´": "1703.04",
    "Ø±Ø§ØªÙˆÙ† Û±Û´Û°Û´": "7305.31",
    "Ù¾Ù„Ù†Øª Û±Û´Û°Û´": "2115.99",
    "Ø¯Ø§Ø´Øª Ú©Ù„ÛŒ Û±Û´Û°Û´": "9421.3"
}

# Create columns for the cards
cols = st.columns(len(summary_data))

# Assign data to cards with custom CSS classes
card_classes = ["metric-card-1", "metric-card-2", "metric-card-3", "metric-card-4", "metric-card-5"]

for i, (label, value) in enumerate(summary_data.items()):
    with cols[i]:
        # Inject custom CSS class using markdown hack
        st.markdown(f'<div class="stMetric {card_classes[i]}">', unsafe_allow_html=True)
        st.metric(label=label, value=value)
        st.markdown('</div>', unsafe_allow_html=True)

st.markdown("---") # Separator

# --- GEE Authentication ---
@st.cache_resource # Cache the GEE initialization
def initialize_gee():
    """Initializes Google Earth Engine using the Service Account."""
    try:
        if not os.path.exists(SERVICE_ACCOUNT_FILE):
            st.error(f"Ø®Ø·Ø§: ÙØ§ÛŒÙ„ Service Account Ø¯Ø± Ù…Ø³ÛŒØ± '{SERVICE_ACCOUNT_FILE}' ÛŒØ§ÙØª Ù†Ø´Ø¯.")
            st.stop()
        credentials = ee.ServiceAccountCredentials(None, key_file=SERVICE_ACCOUNT_FILE)
        ee.Initialize(credentials=credentials, opt_url='https://earthengine-highvolume.googleapis.com')
        print("GEE Initialized Successfully using Service Account.")
        return True
    except ee.EEException as e:
        st.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ Google Earth Engine: {e}")
        st.error("Ù„Ø·ÙØ§Ù‹ Ø§Ø² ØµØ­Øª ÙØ§ÛŒÙ„ Service Account Ùˆ ÙØ¹Ø§Ù„ Ø¨ÙˆØ¯Ù† Ø¢Ù† Ø¯Ø± Ù¾Ø±ÙˆÚ˜Ù‡ GEE Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø­Ø§ØµÙ„ Ú©Ù†ÛŒØ¯.")
        st.stop()
    except Exception as e:
        st.error(f"Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡ Ù‡Ù†Ú¯Ø§Ù… Ø§ØªØµØ§Ù„ Ø¨Ù‡ GEE: {e}")
        st.stop()


# --- Load Farm Data ---
@st.cache_data(show_spinner="Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø²Ø§Ø±Ø¹...")
def load_farm_data(csv_path=CSV_FILE_PATH):
    """Loads farm data from the specified CSV file using new column names."""
    try:
        df = pd.read_csv(csv_path)
        # ***** UPDATED REQUIRED COLUMNS *****
        required_cols = ['Ù…Ø²Ø±Ø¹Ù‡', 'Ø±ÙˆØ²', 'longitude', 'latitude'] # 'Ú¯Ø±ÙˆÙ‡' is optional based on usage
        if not all(col in df.columns for col in required_cols):
            missing_cols = [col for col in required_cols if col not in df.columns]
            st.error(f"âŒ ÙØ§ÛŒÙ„ CSV Ø¨Ø§ÛŒØ¯ Ø´Ø§Ù…Ù„ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¶Ø±ÙˆØ±ÛŒ Ø¨Ø§Ø´Ø¯: {', '.join(required_cols)}. Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ ÛŒØ§ÙØªâ€ŒÙ†Ø´Ø¯Ù‡: {', '.join(missing_cols)}")
            return None

        # ***** USE NEW COORDINATE COLUMN NAMES *****
        # Convert coordinate columns to numeric, coercing errors
        df['longitude'] = pd.to_numeric(df['longitude'], errors='coerce')
        df['latitude'] = pd.to_numeric(df['latitude'], errors='coerce')

        # Handle missing coordinates flag explicitly if needed (Assuming no specific 'missing' flag column in new file)
        # Drop rows where coordinates are actually missing after coercion
        initial_rows = len(df)
        df = df.dropna(subset=['longitude', 'latitude'])
        dropped_rows = initial_rows - len(df)
        if dropped_rows > 0:
            st.warning(f"âš ï¸ {dropped_rows} Ø±Ú©ÙˆØ±Ø¯ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ù†Ø¯Ø§Ø´ØªÙ† Ù…Ø®ØªØµØ§Øª Ù…Ø¹ØªØ¨Ø± Ø­Ø°Ù Ø´Ø¯Ù†Ø¯.")


        if df.empty:
            st.warning("âš ï¸ Ø¯Ø§Ø¯Ù‡ Ù…Ø¹ØªØ¨Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø²Ø§Ø±Ø¹ ÛŒØ§ÙØª Ù†Ø´Ø¯ (Ù¾Ø³ Ø§Ø² Ø­Ø°Ù Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ø¨Ø¯ÙˆÙ† Ù…Ø®ØªØµØ§Øª).")
            return None

        # ***** USE NEW DAY COLUMN NAME *****
        # Ensure 'Ø±ÙˆØ²' is string type for consistent filtering
        if 'Ø±ÙˆØ²' in df.columns:
            df['Ø±ÙˆØ²'] = df['Ø±ÙˆØ²'].astype(str).str.strip()
            # Clean potential whitespace variations in day names
            df['Ø±ÙˆØ²'] = df['Ø±ÙˆØ²'].str.replace(' ', '', regex=False) # Remove all spaces
            # Example: Make 'Ø³Ù‡ Ø´Ù†Ø¨Ù‡' consistent as 'Ø³Ù‡Ø´Ù†Ø¨Ù‡' or handle variations explicitly
            # You might need more specific cleaning based on actual data variations
            df['Ø±ÙˆØ²'] = df['Ø±ÙˆØ²'].replace({'Ø³Ù‡Ø´Ù†Ø¨Ù‡': 'Ø³Ù‡ Ø´Ù†Ø¨Ù‡', 'Ù¾Ù†Ø¬Ø´Ù†Ø¨Ù‡': 'Ù¾Ù†Ø¬ Ø´Ù†Ø¨Ù‡'}) # Example: Standardize specific names if needed AFTER removing spaces


        else:
            st.error("âŒ Ø³ØªÙˆÙ† 'Ø±ÙˆØ²' Ø¯Ø± ÙØ§ÛŒÙ„ CSV ÛŒØ§ÙØª Ù†Ø´Ø¯.")
            return None

        # Ensure 'Ù…Ø²Ø±Ø¹Ù‡' is suitable as key/identifier (e.g., string)
        df['Ù…Ø²Ø±Ø¹Ù‡'] = df['Ù…Ø²Ø±Ø¹Ù‡'].astype(str)

        st.success(f"âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ {len(df)} Ù…Ø²Ø±Ø¹Ù‡ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯.")
        return df
    except FileNotFoundError:
        st.error(f"âŒ ÙØ§ÛŒÙ„ '{csv_path}' ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ ÙØ§ÛŒÙ„ CSV Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø²Ø§Ø±Ø¹ Ø±Ø§ Ø¯Ø± Ù…Ø³ÛŒØ± ØµØ­ÛŒØ­ Ù‚Ø±Ø§Ø± Ø¯Ù‡ÛŒØ¯.")
        return None
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÛŒØ§ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ CSV: {e}")
        st.error(traceback.format_exc())
        return None

# Initialize GEE and Load Data
if initialize_gee():
    farm_data_df = load_farm_data()
else:
    st.error("âŒ Ø§Ù…Ú©Ø§Ù† Ø§Ø¯Ø§Ù…Ù‡ Ú©Ø§Ø± Ø¨Ø¯ÙˆÙ† Ø§ØªØµØ§Ù„ Ø¨Ù‡ Google Earth Engine ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")
    st.stop()

if farm_data_df is None:
    st.error("âŒ Ø§Ù…Ú©Ø§Ù† Ø§Ø¯Ø§Ù…Ù‡ Ú©Ø§Ø± Ø¨Ø¯ÙˆÙ† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø²Ø§Ø±Ø¹ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")
    st.stop()


# ==============================================================================
# Sidebar Filters
# ==============================================================================
st.sidebar.header("âš™ï¸ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù†Ù…Ø§ÛŒØ´")

# --- Day of the Week Selection ---
# ***** USE NEW DAY COLUMN NAME *****
try:
    available_days_raw = farm_data_df['Ø±ÙˆØ²'].unique()
    # Clean the list of available days for display (e.g., add space back if removed earlier for matching)
    # This depends on how you standardized them in load_farm_data
    available_days_display = sorted([day.replace('Ù¾Ù†Ø¬Ø´Ù†Ø¨Ù‡', 'Ù¾Ù†Ø¬ Ø´Ù†Ø¨Ù‡').replace('Ø³Ù‡Ø´Ù†Ø¨Ù‡', 'Ø³Ù‡ Ø´Ù†Ø¨Ù‡') for day in available_days_raw]) # Example for display

    # Map displayed day back to the potentially cleaned version for filtering if needed
    day_display_map = {display: raw for display, raw in zip(available_days_display, sorted(available_days_raw))}


    selected_day_display = st.sidebar.selectbox(
        "ğŸ“… Ø±ÙˆØ² Ù‡ÙØªÙ‡ Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯:",
        options=available_days_display,
        index=0, # Default to the first day
        help="Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø²Ø§Ø±Ø¹ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§ÛŒÙ† Ø±ÙˆØ² ÙÛŒÙ„ØªØ± Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯."
    )
    # Get the potentially 'cleaned' day name for filtering
    selected_day_filter = day_display_map[selected_day_display]

except KeyError:
    st.sidebar.error("Ø®Ø·Ø§: Ø³ØªÙˆÙ† 'Ø±ÙˆØ²' Ø¯Ø± Ø¯Ø§Ø¯Ù‡ Ù‡Ø§ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡ ÛŒØ§ÙØª Ù†Ø´Ø¯.")
    st.stop()
except Exception as e:
    st.sidebar.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø±ÙˆØ²Ù‡Ø§ÛŒ Ù‡ÙØªÙ‡: {e}")
    st.stop()


# --- Filter Data Based on Selected Day ---
# ***** USE cleaned 'selected_day_filter' *****
filtered_farms_df = farm_data_df[farm_data_df['Ø±ÙˆØ²'] == selected_day_filter].copy()

if filtered_farms_df.empty:
    st.warning(f"âš ï¸ Ù‡ÛŒÚ† Ù…Ø²Ø±Ø¹Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø±ÙˆØ² '{selected_day_display}' ÛŒØ§ÙØª Ù†Ø´Ø¯.")
    # Don't stop here, allow user to potentially change the day
    # st.stop() # Avoid stopping abruptly

# --- Farm Selection ---
# Check if filtered_farms_df is not empty before proceeding
if not filtered_farms_df.empty:
    try:
        available_farms = sorted(filtered_farms_df['Ù…Ø²Ø±Ø¹Ù‡'].unique())
        # Add an option for "All Farms"
        farm_options = ["Ù‡Ù…Ù‡ Ù…Ø²Ø§Ø±Ø¹"] + available_farms
        selected_farm_name = st.sidebar.selectbox(
            "ğŸŒ¾ Ù…Ø²Ø±Ø¹Ù‡ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯:",
            options=farm_options,
            index=0, # Default to "All Farms"
            help="Ù…Ø²Ø±Ø¹Ù‡â€ŒØ§ÛŒ Ú©Ù‡ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒØ¯ Ø¬Ø²Ø¦ÛŒØ§Øª Ø¢Ù† Ø±Ø§ Ø¨Ø¨ÛŒÙ†ÛŒØ¯ ÛŒØ§ 'Ù‡Ù…Ù‡ Ù…Ø²Ø§Ø±Ø¹' Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ú©Ù„ÛŒ."
        )
    except KeyError:
        st.sidebar.error("Ø®Ø·Ø§: Ø³ØªÙˆÙ† 'Ù…Ø²Ø±Ø¹Ù‡' Ø¯Ø± Ø¯Ø§Ø¯Ù‡ Ù‡Ø§ÛŒ ÙÛŒÙ„ØªØ± Ø´Ø¯Ù‡ ÛŒØ§ÙØª Ù†Ø´Ø¯.")
        st.stop()
    except Exception as e:
        st.sidebar.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù„ÛŒØ³Øª Ù…Ø²Ø§Ø±Ø¹: {e}")
        st.stop()
else:
    # Handle the case where no farms are available for the selected day
    selected_farm_name = "Ù‡Ù…Ù‡ Ù…Ø²Ø§Ø±Ø¹" # Default or provide a message
    st.sidebar.warning(f"Ù…Ø²Ø±Ø¹Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ø¯Ø± Ø±ÙˆØ² '{selected_day_display}' ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")


# --- Index Selection ---
index_options = {
    "NDVI": "Ø´Ø§Ø®Øµ Ù¾ÙˆØ´Ø´ Ú¯ÛŒØ§Ù‡ÛŒ ØªÙØ§Ø¶Ù„ÛŒ Ù†Ø±Ù…Ø§Ù„ Ø´Ø¯Ù‡",
    "EVI": "Ø´Ø§Ø®Øµ Ù¾ÙˆØ´Ø´ Ú¯ÛŒØ§Ù‡ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡",
    "NDMI": "Ø´Ø§Ø®Øµ Ø±Ø·ÙˆØ¨Øª ØªÙØ§Ø¶Ù„ÛŒ Ù†Ø±Ù…Ø§Ù„ Ø´Ø¯Ù‡",
    "LAI": "Ø´Ø§Ø®Øµ Ø³Ø·Ø­ Ø¨Ø±Ú¯ (ØªØ®Ù…ÛŒÙ†ÛŒ)",
    "MSI": "Ø´Ø§Ø®Øµ ØªÙ†Ø´ Ø±Ø·ÙˆØ¨ØªÛŒ",
    "CVI": "Ø´Ø§Ø®Øµ Ú©Ù„Ø±ÙˆÙÛŒÙ„ (ØªØ®Ù…ÛŒÙ†ÛŒ)",
    # Add more indices if needed and implemented
    # "Biomass": "Ø²ÛŒØ³Øªâ€ŒØªÙˆØ¯Ù‡ (ØªØ®Ù…ÛŒÙ†ÛŒ)",
    # "ET": "ØªØ¨Ø®ÛŒØ± Ùˆ ØªØ¹Ø±Ù‚ (ØªØ®Ù…ÛŒÙ†ÛŒ)",
}
selected_index = st.sidebar.selectbox(
    "ğŸ“ˆ Ø´Ø§Ø®Øµ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø±ÙˆÛŒ Ù†Ù‚Ø´Ù‡:",
    options=list(index_options.keys()),
    format_func=lambda x: f"{x} ({index_options[x]})",
    index=0 # Default to NDVI
)

# --- Date Range Calculation ---
today = datetime.date.today()
# Find the most recent date corresponding to the selected day of the week
# Map Persian day names to Python's weekday() (Monday=0, Sunday=6) - Use the display name
persian_to_weekday = {
    "Ø´Ù†Ø¨Ù‡": 5,
    "ÛŒÚ©Ø´Ù†Ø¨Ù‡": 6,
    "Ø¯ÙˆØ´Ù†Ø¨Ù‡": 0,
    "Ø³Ù‡ Ø´Ù†Ø¨Ù‡": 1, # Use the display format
    "Ú†Ù‡Ø§Ø±Ø´Ù†Ø¨Ù‡": 2,
    "Ù¾Ù†Ø¬ Ø´Ù†Ø¨Ù‡": 3, # Use the display format
    "Ø¬Ù…Ø¹Ù‡": 4,
}

# Check if selected_day_display is valid before proceeding
if selected_day_display in persian_to_weekday:
    try:
        target_weekday = persian_to_weekday[selected_day_display]
        days_ago = (today.weekday() - target_weekday + 7) % 7
        if days_ago == 0: # If today is the selected day, use today
             end_date_current = today
        else:
             end_date_current = today - datetime.timedelta(days=days_ago)

        start_date_current = end_date_current - datetime.timedelta(days=6)
        end_date_previous = start_date_current - datetime.timedelta(days=1)
        start_date_previous = end_date_previous - datetime.timedelta(days=6)

        # Convert to strings for GEE
        start_date_current_str = start_date_current.strftime('%Y-%m-%d')
        end_date_current_str = end_date_current.strftime('%Y-%m-%d')
        start_date_previous_str = start_date_previous.strftime('%Y-%m-%d')
        end_date_previous_str = end_date_previous.strftime('%Y-%m-%d')

        st.sidebar.info(f"ğŸ—“ï¸ Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ ÙØ¹Ù„ÛŒ: {start_date_current_str} ØªØ§ {end_date_current_str}")
        st.sidebar.info(f"ğŸ—“ï¸ Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ù‚Ø¨Ù„ÛŒ: {start_date_previous_str} ØªØ§ {end_date_previous_str}")

    except Exception as e:
        st.sidebar.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ: {e}")
        st.stop()
else:
    st.sidebar.error(f"Ù†Ø§Ù… Ø±ÙˆØ² Ù‡ÙØªÙ‡ '{selected_day_display}' Ø¯Ø± Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ persian_to_weekday ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø³ØªÙˆÙ† 'Ø±ÙˆØ²' Ø¯Ø± CSV Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.")
    # Provide default dates or stop
    start_date_current_str = (today - datetime.timedelta(days=6)).strftime('%Y-%m-%d')
    end_date_current_str = today.strftime('%Y-%m-%d')
    start_date_previous_str = (today - datetime.timedelta(days=13)).strftime('%Y-%m-%d')
    end_date_previous_str = (today - datetime.timedelta(days=7)).strftime('%Y-%m-%d')
    st.sidebar.warning("Ø§Ø² Ø¨Ø§Ø²Ù‡â€ŒÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ Ù¾ÛŒØ´â€ŒÙØ±Ø¶ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆØ¯.")


# ==============================================================================
# Google Earth Engine Functions
# ==============================================================================

# --- Cloud Masking Function for Sentinel-2 ---
def maskS2clouds(image):
    """Masks clouds in a Sentinel-2 SR image using the QA band."""
    try:
        qa = image.select('QA60')
        # Bits 10 and 11 are clouds and cirrus, respectively.
        cloudBitMask = 1 << 10
        cirrusBitMask = 1 << 11
        # Both flags should be set to zero, indicating clear conditions.
        mask = qa.bitwiseAnd(cloudBitMask).eq(0).And(
                 qa.bitwiseAnd(cirrusBitMask).eq(0))

        # Use SCL band for more robust cloud masking if available
        # Check if 'SCL' band exists before selecting
        if 'SCL' in image.bandNames().getInfo():
            scl = image.select('SCL')
            # Cloud Shadow (3), Cloud Medium Probability (8), Cloud High Probability (9), Cirrus (10)
            cloud_mask = scl.eq(3).Or(scl.eq(8)).Or(scl.eq(9)).Or(scl.eq(10))
            mask = mask.And(cloud_mask.Not()) # Combine QA mask and SCL mask

        # Scale and offset factors for Sentinel-2 SR bands (B1 to B12)
        opticalBands = image.select('B.*').multiply(0.0001)

        # Return the image with scaled bands and applied mask
        return image.addBands(opticalBands, None, True).updateMask(mask)
    except Exception as e:
        # st.warning(f"Error masking clouds for an image: {e}") # Optional: log error
        # Return the original image if masking fails for any reason
        # print(f"Warning: Cloud masking failed for image {image.id().getInfo() if image.id() else 'unknown'}. Returning original. Error: {e}")
        return image


# --- Index Calculation Functions ---
def add_indices(image):
    """Calculates and adds various indices as bands to an image."""
    try:
        # Ensure required bands exist before calculation
        required_bands = ['B2', 'B3', 'B4', 'B8', 'B11']
        if not all(band in image.bandNames().getInfo() for band in required_bands):
             # print(f"Warning: Image {image.id().getInfo() if image.id() else 'unknown'} missing required bands for index calculation. Skipping.")
             return image # Return original image if bands are missing

        # NDVI: (NIR - Red) / (NIR + Red) | Sentinel-2: (B8 - B4) / (B8 + B4)
        ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')

        # EVI: 2.5 * (NIR - Red) / (NIR + 6 * Red - 7.5 * Blue + 1) | S2: 2.5 * (B8 - B4) / (B8 + 6 * B4 - 7.5 * B2 + 1)
        evi = image.expression(
            '2.5 * (NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1)', {
                'NIR': image.select('B8'),
                'RED': image.select('B4'),
                'BLUE': image.select('B2')
            }).rename('EVI')

        # NDMI (Normalized Difference Moisture Index): (NIR - SWIR1) / (NIR + SWIR1) | S2: (B8 - B11) / (B8 + B11)
        ndmi = image.normalizedDifference(['B8', 'B11']).rename('NDMI')

        # MSI (Moisture Stress Index): SWIR1 / NIR | S2: B11 / B8
        msi = image.expression('B11 / B8', { # Direct band expression
             'B11': image.select('B11'),
             'B8': image.select('B8')
             }).rename('MSI')


        # LAI (Leaf Area Index) - Simple estimation using NDVI (Needs calibration for accuracy)
        # Using a very basic placeholder: LAI proportional to NDVI
        lai = ndvi.multiply(3.5).rename('LAI') # Placeholder - Needs proper calibration

        # CVI (Chlorophyll Vegetation Index) - (NIR / Green) * (Red / Green) | S2: (B8 / B3) * (B4 / B3)
        # Handle potential division by zero if Green band is 0
        green_safe = image.select('B3').max(ee.Image(0.0001)) # Avoid division by zero
        cvi = image.expression('(NIR / GREEN) * (RED / GREEN)', {
            'NIR': image.select('B8'),
            'GREEN': green_safe,
            'RED': image.select('B4')
        }).rename('CVI')

        return image.addBands([ndvi, evi, ndmi, msi, lai, cvi]) # Add calculated indices
    except Exception as e:
        # st.warning(f"Error calculating indices for an image: {e}") # Optional: log error
        # print(f"Warning: Index calculation failed for image {image.id().getInfo() if image.id() else 'unknown'}. Returning original. Error: {e}")
        # Return the original image if index calculation fails
        return image


# --- Function to get processed image for a date range and geometry ---
#@st.cache_data(show_spinner="Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØµØ§ÙˆÛŒØ± Ù…Ø§Ù‡ÙˆØ§Ø±Ù‡â€ŒØ§ÛŒ...", persist=True) # Caching can be complex with GEE objects, disable if causing issues
def get_processed_image(_geometry, start_date, end_date, index_name):
    """
    Gets cloud-masked, index-calculated Sentinel-2 median composite for a given geometry and date range.
    _geometry: ee.Geometry (Point or Polygon)
    start_date, end_date: YYYY-MM-DD strings
    index_name: Name of the primary index band to return (e.g., 'NDVI')
    """
    try:
        s2_sr_col = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')
                     .filterBounds(_geometry)
                     .filterDate(start_date, end_date)
                     .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 80)) # Pre-filter very cloudy scenes
                     .map(maskS2clouds)) # Apply cloud masking

        # Check if any images are available after filtering
        count = s2_sr_col.size().getInfo()
        if count == 0:
            # st.warning(f"Ù‡ÛŒÚ† ØªØµÙˆÛŒØ± Sentinel-2 Ø¨Ø¯ÙˆÙ† Ø§Ø¨Ø± Ø¯Ø± Ø¨Ø§Ø²Ù‡ {start_date} ØªØ§ {end_date} ÛŒØ§ÙØª Ù†Ø´Ø¯.")
            return None, f"No suitable Sentinel-2 images found for {start_date} to {end_date} (0 images after filtering)."

        # Calculate indices for each image in the collection
        indexed_col = s2_sr_col.map(add_indices)

        # Select the specific index band *before* median to potentially save memory
        index_only_col = indexed_col.select(index_name)

        # Create a median composite image
        # Use .unmask() if you want to fill masked pixels (e.g., with 0 or NaN) before median
        # median_image = index_only_col.median().unmask(ee.Number(0)) # Example: fill with 0
        median_image = index_only_col.median() # Default: median ignores masked pixels

        # Clip to the geometry if it's a polygon to potentially reduce data size
        if _geometry.type().getInfo() == 'Polygon':
            median_image = median_image.clip(_geometry)

        return median_image, None # Return the image and no error message

    except ee.EEException as e:
        error_message = f"Ø®Ø·Ø§ÛŒ Google Earth Engine Ø¯Ø± get_processed_image: {e}"
        # Try to extract more details if available
        try:
            error_details = e.args[0] if e.args else str(e)
            if isinstance(error_details, str):
                if 'computation timed out' in error_details.lower():
                     error_message += "\n(Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ø­Ø¬Ù… Ø¨Ø§Ù„Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÛŒØ§ Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ø·ÙˆÙ„Ø§Ù†ÛŒ)"
                elif 'user memory limit exceeded' in error_details.lower():
                     error_message += "\n(Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ù†Ø·Ù‚Ù‡ Ø¨Ø²Ø±Ú¯ ÛŒØ§ Ø¹Ù…Ù„ÛŒØ§Øª Ù¾ÛŒÚ†ÛŒØ¯Ù‡)"
                elif 'resolution' in error_details.lower():
                     error_message += "\n(Ù…Ù…Ú©Ù† Ø§Ø³Øª Ù…Ø±Ø¨ÙˆØ· Ø¨Ù‡ Ù…Ù‚ÛŒØ§Ø³ (scale) Ø¯Ø± reduceRegion Ø¨Ø§Ø´Ø¯)"
                elif 'image.select' in error_details.lower() and 'not found' in error_details.lower():
                     error_message += f"\n(Ø®Ø·Ø§: Ø¨Ø§Ù†Ø¯ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø¨Ø±Ø§ÛŒ Ø´Ø§Ø®Øµ '{index_name}' Ø¯Ø± Ø¨Ø±Ø®ÛŒ ØªØµØ§ÙˆÛŒØ± ÛŒØ§ÙØª Ù†Ø´Ø¯)"
                elif 'collection.size: Unable to compute' in error_details:
                     error_message += "\n(Ø®Ø·Ø§ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ ØªØµØ§ÙˆÛŒØ± Ø§ÙˆÙ„ÛŒÙ‡ - Ù…Ù…Ú©Ù† Ø§Ø³Øª Ù…Ø´Ú©Ù„ Ø¯Ø³ØªØ±Ø³ÛŒ ÛŒØ§ ÙÛŒÙ„ØªØ± GEE Ø¨Ø§Ø´Ø¯)"

        except Exception:
            pass # Ignore errors during error detail extraction
        # st.error(error_message) # Show error in main app? Maybe return it is better.
        return None, error_message
    except Exception as e:
        error_message = f"Ø®Ø·Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ GEE (get_processed_image): {e}\n{traceback.format_exc()}"
        # st.error(error_message)
        return None, error_message

# --- Function to get time series data for a point ---
#@st.cache_data(show_spinner="Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø±ÛŒØ§ÙØª Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ Ø´Ø§Ø®Øµ...", persist=True) # Caching can be complex with GEE objects
def get_index_time_series(_point_geom, index_name, start_date='2023-01-01', end_date=today.strftime('%Y-%m-%d')):
    """Gets a time series of a specified index for a point geometry."""
    try:
        s2_sr_col = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')
                     .filterBounds(_point_geom)
                     .filterDate(start_date, end_date)
                     # Consider adding a cloud filter here too, although maskS2clouds should handle it
                     .filter(ee.Filter.lt('CLOUDY_PIXEL_PERCENTAGE', 50)) # Stricter cloud filter for time series
                     .map(maskS2clouds)
                     .map(add_indices)
                     .filter(ee.Filter.listContains('system:band_names', index_name))) # Ensure index band exists

        def extract_value(image):
            try:
                # Extract the index value at the point
                # Use reduceRegion for points; scale should match sensor resolution
                # Use firstNonNull to get the first valid pixel value at the point
                value = image.select(index_name).reduceRegion(
                    reducer=ee.Reducer.firstNonNull(),
                    geometry=_point_geom,
                    scale=10 # Scale in meters (10m for Sentinel-2 relevant bands)
                ).get(index_name)

                # Only return feature if value is not null
                return ee.Feature(None, {'date': image.date().format('YYYY-MM-dd'), index_name: value}) \
                        .set('hasValue', ee.Algorithms.IsEqual(value, None).Not()) # Check if value is null

            except Exception as reduce_err:
                 # If reduceRegion fails for an image, return null feature
                 # print(f"Warning: reduceRegion failed for image {image.id().getInfo() if image.id() else 'unknown'} in time series. Error: {reduce_err}")
                 return ee.Feature(None, {'hasValue': False})


        # Map over the collection and filter out features where value extraction failed or was null
        ts_features = s2_sr_col.map(extract_value).filter(ee.Filter.eq('hasValue', True))

        # Limit the number of features to avoid GEE compute limits if necessary
        # ts_features = ts_features.limit(300, 'system:time_start', False) # Get 300 most recent

        # Convert the FeatureCollection to a list of dictionaries
        # Use getInfo() which can be slow/fail for large collections
        try:
            ts_info = ts_features.getInfo()['features']
        except ee.EEException as e:
             if 'too large' in str(e).lower():
                  # If getInfo fails due to size, try exporting or reducing the date range
                  return pd.DataFrame(columns=['date', index_name]), f"Ø®Ø·Ø§ÛŒ GEE: Ù…Ø¬Ù…ÙˆØ¹Ù‡ Ø¯Ø§Ø¯Ù‡ Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ Ø¨ÛŒØ´ Ø§Ø² Ø­Ø¯ Ø¨Ø²Ø±Ú¯ Ø§Ø³Øª ({e}). Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ø±Ø§ Ú©ÙˆØªØ§Ù‡â€ŒØªØ± Ú©Ù†ÛŒØ¯."
             else:
                  raise e # Re-raise other GEE errors


        if not ts_info:
            return pd.DataFrame(columns=['date', index_name]), f"Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ {index_name} ÛŒØ§ÙØª Ù†Ø´Ø¯ (Ù¾Ø³ Ø§Ø² ÙÛŒÙ„ØªØ±)."

        # Convert to Pandas DataFrame more carefully
        ts_data = []
        for f in ts_info:
            props = f.get('properties', {})
            date_val = props.get('date')
            index_val = props.get(index_name)
            # Ensure both date and value are present
            if date_val is not None and index_val is not None:
                 ts_data.append({'date': date_val, index_name: index_val})


        if not ts_data:
             return pd.DataFrame(columns=['date', index_name]), f"Ø¯Ø§Ø¯Ù‡ Ù…Ø¹ØªØ¨Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ {index_name} ÛŒØ§ÙØª Ù†Ø´Ø¯ (Ù¾Ø³ Ø§Ø² ÙÛŒÙ„ØªØ± Ù†Ù‡Ø§ÛŒÛŒ)."

        ts_df = pd.DataFrame(ts_data)
        ts_df['date'] = pd.to_datetime(ts_df['date'])
        ts_df[index_name] = pd.to_numeric(ts_df[index_name], errors='coerce')
        ts_df = ts_df.dropna(subset=[index_name]) # Ensure numeric conversion worked
        ts_df = ts_df.sort_values('date').set_index('date')

        return ts_df, None # Return DataFrame and no error
    except ee.EEException as e:
        error_message = f"Ø®Ø·Ø§ÛŒ GEE Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ: {e}"
        # st.error(error_message)
        return pd.DataFrame(columns=['date', index_name]), error_message
    except Exception as e:
        error_message = f"Ø®Ø·Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ: {e}\n{traceback.format_exc()}"
        # st.error(error_message)
        return pd.DataFrame(columns=['date', index_name]), error_message


# ==============================================================================
# Main Panel Display
# ==============================================================================

# --- Get Selected Farm Geometry and Details ---
selected_farm_details = None
selected_farm_geom = None
map_needs_update = True # Flag to control map rendering

# Only proceed if there are farms for the selected day
if filtered_farms_df.empty and selected_farm_name == "Ù‡Ù…Ù‡ Ù…Ø²Ø§Ø±Ø¹":
     st.warning(f"Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø± Ø±ÙˆØ² '{selected_day_display}' ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")
     map_needs_update = False # Don't try to draw map or tables
elif selected_farm_name == "Ù‡Ù…Ù‡ Ù…Ø²Ø§Ø±Ø¹":
    # Use the bounding box of all filtered farms for the map view
    try:
        # ***** USE NEW COORDINATE COLUMN NAMES *****
        min_lon, min_lat = filtered_farms_df['longitude'].min(), filtered_farms_df['latitude'].min()
        max_lon, max_lat = filtered_farms_df['longitude'].max(), filtered_farms_df['latitude'].max()
        # Create a bounding box geometry
        # Add a small buffer if min/max are the same (single point farm)
        if min_lon == max_lon and min_lat == max_lat:
            buffer = 0.001 # Small buffer in degrees
            selected_farm_geom = ee.Geometry.Rectangle([min_lon-buffer, min_lat-buffer, max_lon+buffer, max_lat+buffer])
        else:
            selected_farm_geom = ee.Geometry.Rectangle([min_lon, min_lat, max_lon, max_lat])

        st.subheader(f"ğŸ—ºï¸ Ù†Ù…Ø§ÛŒØ´ Ú©Ù„ÛŒ Ù…Ø²Ø§Ø±Ø¹ Ø¨Ø±Ø§ÛŒ Ø±ÙˆØ²: {selected_day_display}")
        st.info(f"ØªØ¹Ø¯Ø§Ø¯ Ù…Ø²Ø§Ø±Ø¹ Ø¯Ø± Ø§ÛŒÙ† Ø±ÙˆØ²: {len(filtered_farms_df)}")
    except Exception as e:
        st.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§ÛŒØ¬Ø§Ø¯ Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ù‡Ù…Ù‡ Ù…Ø²Ø§Ø±Ø¹: {e}")
        map_needs_update = False
else:
    # Handle single farm selection
    try:
        selected_farm_details = filtered_farms_df[filtered_farms_df['Ù…Ø²Ø±Ø¹Ù‡'] == selected_farm_name].iloc[0]
        # ***** USE NEW COORDINATE COLUMN NAMES *****
        lat = selected_farm_details['latitude']
        lon = selected_farm_details['longitude']
        if pd.isna(lat) or pd.isna(lon):
            st.error(f"Ù…Ø®ØªØµØ§Øª Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø¨Ø±Ø§ÛŒ Ù…Ø²Ø±Ø¹Ù‡ {selected_farm_name}.")
            map_needs_update = False
        else:
            selected_farm_geom = ee.Geometry.Point([lon, lat])
            st.subheader(f"ğŸ“ Ø¬Ø²Ø¦ÛŒØ§Øª Ù…Ø²Ø±Ø¹Ù‡: {selected_farm_name} (Ø±ÙˆØ²: {selected_day_display})")
            # Display farm details - use .get() for robustness if columns missing
            details_cols = st.columns(3)
            with details_cols[0]:
                # Assuming 'Ù…Ø³Ø§Ø­Øª', 'ÙˆØ§Ø±ÛŒØªÙ‡' etc. might still exist or use 'Ú¯Ø±ÙˆÙ‡' if relevant
                st.metric("Ú¯Ø±ÙˆÙ‡", f"{selected_farm_details.get('Ú¯Ø±ÙˆÙ‡', 'N/A')}") # Example using 'Ú¯Ø±ÙˆÙ‡' if it exists
                st.metric("Ù…Ø³Ø§Ø­Øª (Ù‡Ú©ØªØ§Ø±)", f"{selected_farm_details.get('Ù…Ø³Ø§Ø­Øª', 'N/A'):,.2f}" if pd.notna(selected_farm_details.get('Ù…Ø³Ø§Ø­Øª')) else "N/A") # Keep if 'Ù…Ø³Ø§Ø­Øª' exists
            with details_cols[1]:
                st.metric("ÙˆØ§Ø±ÛŒØªÙ‡", f"{selected_farm_details.get('ÙˆØ§Ø±ÛŒØªÙ‡', 'N/A')}") # Keep if 'ÙˆØ§Ø±ÛŒØªÙ‡' exists
                st.metric("Ø³Ù†", f"{selected_farm_details.get('Ø³Ù†', 'N/A')}") # Keep if 'Ø³Ù†' exists
            with details_cols[2]:
                st.metric("Ú©Ø§Ù†Ø§Ù„", f"{selected_farm_details.get('Ú©Ø§Ù†Ø§Ù„', 'N/A')}") # Keep if 'Ú©Ø§Ù†Ø§Ù„' exists
                st.metric("Ø§Ø¯Ø§Ø±Ù‡", f"{selected_farm_details.get('Ø§Ø¯Ø§Ø±Ù‡', 'N/A')}") # Keep if 'Ø§Ø¯Ø§Ø±Ù‡' exists
                # ***** DISPLAY NEW COORDINATE COLUMN NAMES *****
                st.metric("Ù…Ø®ØªØµØ§Øª (Lat, Lon)", f"{lat:.5f}, {lon:.5f}")
    except IndexError:
        st.error(f"Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø²Ø±Ø¹Ù‡ '{selected_farm_name}' ÛŒØ§ÙØª Ù†Ø´Ø¯ (Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙÛŒÙ„ØªØ± Ø´Ø¯Ù‡ Ù†Ø¨Ø§Ø´Ø¯).")
        map_needs_update = False
    except KeyError as e:
         st.error(f"Ø®Ø·Ø§: Ø³ØªÙˆÙ† Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² ({e}) Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø¬Ø²Ø¦ÛŒØ§Øª Ù…Ø²Ø±Ø¹Ù‡ ÛŒØ§ÙØª Ù†Ø´Ø¯.")
         map_needs_update = False
    except Exception as e:
        st.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù†Ù…Ø§ÛŒØ´ Ø¬Ø²Ø¦ÛŒØ§Øª Ù…Ø²Ø±Ø¹Ù‡: {e}")
        map_needs_update = False


# --- Map Display ---
if map_needs_update:
    st.markdown("---")
    st.subheader("ğŸ›°ï¸ Ù†Ù‚Ø´Ù‡ ÙˆØ¶Ø¹ÛŒØª Ù…Ø²Ø§Ø±Ø¹")

    # Define visualization parameters based on the selected index
    vis_params = {
        'NDVI': {'min': 0, 'max': 1, 'palette': ['#CE7E45', '#DF923D', '#F1B555', '#FCD163', '#99B718', '#74A901', '#66A000', '#529400', '#3E8601', '#207401', '#056201', '#004C00', '#023B01', '#012E01', '#011D01', '#011301']}, # Detailed green palette
        'EVI': {'min': 0, 'max': 1, 'palette': ['#CE7E45', '#DF923D', '#F1B555', '#FCD163', '#99B718', '#74A901', '#66A000', '#529400', '#3E8601', '#207401', '#056201', '#004C00', '#023B01', '#012E01', '#011D01', '#011301']},
        'NDMI': {'min': -0.5, 'max': 0.8, 'palette': ['#b2182b', '#d6604d', '#f4a582', '#fddbc7', '#f7f7f7', '#d1e5f0', '#92c5de', '#4393c3', '#2166ac']}, # Red-Blue for moisture
        'LAI': {'min': 0, 'max': 7, 'palette': ['#f7fcf5', '#e5f5e0', '#c7e9c0', '#a1d99b', '#74c476', '#41ab5d', '#238b45', '#006d2c', '#00441b']}, # Sequential green for LAI
        'MSI': {'min': 0, 'max': 3, 'palette': ['#2166ac', '#4393c3', '#92c5de', '#d1e5f0', '#f7f7f7', '#fddbc7', '#f4a582', '#d6604d', '#b2182b']}, # Blue-Red for stress (lower = wetter/less stress)
        'CVI': {'min': 0, 'max': 25, 'palette': ['#ffffcc', '#c7e9b4', '#7fcdbb', '#41b6c4', '#1d91c0', '#225ea8', '#0c2c84']}, # Yellow-Green-Blue for Chlorophyll
    }

    map_center_lat = INITIAL_LAT
    map_center_lon = INITIAL_LON
    initial_zoom = INITIAL_ZOOM

    # Create a geemap Map instance
    m = geemap.Map(
        location=[map_center_lat, map_center_lon],
        zoom=initial_zoom,
        add_google_map=False # Start clean
    )
    m.add_basemap("HYBRID") # Add Google Satellite Hybrid basemap

    # Get the processed image for the current week
    if selected_farm_geom:
        # Use a spinner while processing the image for the map
        with st.spinner(f"Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØµÙˆÛŒØ± {selected_index} Ø¨Ø±Ø§ÛŒ Ù†Ù‚Ø´Ù‡..."):
            gee_image_current, error_msg_current = get_processed_image(
                selected_farm_geom, start_date_current_str, end_date_current_str, selected_index
            )

        if error_msg_current:
            st.warning(f"Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØµÙˆÛŒØ± Ø¨Ø±Ø§ÛŒ Ø¯ÙˆØ±Ù‡ Ø¬Ø§Ø±ÛŒ: {error_msg_current}")

        if gee_image_current:
            # Add the GEE layer to the map
            try:
                current_vis = vis_params.get(selected_index, {'min': 0, 'max': 1, 'palette': ['red', 'yellow', 'green']}) # Default vis
                m.addLayer(
                    gee_image_current,
                    current_vis,
                    f"{selected_index} ({start_date_current_str} ØªØ§ {end_date_current_str})"
                )

                # --- Add Custom Legend ---
                legend_title_text = f"{selected_index} ({index_options.get(selected_index, '')})"
                palette = current_vis.get('palette', [])
                min_val = current_vis.get('min', 0)
                max_val = current_vis.get('max', 1)

                # Adjust labels for specific indices for clarity
                if selected_index in ['NDVI', 'EVI']:
                    legend_dict = {"Ø¨Ø§Ù„Ø§ (Ø³Ø§Ù„Ù…)": palette[-1], "Ù…ØªÙˆØ³Ø·": palette[len(palette)//2], "Ù¾Ø§ÛŒÛŒÙ† (Ø¶Ø¹ÛŒÙ)": palette[0]}
                elif selected_index == 'NDMI':
                     legend_dict = {"Ù…Ø±Ø·ÙˆØ¨": palette[-1], "Ù…ØªÙˆØ³Ø·": palette[len(palette)//2], "Ø®Ø´Ú©": palette[0]}
                elif selected_index == 'MSI':
                     # Note: Palette for MSI goes Blue(wet) to Red(dry), but dict keys reflect condition
                     legend_dict = {"ØªÙ†Ø´ Ú©Ù… (Ù…Ø±Ø·ÙˆØ¨)": palette[0], "Ù…ØªÙˆØ³Ø·": palette[len(palette)//2], "ØªÙ†Ø´ Ø¨Ø§Ù„Ø§ (Ø®Ø´Ú©)": palette[-1]}
                elif selected_index == 'LAI':
                     legend_dict = {"LAI Ø¨Ø§Ù„Ø§": palette[-1], "LAI Ù…ØªÙˆØ³Ø·": palette[len(palette)//2], "LAI Ù¾Ø§ÛŒÛŒÙ†": palette[0]}
                elif selected_index == 'CVI':
                     legend_dict = {"Ú©Ù„Ø±ÙˆÙÛŒÙ„ Ø¨Ø§Ù„Ø§": palette[-1], "Ú©Ù„Ø±ÙˆÙÛŒÙ„ Ù…ØªÙˆØ³Ø·": palette[len(palette)//2], "Ú©Ù„Ø±ÙˆÙÛŒÙ„ Ù¾Ø§ÛŒÛŒÙ†": palette[0]}
                else: # Default legend based on palette values
                    steps = len(palette)
                    legend_dict = {}
                    if steps > 1:
                         legend_dict[f"Ø¨Ø§Ù„Ø§ ({max_val:.2f})"] = palette[-1]
                         legend_dict[f"Ù…ØªÙˆØ³Ø· ({min_val + (max_val - min_val)/2:.2f})"] = palette[len(palette)//2]
                         legend_dict[f"Ù¾Ø§ÛŒÛŒÙ† ({min_val:.2f})"] = palette[0]
                    elif steps == 1:
                         legend_dict[f"{min_val:.2f} - {max_val:.2f}"] = palette[0]


                # ***** CORRECTED KEYWORD ARGUMENT: Use 'title' instead of 'legend_title' *****
                m.add_legend(title=legend_title_text, legend_dict=legend_dict, position='bottomright')


                # Add markers for farms
                if selected_farm_name == "Ù‡Ù…Ù‡ Ù…Ø²Ø§Ø±Ø¹" and not filtered_farms_df.empty:
                     # Add markers for all filtered farms
                     for idx, farm in filtered_farms_df.iterrows():
                         # ***** USE NEW COORDINATE COLUMN NAMES *****
                         lat_f, lon_f = farm['latitude'], farm['longitude']
                         if pd.notna(lat_f) and pd.notna(lon_f): # Check coords are valid
                             folium.Marker(
                                 location=[lat_f, lon_f],
                                 popup=(f"<b>Ù…Ø²Ø±Ø¹Ù‡:</b> {farm['Ù…Ø²Ø±Ø¹Ù‡']}<br>"
                                        #f"<b>Ú¯Ø±ÙˆÙ‡:</b> {farm.get('Ú¯Ø±ÙˆÙ‡', 'N/A')}<br>" # Uncomment if 'Ú¯Ø±ÙˆÙ‡' column exists and is useful
                                        f"<b>Ø±ÙˆØ²:</b> {farm.get('Ø±ÙˆØ²', 'N/A')}<br>"
                                        f"<b>Ú©Ø§Ù†Ø§Ù„:</b> {farm.get('Ú©Ø§Ù†Ø§Ù„', 'N/A')}<br>" # Keep if relevant
                                        f"<b>Ø§Ø¯Ø§Ø±Ù‡:</b> {farm.get('Ø§Ø¯Ø§Ø±Ù‡', 'N/A')}"), # Keep if relevant
                                 tooltip=farm['Ù…Ø²Ø±Ø¹Ù‡'],
                                 icon=folium.Icon(color='blue', icon='info-sign')
                             ).add_to(m)
                     # Adjust map bounds if showing all farms
                     if selected_farm_geom: # Ensure geom exists
                        m.center_object(selected_farm_geom, zoom=initial_zoom) # Center on the bounding box
                elif selected_farm_details is not None and selected_farm_geom:
                     # Add marker for the single selected farm
                     # ***** USE lat, lon derived earlier *****
                     folium.Marker(
                         location=[lat, lon],
                         popup=(f"<b>Ù…Ø²Ø±Ø¹Ù‡:</b> {selected_farm_name}<br>"
                                f"<b>{selected_index} (Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ):</b> Ø¯Ø± Ø­Ø§Ù„ Ù…Ø­Ø§Ø³Ø¨Ù‡...<br>" # Placeholder, value added later if needed
                                f"<b>Ú©Ø§Ù†Ø§Ù„:</b> {selected_farm_details.get('Ú©Ø§Ù†Ø§Ù„', 'N/A')}<br>"
                                f"<b>Ø§Ø¯Ø§Ø±Ù‡:</b> {selected_farm_details.get('Ø§Ø¯Ø§Ø±Ù‡', 'N/A')}"),
                         tooltip=selected_farm_name,
                         icon=folium.Icon(color='red', icon='star')
                     ).add_to(m)
                     m.center_object(selected_farm_geom, zoom=15) # Zoom closer for a single farm

                m.add_layer_control() # Add layer control to toggle base maps and layers

            except ee.EEException as map_err:
                st.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§ÙØ²ÙˆØ¯Ù† Ù„Ø§ÛŒÙ‡ GEE Ø¨Ù‡ Ù†Ù‚Ø´Ù‡: {map_err}")
                # st.error(traceback.format_exc())
            except Exception as map_err:
                st.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù‚Ø´Ù‡ ÛŒØ§ Ø§ÙØ²ÙˆØ¯Ù† Ù†Ø´Ø§Ù†Ú¯Ø±Ù‡Ø§: {map_err}")
                st.error(traceback.format_exc())
        else:
            # Handle case where GEE image could not be processed but geom exists
            st.warning(f"ØªØµÙˆÛŒØ±ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø±ÙˆÛŒ Ù†Ù‚Ø´Ù‡ Ø¯Ø± Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ø¬Ø§Ø±ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯ ÛŒØ§ Ù‚Ø§Ø¨Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù†Ø¨ÙˆØ¯.")
            # Still show markers if possible
            try:
                if selected_farm_name == "Ù‡Ù…Ù‡ Ù…Ø²Ø§Ø±Ø¹" and not filtered_farms_df.empty:
                     for idx, farm in filtered_farms_df.iterrows():
                          lat_f, lon_f = farm['latitude'], farm['longitude']
                          if pd.notna(lat_f) and pd.notna(lon_f):
                              folium.Marker(location=[lat_f, lon_f], tooltip=farm['Ù…Ø²Ø±Ø¹Ù‡']).add_to(m)
                     if selected_farm_geom: m.center_object(selected_farm_geom, zoom=initial_zoom)
                elif selected_farm_details is not None and selected_farm_geom:
                     folium.Marker(location=[lat, lon], tooltip=selected_farm_name).add_to(m)
                     m.center_object(selected_farm_geom, zoom=15)
                m.add_layer_control()
            except Exception as marker_err:
                 st.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§ÙØ²ÙˆØ¯Ù† Ù†Ø´Ø§Ù†Ú¯Ø±Ù‡Ø§ Ø¨Ù‡ Ù†Ù‚Ø´Ù‡ Ø®Ø§Ù„ÛŒ: {marker_err}")


    # Display the map in Streamlit
    st_folium(m, width=None, height=500, use_container_width=True, returned_objects=[]) # returned_objects=[] might prevent some callback issues
    st.caption("Ø¨Ø±Ø§ÛŒ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø¬Ø²Ø¦ÛŒØ§Øª Ø±ÙˆÛŒ Ù…Ø§Ø±Ú©Ø±Ù‡Ø§ Ú©Ù„ÛŒÚ© Ú©Ù†ÛŒØ¯. Ø§Ø² Ú©Ù†ØªØ±Ù„ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ (Ø¨Ø§Ù„Ø§ Ø³Ù…Øª Ø±Ø§Ø³Øª) Ø¨Ø±Ø§ÛŒ ØªØºÛŒÛŒØ± Ù†Ù‚Ø´Ù‡ Ù¾Ø§ÛŒÙ‡ Ùˆ Ø±ÙˆØ´Ù†/Ø®Ø§Ù…ÙˆØ´ Ú©Ø±Ø¯Ù† Ù„Ø§ÛŒÙ‡ Ø´Ø§Ø®Øµ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.")
    st.info("ğŸ’¡ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ù†Ù‚Ø´Ù‡ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² Ø§Ø¨Ø²Ø§Ø± Ø¹Ú©Ø³ Ú¯Ø±ÙØªÙ† Ø§Ø² ØµÙØ­Ù‡ (Screenshot) Ù…Ø±ÙˆØ±Ú¯Ø± ÛŒØ§ Ø³ÛŒØ³ØªÙ… Ø¹Ø§Ù…Ù„ Ø®ÙˆØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.")
else:
    # If map_needs_update is False, show a message instead of the map section
    st.markdown("---")
    st.warning("Ù†Ù‚Ø´Ù‡ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø¹ØªØ¨Ø± ÛŒØ§ Ø§Ù†ØªØ®Ø§Ø¨ Ù…Ø²Ø±Ø¹Ù‡ Ù†Ø§Ù…Ø¹ØªØ¨Ø±ØŒ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯.")


# --- Time Series Chart ---
if map_needs_update: # Only show chart if map was attempted
    st.markdown("---")
    st.subheader(f"ğŸ“ˆ Ù†Ù…ÙˆØ¯Ø§Ø± Ø±ÙˆÙ†Ø¯ Ø²Ù…Ø§Ù†ÛŒ Ø´Ø§Ø®Øµ {selected_index}")

    if selected_farm_name == "Ù‡Ù…Ù‡ Ù…Ø²Ø§Ø±Ø¹":
        st.info("Ù„Ø·ÙØ§Ù‹ ÛŒÚ© Ù…Ø²Ø±Ø¹Ù‡ Ø®Ø§Øµ Ø±Ø§ Ø§Ø² Ù¾Ù†Ù„ Ú©Ù†Ø§Ø±ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯ ØªØ§ Ù†Ù…ÙˆØ¯Ø§Ø± Ø±ÙˆÙ†Ø¯ Ø²Ù…Ø§Ù†ÛŒ Ø¢Ù† Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆØ¯.")
    elif selected_farm_geom:
        # Check if the geometry is a Point
        is_point = False
        if selected_farm_geom: # Check if geom exists first
            try:
                is_point = selected_farm_geom.type().getInfo() == 'Point'
            except Exception as geom_err:
                st.warning(f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø±Ø±Ø³ÛŒ Ù†ÙˆØ¹ Ù‡Ù†Ø¯Ø³Ù‡ Ù…Ø²Ø±Ø¹Ù‡: {geom_err}")

        if is_point:
            # Define a longer period for the time series chart (e.g., last 6-12 months)
            timeseries_end_date = today.strftime('%Y-%m-%d')
            timeseries_start_date = (today - datetime.timedelta(days=365)).strftime('%Y-%m-%d') # 1 year

            with st.spinner(f"Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ {selected_index} Ø¨Ø±Ø§ÛŒ {selected_farm_name}..."):
                ts_df, ts_error = get_index_time_series(
                    selected_farm_geom,
                    selected_index,
                    start_date=timeseries_start_date,
                    end_date=timeseries_end_date
                )

            if ts_error:
                st.warning(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ: {ts_error}")
            elif not ts_df.empty:
                try:
                    # Create interactive chart with Plotly
                    fig = px.line(ts_df, y=selected_index, markers=True,
                                  title=f"Ø±ÙˆÙ†Ø¯ ØªØºÛŒÛŒØ±Ø§Øª {selected_index} Ø¨Ø±Ø§ÛŒ Ù…Ø²Ø±Ø¹Ù‡ {selected_farm_name}",
                                  labels={'date': 'ØªØ§Ø±ÛŒØ®', selected_index: f'Ù…Ù‚Ø¯Ø§Ø± {selected_index}'})
                    fig.update_layout(xaxis_title="ØªØ§Ø±ÛŒØ®", yaxis_title=f"Ù…Ù‚Ø¯Ø§Ø± {selected_index}", title_x=0.5, title_font_family="Vazirmatn", font_family="Vazirmatn")
                    fig.update_traces(line_color='#17a2b8', marker=dict(color='#17a2b8'))
                    st.plotly_chart(fig, use_container_width=True)
                    st.caption(f"Ù†Ù…ÙˆØ¯Ø§Ø± ØªØºÛŒÛŒØ±Ø§Øª Ø´Ø§Ø®Øµ {selected_index} Ø¨Ø±Ø§ÛŒ Ù…Ø²Ø±Ø¹Ù‡ {selected_farm_name} Ø¯Ø± ÛŒÚ© Ø³Ø§Ù„ Ú¯Ø°Ø´ØªÙ‡ (Ù†Ù‚Ø§Ø· Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ù‡ ØªØµØ§ÙˆÛŒØ± Ù…Ø§Ù‡ÙˆØ§Ø±Ù‡â€ŒØ§ÛŒ Ø¨Ø¯ÙˆÙ† Ø§Ø¨Ø± Ù‡Ø³ØªÙ†Ø¯).")
                except Exception as chart_err:
                    st.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø±: {chart_err}")
                    # Fallback to basic chart if plotly fails
                    try:
                        st.line_chart(ts_df[selected_index])
                    except Exception as basic_chart_err:
                        st.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø± Ù¾Ø§ÛŒÙ‡: {basic_chart_err}")
            else:
                st.info(f"Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆØ¯Ø§Ø± Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ {selected_index} Ø¯Ø± Ø¨Ø§Ø²Ù‡ Ù…Ø´Ø®Øµ Ø´Ø¯Ù‡ (ÛŒÚ© Ø³Ø§Ù„ Ø§Ø®ÛŒØ±) ÛŒØ§ÙØª Ù†Ø´Ø¯.")
        else:
            st.warning("Ù†Ù…ÙˆØ¯Ø§Ø± Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ù…Ø²Ø§Ø±Ø¹ Ù…Ù†ÙØ±Ø¯ (Ø¨Ø§ Ù…Ø®ØªØµØ§Øª Ù†Ù‚Ø·Ù‡â€ŒØ§ÛŒ) Ù‚Ø§Ø¨Ù„ Ù†Ù…Ø§ÛŒØ´ Ø§Ø³Øª.")
    # else: # Case handled by selected_farm_name == "Ù‡Ù…Ù‡ Ù…Ø²Ø§Ø±Ø¹" or map_needs_update=False
    #     st.warning("Ù‡Ù†Ø¯Ø³Ù‡ Ù…Ø²Ø±Ø¹Ù‡ Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆØ¯Ø§Ø± Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª.")


# --- Ranking Table ---
if map_needs_update and not filtered_farms_df.empty: # Only show table if map was attempted and data exists for the day
    st.markdown("---")
    st.subheader(f"ğŸ“Š Ø¬Ø¯ÙˆÙ„ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…Ø²Ø§Ø±Ø¹ Ø¨Ø± Ø§Ø³Ø§Ø³ {selected_index} (Ø±ÙˆØ²: {selected_day_display})")
    st.markdown("Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù…ØªÙˆØ³Ø· Ø´Ø§Ø®Øµ Ø¯Ø± Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ Ø¨Ø§ Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„.")

    # Use a separate cache for ranking calculation as it depends on more inputs
    @st.cache_data(show_spinner=f"Ø¯Ø± Ø­Ø§Ù„ Ù…Ø­Ø§Ø³Ø¨Ù‡ {selected_index} Ø¨Ø±Ø§ÛŒ Ù…Ø²Ø§Ø±Ø¹...", persist="disk") # Persist to disk might help with large datasets
    def calculate_weekly_indices(_farms_df_subset_records, index_name, start_curr, end_curr, start_prev, end_prev, _selected_day_cache_key):
        """Calculates the average index value for the current and previous week for a list of farms."""
        # Input is now a list of dicts to be cache-friendly
        # Note: _selected_day_cache_key added to make cache specific to the selected day

        results = []
        errors = []
        total_farms = len(_farms_df_subset_records)
        # Avoid progress bar inside cached function directly

        for i, farm_info in enumerate(_farms_df_subset_records):
            farm_name = farm_info.get('Ù…Ø²Ø±Ø¹Ù‡', 'Ù†Ø§Ù…Ø´Ø®Øµ')
            lat = farm_info.get('latitude')
            lon = farm_info.get('longitude')

            # Basic check before creating point
            if lat is None or lon is None or pd.isna(lat) or pd.isna(lon):
                errors.append(f"{farm_name}: Ù…Ø®ØªØµØ§Øª Ù†Ø§Ù…Ø¹ØªØ¨Ø±.")
                # Add placeholder result to maintain row count if needed, or just skip
                results.append({
                    'Ù…Ø²Ø±Ø¹Ù‡': farm_name,
                    'Ú©Ø§Ù†Ø§Ù„': farm_info.get('Ú©Ø§Ù†Ø§Ù„', 'N/A'),
                    'Ø§Ø¯Ø§Ø±Ù‡': farm_info.get('Ø§Ø¯Ø§Ø±Ù‡', 'N/A'),
                    f'{index_name} (Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ)': None,
                    f'{index_name} (Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„)': None,
                    'ØªØºÛŒÛŒØ±': None
                })
                continue

            point_geom = ee.Geometry.Point([lon, lat])

            def get_mean_value(start, end):
                # This function uses GEE and cannot be directly part of the cached function's core logic
                # if we want the results to be cached. We calculate GEE results *outside* or accept
                # that this part is re-run. For simplicity here, we keep it, but acknowledge cache limitations.
                try:
                    image, error = get_processed_image(point_geom, start, end, index_name)
                    if image:
                        # Reduce region to get the mean value at the point
                        mean_dict = image.reduceRegion(
                            reducer=ee.Reducer.mean(),
                            geometry=point_geom,
                            scale=10,  # Scale in meters
                            maxPixels=1e9 # Increase maxPixels if needed
                        ).getInfo()
                        # Check if the key exists and value is not None
                        value = mean_dict.get(index_name) if mean_dict else None
                        return value if value is not None else None, None # Return None if value is null
                    else:
                        # If no image, error message should explain why
                        return None, error if error else "No image found"
                except ee.EEException as e:
                    # Catch GEE errors during reduceRegion or getInfo
                    err_detail = str(e)
                    error_reason = f"Ø®Ø·Ø§ÛŒ GEE ({e})"
                    if 'reduceRegion' in err_detail.lower() and 'memory' in err_detail.lower():
                        error_reason = "Ø®Ø·Ø§ÛŒ Ø­Ø§ÙØ¸Ù‡ GEE"
                    elif 'reduceRegion' in err_detail.lower() and 'time limit' in err_detail.lower():
                        error_reason = "Ù¾Ø§ÛŒØ§Ù† Ø²Ù…Ø§Ù† GEE"
                    elif 'cannot be applied to objects of type <null>' in err_detail.lower():
                         error_reason = "Ø®Ø·Ø§ÛŒ Ø¯Ø§Ø¯Ù‡ Null Ø¯Ø± GEE" # Often due to masked inputs
                    return None, error_reason
                except Exception as e:
                     # Catch other potential errors
                     return None, f"Ø®Ø·Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡: {e}"


            # Calculate for current week
            current_val, err_curr = get_mean_value(start_curr, end_curr)
            if err_curr: errors.append(f"{farm_name} (Ø¬Ø§Ø±ÛŒ: {start_curr}-{end_curr}): {err_curr}")

            # Calculate for previous week
            previous_val, err_prev = get_mean_value(start_prev, end_prev)
            if err_prev: errors.append(f"{farm_name} (Ù‚Ø¨Ù„: {start_prev}-{end_prev}): {err_prev}")


            # Calculate change only if both values are valid numbers
            change = None
            # Check if values are numeric before subtracting
            if isinstance(current_val, (int, float)) and isinstance(previous_val, (int, float)):
                 change = current_val - previous_val


            results.append({
                'Ù…Ø²Ø±Ø¹Ù‡': farm_name,
                # Include other details if needed and present in farm_info
                'Ú©Ø§Ù†Ø§Ù„': farm_info.get('Ú©Ø§Ù†Ø§Ù„', 'N/A'),
                'Ø§Ø¯Ø§Ø±Ù‡': farm_info.get('Ø§Ø¯Ø§Ø±Ù‡', 'N/A'),
                f'{index_name} (Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ)': current_val,
                f'{index_name} (Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„)': previous_val,
                'ØªØºÛŒÛŒØ±': change
            })

            # Update progress outside the loop if needed

        return pd.DataFrame(results), errors

    # Prepare subset of data for caching (convert to list of dicts)
    cols_needed = ['Ù…Ø²Ø±Ø¹Ù‡', 'latitude', 'longitude', 'Ú©Ø§Ù†Ø§Ù„', 'Ø§Ø¯Ø§Ø±Ù‡'] # Adjust if needed
    cols_available = [col for col in cols_needed if col in filtered_farms_df.columns]
    # Use 'records' that are JSON serializable for caching
    farms_subset_records = filtered_farms_df[cols_available].to_dict('records')


    # Calculate and display the ranking table
    # Add a progress bar here
    ranking_progress = st.progress(0)
    status_text = st.text(f"Ø¯Ø± Ø­Ø§Ù„ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø§Ø®Øµ {selected_index} Ø¨Ø±Ø§ÛŒ {len(farms_subset_records)} Ù…Ø²Ø±Ø¹Ù‡...")
    ranking_df = pd.DataFrame() # Initialize empty DataFrame
    calculation_errors = []
    try:
        ranking_df, calculation_errors = calculate_weekly_indices(
            farms_subset_records, # Pass list of dicts
            selected_index,
            start_date_current_str,
            end_date_current_str,
            start_date_previous_str,
            end_date_previous_str,
            selected_day_display # Pass display day as cache key part
        )
        ranking_progress.progress(1.0) # Mark as complete
        status_text.text(f"Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø§Ø®Øµ {selected_index} Ø¨Ø±Ø§ÛŒ {len(ranking_df)} Ù…Ø²Ø±Ø¹Ù‡ Ú©Ø§Ù…Ù„ Ø´Ø¯.")
        ranking_progress.empty() # Remove progress bar after a short delay or success message
    except Exception as calc_err:
        ranking_progress.empty()
        status_text.text(f"Ø®Ø·Ø§ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ.")
        st.error(f"Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ: {calc_err}")
        st.error(traceback.format_exc())
        calculation_errors.append(f"Ø®Ø·Ø§ÛŒ Ú©Ù„ÛŒ: {calc_err}")


    # Display any errors that occurred during calculation
    if calculation_errors:
        st.warning("âš ï¸ Ø¨Ø±Ø®ÛŒ Ø®Ø·Ø§Ù‡Ø§ Ø¯Ø± Ø­ÛŒÙ† Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ Ø±Ø® Ø¯Ø§Ø¯:")
        error_expander = st.expander("Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø¬Ø²Ø¦ÛŒØ§Øª Ø®Ø·Ø§Ù‡Ø§", expanded=False)
        with error_expander:
            # Show unique errors to avoid repetition
            unique_errors = sorted(list(set(calculation_errors)))
            for error in unique_errors[:20]: # Limit displayed errors
                st.caption(f"- {error}")
            if len(unique_errors) > 20:
                 st.caption(f"... Ùˆ {len(unique_errors) - 20} Ø®Ø·Ø§ÛŒ Ù…Ù†Ø­ØµØ±Ø¨ÙØ±Ø¯ Ø¯ÛŒÚ¯Ø±.")
        # Show only a summary count if too many errors
        if len(calculation_errors) > 5:
             st.warning(f"(ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ø±Ø®Ø¯Ø§Ø¯Ù‡Ø§ÛŒ Ø®Ø·Ø§: {len(calculation_errors)})")


    if not ranking_df.empty:
        # Drop rows that might have been added as placeholders for invalid coords
        ranking_df = ranking_df.dropna(subset=[f'{selected_index} (Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ)', f'{selected_index} (Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„)'], how='all')


        if not ranking_df.empty:
            # Sort by the current week's index value
            ascending_sort = selected_index in ['MSI'] # Indices where lower is generally 'better' (less stress)
            sort_col = f'{selected_index} (Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ)'
            try:
                # Convert column to numeric before sorting, coercing errors
                ranking_df[sort_col] = pd.to_numeric(ranking_df[sort_col], errors='coerce')
                ranking_df_sorted = ranking_df.sort_values(
                    by=sort_col,
                    ascending=ascending_sort,
                    na_position='last' # Put farms with no data at the bottom
                ).reset_index(drop=True)
            except KeyError:
                 st.error(f"Ø®Ø·Ø§: Ø³ØªÙˆÙ† Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ '{sort_col}' ÛŒØ§ÙØª Ù†Ø´Ø¯.")
                 ranking_df_sorted = ranking_df.reset_index(drop=True) # Use unsorted if sort fails
            except Exception as sort_err:
                 st.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù…Ø±ØªØ¨â€ŒØ³Ø§Ø²ÛŒ Ø¬Ø¯ÙˆÙ„: {sort_err}")
                 ranking_df_sorted = ranking_df.reset_index(drop=True) # Use unsorted if sort fails


            # Add rank number (1-based index)
            ranking_df_sorted.index = ranking_df_sorted.index + 1
            ranking_df_sorted.index.name = 'Ø±ØªØ¨Ù‡'

            # Add a status column based on 'change'
            def determine_status(row, index_name):
                change_val = row['ØªØºÛŒÛŒØ±'] # Already calculated or None
                current_val = row[f'{index_name} (Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ)']

                # Check for None or NaN explicitly
                if pd.isna(change_val):
                    if pd.isna(current_val):
                         return "Ø¨Ø¯ÙˆÙ† Ø¯Ø§Ø¯Ù‡" # No data for either week
                    else:
                         return "Ø¯Ø§Ø¯Ù‡ Ø¬Ø¯ÛŒØ¯" # Data only for current week
                elif not isinstance(change_val, (int, float)):
                    # If change is not numeric (e.g., an error string), mark as error
                    return "Ø®Ø·Ø§ Ø¯Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡"

                # Define thresholds for significant change (adjust as needed)
                threshold = 0.05 # Example threshold for NDVI/EVI/NDMI
                if index_name == 'MSI': threshold = 0.1 # Example for MSI
                if index_name == 'LAI': threshold = 0.2 # Example for LAI
                if index_name == 'CVI': threshold = 1.0 # Example for CVI

                # Status logic based on index type
                positive_change_label = "Ø±Ø´Ø¯ Ù…Ø«Ø¨Øª"
                negative_change_label = "ØªÙ†Ø´/Ú©Ø§Ù‡Ø´"
                improve_label = "Ø¨Ù‡Ø¨ÙˆØ¯ (Ø±Ø·ÙˆØ¨Øª/ØªÙ†Ø´)"
                worsen_label = "Ø¨Ø¯ØªØ± Ø´Ø¯Ù† (Ø±Ø·ÙˆØ¨Øª/ØªÙ†Ø´)"

                if index_name in ['NDVI', 'EVI', 'LAI', 'CVI']: # Higher is better
                    if change_val > threshold: return positive_change_label
                    elif change_val < -threshold: return negative_change_label
                    else: return "Ø«Ø§Ø¨Øª"
                elif index_name in ['NDMI']: # Higher is wetter (often better for crops)
                    if change_val > threshold: return improve_label # More moist
                    elif change_val < -threshold: return worsen_label # Dryer
                    else: return "Ø«Ø§Ø¨Øª"
                elif index_name in ['MSI']: # Lower is wetter/less stress (better)
                    if change_val < -threshold: return improve_label # Less stress
                    elif change_val > threshold: return worsen_label # More stress
                    else: return "Ø«Ø§Ø¨Øª"
                else: # Default/unknown index
                    if change_val > threshold: return "Ø§ÙØ²Ø§ÛŒØ´"
                    elif change_val < -threshold: return "Ú©Ø§Ù‡Ø´"
                    else: return "Ø«Ø§Ø¨Øª"

            ranking_df_sorted['ÙˆØ¶Ø¹ÛŒØª'] = ranking_df_sorted.apply(lambda row: determine_status(row, selected_index), axis=1)

            # Format numbers for better readability *after* status calculation
            cols_to_format = [f'{selected_index} (Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ)', f'{selected_index} (Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„)', 'ØªØºÛŒÛŒØ±']
            for col in cols_to_format:
                if col in ranking_df_sorted.columns:
                     # Apply formatting, handling potential non-numeric 'N/A' or None
                     ranking_df_sorted[col] = ranking_df_sorted[col].apply(lambda x: f"{x:.3f}" if isinstance(x, (int, float)) else ("N/A" if pd.isna(x) else str(x)))


            # Define columns to display
            display_cols = ['Ù…Ø²Ø±Ø¹Ù‡', 'Ú©Ø§Ù†Ø§Ù„', 'Ø§Ø¯Ø§Ø±Ù‡', f'{selected_index} (Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ)', f'{selected_index} (Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„)', 'ØªØºÛŒÛŒØ±', 'ÙˆØ¶Ø¹ÛŒØª']
            # Filter out columns that might not exist in the source CSV (like Ú©Ø§Ù†Ø§Ù„, Ø§Ø¯Ø§Ø±Ù‡) if they weren't found
            display_cols_final = ['Ø±ØªØ¨Ù‡'] + [col for col in display_cols if col in ranking_df_sorted.columns]


            # Display the table using st.dataframe for better interactivity
            st.dataframe(ranking_df_sorted[display_cols_final], use_container_width=True)

            # --- Status Summary ---
            st.subheader("ğŸ“Š Ø®Ù„Ø§ØµÙ‡ ÙˆØ¶Ø¹ÛŒØª ØªØºÛŒÛŒØ±Ø§Øª Ù‡ÙØªÚ¯ÛŒ")
            if 'ÙˆØ¶Ø¹ÛŒØª' in ranking_df_sorted.columns:
                status_counts = ranking_df_sorted['ÙˆØ¶Ø¹ÛŒØª'].value_counts()

                # Map status labels to icons/colors for summary display
                status_map = {
                    "Ø±Ø´Ø¯ Ù…Ø«Ø¨Øª": ("ğŸŸ¢", "Ù…Ø«Ø¨Øª"),
                    "Ø¨Ù‡Ø¨ÙˆØ¯ (Ø±Ø·ÙˆØ¨Øª/ØªÙ†Ø´)": ("ğŸŸ¢", "Ù…Ø«Ø¨Øª"),
                    "Ø«Ø§Ø¨Øª": ("âšª", "Ø®Ù†Ø«ÛŒ"),
                    "ØªÙ†Ø´/Ú©Ø§Ù‡Ø´": ("ğŸ”´", "Ù…Ù†ÙÛŒ"),
                    "Ø¨Ø¯ØªØ± Ø´Ø¯Ù† (Ø±Ø·ÙˆØ¨Øª/ØªÙ†Ø´)": ("ğŸ”´", "Ù…Ù†ÙÛŒ"),
                    "Ø¯Ø§Ø¯Ù‡ Ø¬Ø¯ÛŒØ¯": ("ğŸ†•", "Ù†Ø§Ù…Ø´Ø®Øµ"), # New status for first week data
                    "Ø®Ø·Ø§ Ø¯Ø± Ù…Ù‚Ø§ÛŒØ³Ù‡": ("âš ï¸", "Ù†Ø§Ù…Ø´Ø®Øµ"),
                    "Ø¨Ø¯ÙˆÙ† Ø¯Ø§Ø¯Ù‡": ("âš«", "Ù†Ø§Ù…Ø´Ø®Øµ"),
                     # Add default for unknown statuses
                     "Ø§ÙØ²Ø§ÛŒØ´": ("ğŸŸ¢", "Ù…Ø«Ø¨Øª"),
                     "Ú©Ø§Ù‡Ø´": ("ğŸ”´", "Ù…Ù†ÙÛŒ"),
                }

                # Group counts by type (Positive, Negative, Neutral, Unknown)
                summary_counts = {"Ù…Ø«Ø¨Øª": 0, "Ù…Ù†ÙÛŒ": 0, "Ø®Ù†Ø«ÛŒ": 0, "Ù†Ø§Ù…Ø´Ø®Øµ": 0}
                for status, count in status_counts.items():
                    icon, type = status_map.get(status, ("â“", "Ù†Ø§Ù…Ø´Ø®Øµ"))
                    summary_counts[type] += count

                # Display summary counts using columns and metrics
                num_cols_to_show = len([c for c in summary_counts.values() if c > 0])
                if num_cols_to_show > 0:
                    summary_cols = st.columns(num_cols_to_show)
                    col_idx = 0
                    if summary_counts["Ù…Ø«Ø¨Øª"] > 0:
                        with summary_cols[col_idx]:
                            st.metric("ğŸŸ¢ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡", summary_counts["Ù…Ø«Ø¨Øª"])
                        col_idx += 1
                    if summary_counts["Ø®Ù†Ø«ÛŒ"] > 0:
                         with summary_cols[col_idx]:
                             st.metric("âšª Ø«Ø§Ø¨Øª", summary_counts["Ø®Ù†Ø«ÛŒ"])
                         col_idx += 1
                    if summary_counts["Ù…Ù†ÙÛŒ"] > 0:
                        with summary_cols[col_idx]:
                            st.metric("ğŸ”´ Ø¨Ø¯ØªØ± Ø´Ø¯Ù‡", summary_counts["Ù…Ù†ÙÛŒ"])
                        col_idx += 1
                    # Combine all 'Ù†Ø§Ù…Ø´Ø®Øµ' types
                    if summary_counts["Ù†Ø§Ù…Ø´Ø®Øµ"] > 0:
                         with summary_cols[col_idx]:
                             st.metric("âš« Ù†Ø§Ù…Ø´Ø®Øµ/Ø¬Ø¯ÛŒØ¯/Ø®Ø·Ø§", summary_counts["Ù†Ø§Ù…Ø´Ø®Øµ"])
                         col_idx += 1
                else:
                     st.info("Ø®Ù„Ø§ØµÙ‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")


                # Add explanation expander
                with st.expander("Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ ÙˆØ¶Ø¹ÛŒØªâ€ŒÙ‡Ø§", expanded=False):
                    st.markdown("""
                    *   **ğŸŸ¢ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡**: ÙˆØ¶Ø¹ÛŒØª Ø´Ø§Ø®Øµ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„ Ø¨Ù‡ØªØ± Ø´Ø¯Ù‡ Ø§Ø³Øª.
                    *   **âšª Ø«Ø§Ø¨Øª**: ØªØºÛŒÛŒØ± Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡ÛŒ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.
                    *   **ğŸ”´ Ø¨Ø¯ØªØ± Ø´Ø¯Ù‡**: ÙˆØ¶Ø¹ÛŒØª Ø´Ø§Ø®Øµ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„ Ù†Ø§Ù…Ø·Ù„ÙˆØ¨â€ŒØªØ± Ø´Ø¯Ù‡ Ø§Ø³Øª.
                    *   **âš« Ù†Ø§Ù…Ø´Ø®Øµ/Ø¬Ø¯ÛŒØ¯/Ø®Ø·Ø§**: Ø¯Ø§Ø¯Ù‡ Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ù‚Ø§ÛŒØ³Ù‡ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´ØªÙ‡ (Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¯Ø§Ø¯Ù‡ ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ø´Ø¯ 'Ø¯Ø§Ø¯Ù‡ Ø¬Ø¯ÛŒØ¯'ØŒ ÛŒØ§ Ø®Ø·Ø§ÛŒÛŒ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø±Ø® Ø¯Ø§Ø¯Ù‡ Ø¨Ø§Ø´Ø¯ØŒ ÛŒØ§ Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ù…ÙˆØ¬ÙˆØ¯ Ù†Ø¨Ø§Ø´Ø¯ 'Ø¨Ø¯ÙˆÙ† Ø¯Ø§Ø¯Ù‡').
                    """)

                # --- Download Button ---
                try:
                    # Re-create the final display DF for download if needed (with rank as column)
                    download_df = ranking_df_sorted[display_cols_final].reset_index() # Keep rank as column
                    csv_data = download_df.to_csv(index=False, encoding='utf-8-sig') # Use utf-8-sig for Excel compatibility
                    st.download_button(
                        label="ğŸ“¥ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¬Ø¯ÙˆÙ„ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ (CSV)",
                        data=csv_data,
                        file_name=f'ranking_{selected_index}_{selected_day_display.replace(" ", "_")}_{end_date_current_str}.csv',
                        mime='text/csv',
                    )
                except Exception as e:
                    st.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§ÛŒØ¬Ø§Ø¯ ÙØ§ÛŒÙ„ Ø¯Ø§Ù†Ù„ÙˆØ¯: {e}")

            else:
                 st.info("Ø³ØªÙˆÙ† 'ÙˆØ¶Ø¹ÛŒØª' Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø®Ù„Ø§ØµÙ‡ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª.")
        else:
            st.info(f"Ù¾Ø³ Ø§Ø² Ø­Ø°Ù Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ù†Ø§Ù…Ø¹ØªØ¨Ø±ØŒ Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø¬Ø¯ÙˆÙ„ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ {selected_index} Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.")

    elif not calculation_errors: # If DF is empty but no errors reported during calculation
        st.info(f"Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯ØŒ Ø§Ù…Ø§ Ù†ØªÛŒØ¬Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø¬Ø¯ÙˆÙ„ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ {selected_index} Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.")


st.markdown("---")
st.sidebar.markdown("---")
st.sidebar.markdown("Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡ Ø¨Ø§ ğŸ’» ØªÙˆØ³Ø· [Ú¯Ø±ÙˆÙ‡ Ù…Ø·Ø§Ù„Ø¹Ø§Øª Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ] Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Streamlit, Google Earth Engine, Ùˆ geemap.")
st.sidebar.markdown(f"Ø¢Ø®Ø±ÛŒÙ† Ø¨Ø±ÙˆØ²Ø±Ø³Ø§Ù†ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§: {today.strftime('%Y-%m-%d')}") # Indicate data freshness