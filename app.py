import streamlit as st
import pandas as pd
import ee
import geemap.foliumap as geemap
import folium
import json
import datetime
import plotly.express as px
import plotly.graph_objects as go # Add plotly graph objects
import os
from io import BytesIO
import requests # Needed for getThumbUrl download
import traceback  # Add missing traceback import
from streamlit_folium import st_folium  # Add missing st_folium import
import base64
import google.generativeai as genai # Gemini API
#TODO: Add 'import pyproj' here after installing it
import pyproj

def fix_farm_name_display(farm_name):
    """Fixes the display order of farm names like XX-YY to maintain original order."""
    if isinstance(farm_name, str) and '-' in farm_name:
        try:
            # Split the farm name and preserve the order
            parts = farm_name.split('-')
            if len(parts) == 2 and all(part.strip().isdigit() for part in parts):
                # Keep original order by using Unicode control characters
                return f"{parts[0]}-{parts[1]}"
        except:
            pass
    return farm_name

# --- Custom CSS ---
st.set_page_config(
    page_title="Ø³Ø§Ù…Ø§Ù†Ù‡ Ù¾Ø§ÛŒØ´ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù†ÛŒØ´Ú©Ø±",
    page_icon="ğŸŒ¾",
    layout="wide"
)

# Custom CSS for Persian text alignment, professional styling, and animations
st.markdown("""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Vazirmatn:wght@400;700&display=swap');
        
        /* Main container with animation */
        .main {
            font-family: 'Vazirmatn', sans-serif;
            animation: fadeIn 1s ease-in;
        }
        
        /* Animated sugarcane background */
        .stApp::before {
            content: '';
            position: fixed;
            top: 0;
            left: 0;
            right: 0;
            bottom: 0;
            background: linear-gradient(rgba(255,255,255,0.95), rgba(255,255,255,0.95)),
                        url('data:image/svg+xml;utf8,<svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 100 100"><path d="M30,90 Q50,20 70,90" stroke="green" fill="none" stroke-width="2"><animate attributeName="d" dur="3s" repeatCount="indefinite" values="M30,90 Q50,20 70,90;M30,90 Q50,30 70,90;M30,90 Q50,20 70,90"/></path></svg>');
            background-size: 100px 100px;
            opacity: 0.1;
            z-index: -1;
            animation: sway 3s ease-in-out infinite;
        }
        
        /* Animations */
        @keyframes fadeIn {
            from { opacity: 0; transform: translateY(20px); }
            to { opacity: 1; transform: translateY(0); }
        }
        
        @keyframes sway {
            0% { background-position: 0 0; }
            50% { background-position: -50px 0; }
            100% { background-position: 0 0; }
        }
        
        /* Cards with hover effect */
        .element-container {
            background: rgba(255,255,255,0.8);
            border-radius: 10px;
            padding: 1rem;
            margin: 0.5rem 0;
            transition: all 0.3s ease;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        
        .element-container:hover {
            transform: translateY(-2px);
            box-shadow: 0 4px 10px rgba(0,0,0,0.15);
        }
        
        /* Headers with animation */
        h1, h2, h3 {
            font-family: 'Vazirmatn', sans-serif;
            color: #2c3e50;
            text-align: right;
            position: relative;
            overflow: hidden;
        }
        
        h1::after, h2::after, h3::after {
            content: '';
            position: absolute;
            bottom: 0;
            right: 0;
            width: 100%;
            height: 2px;
            background: linear-gradient(to left, #2ecc71, transparent);
            animation: slideIn 1s ease-out;
        }
        
        @keyframes slideIn {
            from { transform: translateX(100%); }
            to { transform: translateX(0); }
        }
        
        /* Metrics with animation */
        .css-1xarl3l {
            font-family: 'Vazirmatn', sans-serif;
            background-color: #f8f9fa;
            border-radius: 10px;
            padding: 1rem;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
            transition: all 0.3s ease;
            animation: scaleIn 0.5s ease-out;
        }
        
        @keyframes scaleIn {
            from { transform: scale(0.95); opacity: 0; }
            to { transform: scale(1); opacity: 1; }
        }
        
        /* Other existing styles... */
        
        /* Fix for farm name display */
        .farm-name {
            unicode-bidi: plaintext;
            text-align: right;
        }
        
    </style>
""", unsafe_allow_html=True)

# --- Configuration ---
APP_TITLE = "Ø³Ø§Ù…Ø§Ù†Ù‡ Ù¾Ø§ÛŒØ´ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù†ÛŒØ´Ú©Ø±"
APP_SUBTITLE = "Ù…Ø·Ø§Ù„Ø¹Ø§Øª Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ Ø´Ø±Ú©Øª Ú©Ø´Øª Ùˆ ØµÙ†Ø¹Øª Ø¯Ù‡Ø®Ø¯Ø§"
INITIAL_LAT = 31.534442
INITIAL_LON = 48.724416
INITIAL_ZOOM = 12

# --- File Paths (Relative to the script location in Hugging Face) ---
# GEOJSON_FILE_PATH = 'farm_geodata_ready.geojson' # Old GeoJSON path
CSV_FILE_PATH = 'farm_geodata_ready (1).csv' # Use the new CSV file
SERVICE_ACCOUNT_FILE = 'ee-esmaeilkiani13877-cfdea6eaf411 (4).json'

# --- GEE Authentication ---
@st.cache_resource # Cache the GEE initialization
def initialize_gee():
    """Initializes Google Earth Engine using the Service Account."""
    try:
        if not os.path.exists(SERVICE_ACCOUNT_FILE):
            st.error(f"Ø®Ø·Ø§: ÙØ§ÛŒÙ„ Service Account Ø¯Ø± Ù…Ø³ÛŒØ± '{SERVICE_ACCOUNT_FILE}' ÛŒØ§ÙØª Ù†Ø´Ø¯.")
            st.stop()
            
        # Try to read and validate the service account file
        try:
            with open(SERVICE_ACCOUNT_FILE, 'r') as f:
                creds_json = json.load(f)
                required_keys = ['type', 'project_id', 'private_key', 'client_email']
                if not all(key in creds_json for key in required_keys):
                    st.error("Ø®Ø·Ø§: ÙØ§ÛŒÙ„ Service Account Ù†Ø§Ù‚Øµ Ø§Ø³Øª. Ù„Ø·ÙØ§Ù‹ Ø§Ø² ØµØ­Øª Ù…Ø­ØªÙˆÛŒØ§Øª ÙØ§ÛŒÙ„ Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø­Ø§ØµÙ„ Ú©Ù†ÛŒØ¯.")
                    st.stop()
                st.info(f"Service Account Email: {creds_json['client_email']}")
        except json.JSONDecodeError:
            st.error("Ø®Ø·Ø§: ÙØ§ÛŒÙ„ Service Account Ù‚Ø§Ø¨Ù„ Ø®ÙˆØ§Ù†Ø¯Ù† Ù†ÛŒØ³Øª. Ù„Ø·ÙØ§Ù‹ Ø§Ø² ØµØ­Øª ÙØ±Ù…Øª JSON Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø­Ø§ØµÙ„ Ú©Ù†ÛŒØ¯.")
            st.stop()
            
        # Initialize with more detailed error handling
        try:
            credentials = ee.ServiceAccountCredentials(None, key_file=SERVICE_ACCOUNT_FILE)
            ee.Initialize(credentials=credentials, opt_url='https://earthengine-highvolume.googleapis.com')
            
            # Test the connection by making a simple API call
            ee.Number(1).getInfo()
            
            print("GEE Initialized Successfully using Service Account.")
            st.success("âœ… Ø§ØªØµØ§Ù„ Ø¨Ù‡ Google Earth Engine Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø±Ù‚Ø±Ø§Ø± Ø´Ø¯.")
            return True
            
        except ee.EEException as e:
            error_msg = str(e)
            if "permission" in error_msg.lower() or "forbidden" in error_msg.lower():
                st.error("""
                Ø®Ø·Ø§ÛŒ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Google Earth Engine. Ù„Ø·ÙØ§Ù‹ Ù…ÙˆØ§Ø±Ø¯ Ø²ÛŒØ± Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯:
                1. Ø¢ÛŒØ§ Service Account Ø¯Ø± Earth Engine Ø«Ø¨Øª Ø´Ø¯Ù‡ Ø§Ø³ØªØŸ (https://code.earthengine.google.com/register)
                2. Ø¢ÛŒØ§ API Ù‡Ø§ÛŒ Ù„Ø§Ø²Ù… Ø¯Ø± Google Cloud Console ÙØ¹Ø§Ù„ Ù‡Ø³ØªÙ†Ø¯ØŸ
                3. Ø¢ÛŒØ§ Service Account Ø¯Ø³ØªØ±Ø³ÛŒâ€ŒÙ‡Ø§ÛŒ Ù„Ø§Ø²Ù… Ø±Ø§ Ø¯Ø§Ø±Ø¯ØŸ
                
                Ø®Ø·Ø§ÛŒ Ø§ØµÙ„ÛŒ: {}
                """.format(error_msg))
            else:
                st.error(f"Ø®Ø·Ø§ÛŒ Google Earth Engine: {error_msg}")
            st.stop()
            
    except Exception as e:
        st.error(f"Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡ Ù‡Ù†Ú¯Ø§Ù… Ø§ØªØµØ§Ù„ Ø¨Ù‡ GEE: {e}")
        st.error(traceback.format_exc())
        st.stop()


# --- Load Farm Data ---
@st.cache_data(show_spinner="Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø²Ø§Ø±Ø¹ (CSV Ø¨Ø§ Ú¯ÙˆØ´Ù‡â€ŒÙ‡Ø§)...")
def load_farm_data(csv_path=CSV_FILE_PATH):
    """Loads farm data from CSV including corner coordinates and creates ee.Geometry.Polygon."""
    try:
        # Read CSV, handle potential BOM in the first column name
        df = pd.read_csv(csv_path)
        # Clean column names (remove BOM if present)
        df.columns = df.columns.str.replace('\ufeff', '', regex=True)
        st.info(f"Columns from CSV: {list(df.columns)}")

        # --- Column Validation ---
        required_cols = [
            'Ù…Ø²Ø±Ø¹Ù‡', 'Ø³Ù†', 'ÙˆØ§Ø±ÛŒØªÙ‡', 'Ø±ÙˆØ²', 'Ú¯Ø±ÙˆÙ‡',
            'lat1', 'lon1', 'lat2', 'lon2',
            'lat3', 'lon3', 'lat4', 'lon4'
        ]
        if not all(col in df.columns for col in required_cols):
            missing_cols = [col for col in required_cols if col not in df.columns]
            st.error(f"âŒ ÙØ§ÛŒÙ„ CSV Ø¨Ø§ÛŒØ¯ Ø´Ø§Ù…Ù„ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¶Ø±ÙˆØ±ÛŒ Ø¨Ø§Ø´Ø¯. Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯Ù‡: {', '.join(missing_cols)}")
            st.stop()

        # --- Data Cleaning and Conversion ---
        coord_cols = ['lat1', 'lon1', 'lat2', 'lon2', 'lat3', 'lon3', 'lat4', 'lon4']
        # Replace Persian decimal separators and slashes with periods IN COORDINATE COLUMNS ONLY
        for col in coord_cols:
            if col in df.columns:
                # Ensure column is string type before replacing
                df[col] = df[col].astype(str)
                df[col] = df[col].str.replace(',', '.', regex=False) # Replace Persian comma (momayyez)
                df[col] = df[col].str.replace('/', '.', regex=False) # Replace slash
                # Add replacement for standard comma if necessary
                # df[col] = df[col].str.replace(',', '.', regex=False) 
                
        # Convert coordinate columns to numeric after replacement
        for col in coord_cols:
            if col in df.columns: # Check again in case a column was missing
                df[col] = pd.to_numeric(df[col], errors='coerce')

        # Drop rows with missing coordinates or essential identifiers
        initial_count = len(df)
        essential_check_cols = ['Ù…Ø²Ø±Ø¹Ù‡', 'Ø±ÙˆØ²'] + coord_cols
        df = df.dropna(subset=essential_check_cols)
        dropped_count = initial_count - len(df)
        if dropped_count > 0:
            st.warning(f"âš ï¸ {dropped_count} Ø±Ú©ÙˆØ±Ø¯ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ù…Ù‚Ø§Ø¯ÛŒØ± Ø®Ø§Ù„ÛŒ ÛŒØ§ Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø¯Ø± Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ 'Ù…Ø²Ø±Ø¹Ù‡', 'Ø±ÙˆØ²' ÛŒØ§ Ù…Ø®ØªØµØ§Øª Ø­Ø°Ù Ø´Ø¯Ù†Ø¯.")

        if df.empty:
            st.warning("âš ï¸ Ø¯Ø§Ø¯Ù‡ Ù…Ø¹ØªØ¨Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø²Ø§Ø±Ø¹ ÛŒØ§ÙØª Ù†Ø´Ø¯ (Ù¾Ø³ Ø§Ø² Ø­Ø°Ù Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ù†Ø§Ù…Ø¹ØªØ¨Ø±).")
            st.stop()

        # Ensure 'Ø±ÙˆØ²' and 'Ú¯Ø±ÙˆÙ‡' are strings and normalized
        df['Ø±ÙˆØ²'] = df['Ø±ÙˆØ²'].astype(str).str.replace(r'\s+', ' ', regex=True).str.strip()
        df['Ú¯Ø±ÙˆÙ‡'] = df['Ú¯Ø±ÙˆÙ‡'].astype(str).str.strip()
        # Convert other attributes if needed (e.g., Ø³Ù†, Ù…Ø³Ø§Ø­Øª if it existed)
        # df['Ø³Ù†'] = pd.to_numeric(df['Ø³Ù†'], errors='coerce') # Example

        # --- Coordinate System Conversion (UTM to Lat/Lon) ---
        # The coordinates in the CSV appear to be UTM (likely Zone 39N for this region).
        # Google Earth Engine requires geographic coordinates (Latitude/Longitude, WGS84).
        # We need to convert them using the pyproj library.
        # Make sure 'pyproj' is installed (pip install pyproj) and added to requirements.txt
        try:
            import pyproj
        except ImportError:
            st.error("âŒ Ú©ØªØ§Ø¨Ø®Ø§Ù†Ù‡ 'pyproj' Ø¨Ø±Ø§ÛŒ ØªØ¨Ø¯ÛŒÙ„ Ù…Ø®ØªØµØ§Øª UTM Ø¨Ù‡ Ø¬ØºØ±Ø§ÙÛŒØ§ÛŒÛŒ Ù…ÙˆØ±Ø¯ Ù†ÛŒØ§Ø² Ø§Ø³Øª.")
            st.error("Ù„Ø·ÙØ§Ù‹ Ø¢Ù† Ø±Ø§ Ù†ØµØ¨ Ú©Ù†ÛŒØ¯: pip install pyproj Ùˆ Ø¨Ù‡ requirements.txt Ø§Ø¶Ø§ÙÙ‡ Ú©Ù†ÛŒØ¯ Ùˆ Ø¨Ø±Ù†Ø§Ù…Ù‡ Ø±Ø§ Ù…Ø¬Ø¯Ø¯Ø§ Ø§Ø¬Ø±Ø§ Ú©Ù†ÛŒØ¯.")
            st.stop()

        # Define the UTM projection (Zone 39N, WGS84 datum) and the target geographic projection (WGS84)
        # Confirmed Khuzestan is typically Zone 39N.
        utm_proj = pyproj.Proj(proj='utm', zone=39, ellps='WGS84', south=False)
        wgs84_proj = pyproj.Proj(proj='latlong', datum='WGS84')
        transformer = pyproj.Transformer.from_proj(utm_proj, wgs84_proj, always_xy=True) # Ensure lon, lat output order

        # Convert corner coordinates
        try:
            # Apply the transformation - note the input order might be easting (lon-like), northing (lat-like) from UTM
            # pyproj expects x, y input, which corresponds to lon, lat for geographic but easting, northing for UTM
            df['lon1_geo'], df['lat1_geo'] = transformer.transform(df['lon1'].values, df['lat1'].values)
            df['lon2_geo'], df['lat2_geo'] = transformer.transform(df['lon2'].values, df['lat2'].values)
            df['lon3_geo'], df['lat3_geo'] = transformer.transform(df['lon3'].values, df['lat3'].values)
            df['lon4_geo'], df['lat4_geo'] = transformer.transform(df['lon4'].values, df['lat4'].values)
            st.info("Coordinates successfully converted from UTM to Geographic (Lat/Lon).")
        except Exception as proj_err:
            st.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªØ¨Ø¯ÛŒÙ„ Ù…Ø®ØªØµØ§Øª UTM Ø¨Ù‡ Ø¬ØºØ±Ø§ÙÛŒØ§ÛŒÛŒ: {proj_err}")
            st.error(traceback.format_exc())
            st.error("Ù„Ø·ÙØ§Ù‹ Ø§Ø² ØµØ­Øª ÙØ±Ù…Øª Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ù…Ø®ØªØµØ§Øª Ùˆ Ø§Ù†ØªØ®Ø§Ø¨ Ø²ÙˆÙ† UTM ØµØ­ÛŒØ­ (39N) Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø­Ø§ØµÙ„ Ú©Ù†ÛŒØ¯.")
            st.stop()
        # --- End Coordinate Conversion ---


        # --- Create ee.Geometry.Polygon for each farm ---
        def create_ee_polygon(row):
            try:
                # Coordinates must be in counter-clockwise order for GEE Polygons
                # Ensure the order [lon, lat] for GEE, using the CONVERTED coordinates
                coords = [
                    [row['lon1_geo'], row['lat1_geo']],
                    [row['lon2_geo'], row['lat2_geo']],
                    [row['lon3_geo'], row['lat3_geo']],
                    [row['lon4_geo'], row['lat4_geo']],
                    [row['lon1_geo'], row['lat1_geo']] # Close the loop
                ]

                # --- DEBUGGING: Print CONVERTED coordinates ---
                if row.name < 2: # Print for first 2 rows
                    # st.info(f"Row {row.name} - Original UTM Coords (lon1, lat1): {row['lon1']}, {row['lat1']}")
                    st.info(f"Row {row.name} - Converted Geo Coords for {row['Ù…Ø²Ø±Ø¹Ù‡']}: {coords}")
                    # coord_types = [(type(lon), type(lat)) for lon, lat in coords[:-1]]
                    # st.info(f"Row {row.name} - Converted Coord Types: {coord_types}")
                # --- END DEBUGGING ---

                # Basic check for valid CONVERTED coordinates (e.g., within expected range and numeric)
                # Use pd.isna explicitly to check for NaN after coercion
                is_valid = True
                for i, (lon, lat) in enumerate(coords[:-1]): # Check points 1 to 4
                    if pd.isna(lon) or pd.isna(lat) or not (-180 <= lon <= 180) or not (-90 <= lat <= 90):
                         if row.name < 5: # Log details for first few failures
                             st.warning(f"Invalid CONVERTED coordinate found in row {row.name} for {row['Ù…Ø²Ø±Ø¹Ù‡']} at point {i+1}: lon={lon}, lat={lat}")
                         is_valid = False
                         break # No need to check further points in this row

                if not is_valid:
                     st.warning(f"Skipping row {row.name} ({row['Ù…Ø²Ø±Ø¹Ù‡']}) due to invalid converted coordinates.")
                     return None # Return None if any coordinate is invalid

                return ee.Geometry.Polygon(coords)
            except Exception as e:
                # Log the error with more details for the first few rows
                if row.name < 5:
                    st.error(f"Error creating polygon for Ù…Ø²Ø±Ø¹Ù‡ {row['Ù…Ø²Ø±Ø¹Ù‡']} (Row {row.name}) using converted coords: {e}")
                    st.error(f"Converted Data: lon1_geo={row.get('lon1_geo', 'N/A')}, lat1_geo={row.get('lat1_geo', 'N/A')}, ...")
                    st.error(traceback.format_exc()) # Add traceback for detailed debugging
                # else: # Optionally log a generic warning for subsequent errors to avoid flooding the UI
                    # st.warning(f"Failed to create polygon for Ù…Ø²Ø±Ø¹Ù‡ {row['Ù…Ø²Ø±Ø¹Ù‡']}")
                return None

        df['ee_geometry'] = df.apply(create_ee_polygon, axis=1)

        # Drop rows where polygon creation failed
        initial_count_geom = len(df)
        df = df.dropna(subset=['ee_geometry']) # Ensure we have a valid geometry object
        dropped_geom_count = initial_count_geom - len(df)
        if dropped_geom_count > 0:
             st.warning(f"âš ï¸ {dropped_geom_count} Ø±Ú©ÙˆØ±Ø¯ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ø®Ø·Ø§ Ø¯Ø± Ø§ÛŒØ¬Ø§Ø¯ Ù‡Ù†Ø¯Ø³Ù‡ Ú†Ù†Ø¯Ø¶Ù„Ø¹ÛŒ Ø­Ø°Ù Ø´Ø¯Ù†Ø¯.")

        if df.empty:
            st.warning("âš ï¸ Ù‡ÛŒÚ† Ù…Ø²Ø±Ø¹Ù‡â€ŒØ§ÛŒ Ø¨Ø§ Ù‡Ù†Ø¯Ø³Ù‡ Ù…Ø¹ØªØ¨Ø± ÛŒØ§ÙØª Ù†Ø´Ø¯.")
            st.stop()

        st.success(f"âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ {len(df)} Ù…Ø²Ø±Ø¹Ù‡ (Ø¨Ø§ Ù‡Ù†Ø¯Ø³Ù‡ Ú†Ù†Ø¯Ø¶Ù„Ø¹ÛŒ Ø§Ø² CSV) Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯.")
        return df
    except FileNotFoundError:
        st.error(f"âŒ ÙØ§ÛŒÙ„ '{csv_path}' ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ ÙØ§ÛŒÙ„ CSV Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø²Ø§Ø±Ø¹ Ø±Ø§ Ø¯Ø± Ù…Ø³ÛŒØ± ØµØ­ÛŒØ­ Ù‚Ø±Ø§Ø± Ø¯Ù‡ÛŒØ¯.")
        st.stop()
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÛŒØ§ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ CSV: {e}")
        st.error(traceback.format_exc())
        st.stop()

# --- Load Analysis Data ---
@st.cache_data(show_spinner="Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ø§Øª...")
def load_analysis_data(csv_path='Ù…Ø­Ø§Ø³Ø¨Ø§Øª 2.csv'):
    """Loads and preprocesses data from the analysis CSV file."""
    try:
        # Read the raw lines to identify sections
        with open(csv_path, 'r', encoding='utf-8') as f:
            lines = f.readlines()

        # Find the headers and split points
        headers_indices = [i for i, line in enumerate(lines) if 'Ø§Ø¯Ø§Ø±Ù‡,Ø³Ù†,' in line or 'ØªÙˆÙ„ÛŒØ¯,Ø³Ù†,' in line]
        if len(headers_indices) < 2:
            # Fallback if only one section header is found (less robust)
            headers_indices = [i for i, line in enumerate(lines) if ',Ø³Ù†,' in line]
            if len(headers_indices) < 1:
                st.error(f"âŒ Ø³Ø§Ø®ØªØ§Ø± ÙØ§ÛŒÙ„ '{csv_path}' Ù‚Ø§Ø¨Ù„ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†ÛŒØ³Øª. Ù‡Ø¯Ø±Ù‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø± ÛŒØ§ÙØª Ù†Ø´Ø¯.")
                st.stop()
            st.warning("âš ï¸ ÙÙ‚Ø· ÛŒÚ© Ø¨Ø®Ø´ Ø¯Ø§Ø¯Ù‡ Ø¯Ø± ÙØ§ÛŒÙ„ Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯.")
            section1_start = headers_indices[0] + 1
            section2_start = None
            # Try to find a likely separator (e.g., a mostly blank line)
            blank_lines = [i for i, line in enumerate(lines[section1_start:]) if len(line.strip()) < 5]
            if blank_lines:
                section2_start = section1_start + blank_lines[0] + 1 # Heuristic guess
        else:
            section1_start = headers_indices[0] + 1
            section2_start = headers_indices[1] + 1 # Line after the second header

        # Read the first section (Area)
        df_area = pd.read_csv(csv_path, skiprows=headers_indices[0], nrows=(section2_start - section1_start - 2) if section2_start else None, encoding='utf-8')
        df_area.rename(columns={'Ø§Ø¯Ø§Ø±Ù‡': 'Ù…Ø³Ø§Ø­Øª_Ø§Ø¯Ø§Ø±Ù‡'}, inplace=True) # Rename first col for clarity
        df_area.rename(columns={df_area.columns[0]: 'Ø§Ø¯Ø§Ø±Ù‡'}, inplace=True) # The actual 'Ø§Ø¯Ø§Ø±Ù‡' column might be the first if unnamed

        # Read the second section (Production) if found
        df_prod = None
        if section2_start:
            # Skip rows until the second header, read until end or grand total
            end_row_prod = None
            for i in range(section2_start, len(lines)):
                if "Grand Total" in lines[i]:
                    end_row_prod = i
                    break
            nrows_prod = (end_row_prod - section2_start) if end_row_prod else None
            df_prod = pd.read_csv(csv_path, skiprows=section2_start-1, nrows=nrows_prod, encoding='utf-8') # Read including header
            # The first column name in the second section is actually 'ØªÙˆÙ„ÛŒØ¯', needs renaming
            df_prod.rename(columns={df_prod.columns[0]: 'Ø§Ø¯Ø§Ø±Ù‡'}, inplace=True)


        # --- Preprocessing Function ---
        def preprocess_df(df, section_name):
            if df is None:
                return None
            # Ensure 'Ø§Ø¯Ø§Ø±Ù‡' is the first column if it got misplaced
            if 'Ø§Ø¯Ø§Ø±Ù‡' not in df.columns and len(df.columns) > 0:
                 df.rename(columns={df.columns[0]: 'Ø§Ø¯Ø§Ø±Ù‡'}, inplace=True)

            # Check for required columns
            if not all(col in df.columns for col in ['Ø§Ø¯Ø§Ø±Ù‡', 'Ø³Ù†']):
                 st.warning(f"âš ï¸ Ø³ØªÙˆÙ† Ù‡Ø§ÛŒ Ø¶Ø±ÙˆØ±ÛŒ 'Ø§Ø¯Ø§Ø±Ù‡' ÛŒØ§ 'Ø³Ù†' Ø¯Ø± Ø¨Ø®Ø´ '{section_name}' ÛŒØ§ÙØª Ù†Ø´Ø¯.")
                 return None

            # Forward fill 'Ø§Ø¯Ø§Ø±Ù‡'
            df['Ø§Ø¯Ø§Ø±Ù‡'] = df['Ø§Ø¯Ø§Ø±Ù‡'].ffill()

            # Filter out 'total' and 'Grand Total' rows in 'Ø³Ù†' and 'Ø§Ø¯Ø§Ø±Ù‡'
            df = df[~df['Ø³Ù†'].astype(str).str.contains('total', case=False, na=False)]
            df = df[~df['Ø§Ø¯Ø§Ø±Ù‡'].astype(str).str.contains('total|Ø¯Ù‡Ø®Ø¯Ø§', case=False, na=False)] # Filter Grand Total/summary rows in Ø§Ø¯Ø§Ø±Ù‡

            # Remove rows where 'Ø§Ø¯Ø§Ø±Ù‡' is NaN after ffill (first rows before a number)
            df = df.dropna(subset=['Ø§Ø¯Ø§Ø±Ù‡'])

             # Convert 'Ø§Ø¯Ø§Ø±Ù‡' to integer where possible
            try:
                df['Ø§Ø¯Ø§Ø±Ù‡'] = pd.to_numeric(df['Ø§Ø¯Ø§Ø±Ù‡'], errors='coerce')
                df = df.dropna(subset=['Ø§Ø¯Ø§Ø±Ù‡']) # Drop if conversion failed
                df['Ø§Ø¯Ø§Ø±Ù‡'] = df['Ø§Ø¯Ø§Ø±Ù‡'].astype(int)
            except Exception:
                st.warning(f"âš ï¸ Ø§Ù…Ú©Ø§Ù† ØªØ¨Ø¯ÛŒÙ„ Ø³ØªÙˆÙ† 'Ø§Ø¯Ø§Ø±Ù‡' Ø¨Ù‡ Ø¹Ø¯Ø¯ ØµØ­ÛŒØ­ Ø¯Ø± Ø¨Ø®Ø´ '{section_name}' ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")
                # Keep as is if conversion fails

            # Convert numeric columns, coerce errors to NaN
            value_cols = [col for col in df.columns if col not in ['Ø§Ø¯Ø§Ø±Ù‡', 'Ø³Ù†', 'Ø¯Ø±ØµØ¯', 'Grand Total']]
            for col in value_cols:
                df[col] = pd.to_numeric(df[col], errors='coerce')

            # Drop Grand Total and Ø¯Ø±ØµØ¯ columns if they exist
            df = df.drop(columns=['Grand Total', 'Ø¯Ø±ØµØ¯'], errors='ignore')

            # Set multi-index for easier access
            if 'Ø§Ø¯Ø§Ø±Ù‡' in df.columns and 'Ø³Ù†' in df.columns:
                df = df.set_index(['Ø§Ø¯Ø§Ø±Ù‡', 'Ø³Ù†'])
            else:
                 st.warning(f"âš ï¸ Ø§Ù…Ú©Ø§Ù† ØªÙ†Ø¸ÛŒÙ… Ø§ÛŒÙ†Ø¯Ú©Ø³ Ú†Ù†Ø¯Ú¯Ø§Ù†Ù‡ Ø¯Ø± Ø¨Ø®Ø´ '{section_name}' ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.")


            return df

        df_area_processed = preprocess_df(df_area, "Ù…Ø³Ø§Ø­Øª")
        df_prod_processed = preprocess_df(df_prod, "ØªÙˆÙ„ÛŒØ¯")


        st.success(f"âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯.")
        return df_area_processed, df_prod_processed

    except FileNotFoundError:
        st.error(f"âŒ ÙØ§ÛŒÙ„ '{csv_path}' ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ ÙØ§ÛŒÙ„ CSV Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ø±Ø§ Ø¯Ø± Ù…Ø³ÛŒØ± ØµØ­ÛŒØ­ Ù‚Ø±Ø§Ø± Ø¯Ù‡ÛŒØ¯.")
        return None, None
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÛŒØ§ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ Ù…Ø­Ø§Ø³Ø¨Ø§Øª CSV: {e}")
        st.error(traceback.format_exc()) # Print detailed error
        return None, None


# Initialize GEE and Load Data
if initialize_gee():
    farm_data_df = load_farm_data() # Now returns DataFrame with ee_geometry column

# Load Analysis Data
analysis_area_df, analysis_prod_df = load_analysis_data()

# ==============================================================================
# Sidebar Filters
# ==============================================================================
st.sidebar.header("ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù†Ù…Ø§ÛŒØ´")

# --- Day of the Week Selection ---
available_days = sorted(farm_data_df['Ø±ÙˆØ²'].unique()) # Use df
selected_day = st.sidebar.selectbox(
    "ğŸ“… Ø±ÙˆØ² Ù‡ÙØªÙ‡ Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯:",
    options=available_days,
    index=0, # Default to the first day
    help="Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø²Ø§Ø±Ø¹ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§ÛŒÙ† Ø±ÙˆØ² ÙÛŒÙ„ØªØ± Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯."
)

# --- Filter Data Based on Selected Day ---
filtered_farms_df = farm_data_df[farm_data_df['Ø±ÙˆØ²'] == selected_day].copy() # Use df

if filtered_farms_df.empty:
    st.warning(f"âš ï¸ Ù‡ÛŒÚ† Ù…Ø²Ø±Ø¹Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø±ÙˆØ² '{selected_day}' ÛŒØ§ÙØª Ù†Ø´Ø¯.")
    st.stop()

# --- Farm Selection ---
available_farms = sorted(filtered_farms_df['Ù…Ø²Ø±Ø¹Ù‡'].unique()) # Use df
# Add an option for "All Farms"
farm_options = ["Ù‡Ù…Ù‡ Ù…Ø²Ø§Ø±Ø¹"] + [fix_farm_name_display(farm) for farm in available_farms]
selected_farm_name = st.sidebar.selectbox(
    "ğŸŒ¾ Ù…Ø²Ø±Ø¹Ù‡ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯:",
    options=farm_options,
    index=0,
    help="Ù…Ø²Ø±Ø¹Ù‡â€ŒØ§ÛŒ Ú©Ù‡ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒØ¯ Ø¬Ø²Ø¦ÛŒØ§Øª Ø¢Ù† Ø±Ø§ Ø¨Ø¨ÛŒÙ†ÛŒØ¯ ÛŒØ§ 'Ù‡Ù…Ù‡ Ù…Ø²Ø§Ø±Ø¹' Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ú©Ù„ÛŒ."
)

# --- Index Selection ---
index_options = {
    "NDVI": "Ø´Ø§Ø®Øµ Ù¾ÙˆØ´Ø´ Ú¯ÛŒØ§Ù‡ÛŒ ØªÙØ§Ø¶Ù„ÛŒ Ù†Ø±Ù…Ø§Ù„ Ø´Ø¯Ù‡",
    "EVI": "Ø´Ø§Ø®Øµ Ù¾ÙˆØ´Ø´ Ú¯ÛŒØ§Ù‡ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡",
    "NDMI": "Ø´Ø§Ø®Øµ Ø±Ø·ÙˆØ¨Øª ØªÙØ§Ø¶Ù„ÛŒ Ù†Ø±Ù…Ø§Ù„ Ø´Ø¯Ù‡ (ÙˆØ¶Ø¹ÛŒØª Ø¢Ø¨ÛŒ)",
    "LAI": "Ø´Ø§Ø®Øµ Ø³Ø·Ø­ Ø¨Ø±Ú¯ (ØªØ®Ù…ÛŒÙ†ÛŒ)",
    "MSI": "Ø´Ø§Ø®Øµ ØªÙ†Ø´ Ø±Ø·ÙˆØ¨ØªÛŒ",
    "CVI": "Ø´Ø§Ø®Øµ Ú©Ù„Ø±ÙˆÙÛŒÙ„ (ØªØ®Ù…ÛŒÙ†ÛŒ)",
    "NI": "Ø´Ø§Ø®Øµ Ù†ÛŒØªØ±ÙˆÚ˜Ù† (ØªØ®Ù…ÛŒÙ†ÛŒ)",
    # Add more indices if needed and implemented
    # "Biomass": "Ø²ÛŒØ³Øªâ€ŒØªÙˆØ¯Ù‡ (ØªØ®Ù…ÛŒÙ†ÛŒ)",
    # "ET": "ØªØ¨Ø®ÛŒØ± Ùˆ ØªØ¹Ø±Ù‚ (ØªØ®Ù…ÛŒÙ†ÛŒ)",
}
selected_index = st.sidebar.selectbox(
    "ğŸ“ˆ Ø´Ø§Ø®Øµ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø±ÙˆÛŒ Ù†Ù‚Ø´Ù‡:",
    options=list(index_options.keys()),
    format_func=lambda x: f"{x} ({index_options[x]})",
    index=0 # Default to NDVI
)

# --- Date Range Calculation ---
today = datetime.date.today()
# Find the most recent date corresponding to the selected day of the week
# Map Persian day names to Python's weekday() (Monday=0, Sunday=6) - Adjust if needed
persian_to_weekday = {
    "Ø´Ù†Ø¨Ù‡": 5,
    "ÛŒÚ©Ø´Ù†Ø¨Ù‡": 6,
    "Ø¯ÙˆØ´Ù†Ø¨Ù‡": 0,
    "Ø³Ù‡ Ø´Ù†Ø¨Ù‡": 1, # Handle potential space variations (normalized in loading)
    "Ú†Ù‡Ø§Ø±Ø´Ù†Ø¨Ù‡": 2,
    "Ù¾Ù†Ø¬Ø´Ù†Ø¨Ù‡": 3,
    "Ø¬Ù…Ø¹Ù‡": 4,
}
try:
    target_weekday = persian_to_weekday[selected_day]
    days_ago = (today.weekday() - target_weekday + 7) % 7
    if days_ago == 0: # If today is the selected day, use today
         end_date_current = today
    else:
         end_date_current = today - datetime.timedelta(days=days_ago)

    start_date_current = end_date_current - datetime.timedelta(days=6)
    end_date_previous = start_date_current - datetime.timedelta(days=1)
    start_date_previous = end_date_previous - datetime.timedelta(days=6)

    # Convert to strings for GEE
    start_date_current_str = start_date_current.strftime('%Y-%m-%d')
    end_date_current_str = end_date_current.strftime('%Y-%m-%d')
    start_date_previous_str = start_date_previous.strftime('%Y-%m-%d')
    end_date_previous_str = end_date_previous.strftime('%Y-%m-%d')

    st.sidebar.info(f"Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ ÙØ¹Ù„ÛŒ: {start_date_current_str} ØªØ§ {end_date_current_str}")
    st.sidebar.info(f"Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ù‚Ø¨Ù„ÛŒ: {start_date_previous_str} ØªØ§ {end_date_previous_str}")

except KeyError:
    st.sidebar.error(f"Ù†Ø§Ù… Ø±ÙˆØ² Ù‡ÙØªÙ‡ '{selected_day}' Ù‚Ø§Ø¨Ù„ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†ÛŒØ³Øª.")
    st.stop()
except Exception as e:
    st.sidebar.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ: {e}")
    st.stop()


# ==============================================================================
# Google Earth Engine Functions
# ==============================================================================

# --- Cloud Masking Function for Sentinel-2 ---
def maskS2clouds(image):
    """Masks clouds in a Sentinel-2 SR image using the QA band."""
    qa = image.select('QA60')
    # Bits 10 and 11 are clouds and cirrus, respectively.
    cloudBitMask = 1 << 10
    cirrusBitMask = 1 << 11
    # Both flags should be set to zero, indicating clear conditions.
    mask = qa.bitwiseAnd(cloudBitMask).eq(0).And(
             qa.bitwiseAnd(cirrusBitMask).eq(0))
    # Also mask based on SCL band if available (more robust)
    scl = image.select('SCL')
    # Keep 'Vegetation', 'Not Vegetated', 'Water', 'Snow/Ice', 'Bare Soil'
    # Mask out 'Cloud Medium Probability', 'Cloud High Probability', 'Cirrus', 'Cloud Shadow'
    good_quality = scl.remap([4, 5, 6, 7, 11], [1, 1, 1, 1, 1], 0) # Map good classes to 1, others to 0

    # Scale and offset factors for Sentinel-2 SR bands
    opticalBands = image.select('B.*').multiply(0.0001)
    
    # Remove thermal band processing as it's not available in the dataset
    # thermalBands = image.select('ST_B.*').multiply(0.00341802).add(149.0) # If using thermal

    return image.addBands(opticalBands, None, True)\
                .updateMask(mask).updateMask(good_quality) # Apply both masks


# --- Index Calculation Functions ---
def add_indices(image):
    """Calculates and adds various indices as bands to an image."""
    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')
    evi = image.expression(
        '2.5 * (NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1)', {
            'NIR': image.select('B8'),
            'RED': image.select('B4'),
            'BLUE': image.select('B2')
        }).rename('EVI')
    ndmi = image.normalizedDifference(['B8', 'B11']).rename('NDMI')
    msi = image.expression('SWIR1 / NIR', {
        'SWIR1': image.select('B11'),
        'NIR': image.select('B8')
    }).rename('MSI')
    lai = ndvi.multiply(3.5).rename('LAI')
    green_safe = image.select('B3').max(ee.Image(0.0001))
    cvi = image.expression('(NIR / GREEN) * (RED / GREEN)', {
        'NIR': image.select('B8'),
        'GREEN': green_safe,
        'RED': image.select('B4')
    }).rename('CVI')
    ni = image.expression(
        '((RE3 - RE1) / (RE3 + RE1)) * ((RE2) / (NIR))', {
            'RE1': image.select('B5'),
            'RE2': image.select('B6'),
            'RE3': image.select('B7'),
            'NIR': image.select('B8')
        }).rename('NI')
    return image.addBands([ndvi, evi, ndmi, msi, lai, cvi, ni])

# --- Function to get processed image for a date range and geometry ---
@st.cache_data(show_spinner="Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØµØ§ÙˆÛŒØ± Ù…Ø§Ù‡ÙˆØ§Ø±Ù‡â€ŒØ§ÛŒ...", persist=True)
def get_processed_image(_geometry, start_date, end_date, index_name):
    """
    Gets cloud-masked, index-calculated Sentinel-2 median composite for a given geometry and date range.
    _geometry: ee.Geometry (Polygon, MultiPolygon, Point, Rectangle)
    start_date, end_date: YYYY-MM-DD strings
    index_name: Name of the primary index band to return (e.g., 'NDVI')
    """
    try:
        s2_sr_col = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')
                     .filterBounds(_geometry) # Works with polygons too
                     .filterDate(start_date, end_date)
                     .map(maskS2clouds)) # Apply cloud masking

        # Check if any images are available after filtering
        count = s2_sr_col.size().getInfo()
        if count == 0:
            # st.warning(f"Ù‡ÛŒÚ† ØªØµÙˆÛŒØ± Sentinel-2 Ø¨Ø¯ÙˆÙ† Ø§Ø¨Ø± Ø¯Ø± Ø¨Ø§Ø²Ù‡ {start_date} ØªØ§ {end_date} ÛŒØ§ÙØª Ù†Ø´Ø¯.")
            return None, f"No cloud-free Sentinel-2 images found for {start_date} to {end_date}."

        # Calculate indices for each image in the collection
        indexed_col = s2_sr_col.map(add_indices)

        # Create a median composite image
        median_image = indexed_col.median() # Use median to reduce noise/outliers

        # Select the desired index band
        output_image = median_image.select(index_name)

        return output_image, None # Return the image and no error message
    except ee.EEException as e:
        # Handle GEE specific errors
        error_message = f"Ø®Ø·Ø§ÛŒ Google Earth Engine: {e}"
        st.error(error_message)
        # Try to extract more details if available
        try:
            # GEE errors sometimes have details nested
            error_details = e.args[0] if e.args else str(e)
            if isinstance(error_details, str) and 'computation timed out' in error_details.lower():
                 error_message += "\n(Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ø­Ø¬Ù… Ø¨Ø§Ù„Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÛŒØ§ Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ø·ÙˆÙ„Ø§Ù†ÛŒ)"
            elif isinstance(error_details, str) and 'user memory limit exceeded' in error_details.lower():
                 error_message += "\n(Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ù†Ø·Ù‚Ù‡ Ø¨Ø²Ø±Ú¯ ÛŒØ§ Ø¹Ù…Ù„ÛŒØ§Øª Ù¾ÛŒÚ†ÛŒØ¯Ù‡)"
        except Exception:
            pass # Ignore errors during error detail extraction
        return None, error_message
    except Exception as e:
        error_message = f"Ø®Ø·Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ GEE: {e}\n{traceback.format_exc()}"
        st.error(error_message)
        return None, error_message

# --- Function to get time series data for a point ---
@st.cache_data(show_spinner="Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø±ÛŒØ§ÙØª Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ Ø´Ø§Ø®Øµ...", persist=True)
def get_index_time_series(_geometry, index_name, start_date='2023-01-01', end_date=today.strftime('%Y-%m-%d')):
    """
    Gets a time series of a specified index for a geometry (calculates mean over the area).
    _geometry: ee.Geometry (Polygon, MultiPolygon, Point)
    """
    try:
        s2_sr_col = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')
                     .filterBounds(_geometry)
                     .filterDate(start_date, end_date)
                     .map(maskS2clouds)
                     .map(add_indices))

        def extract_value(image):
            # Extract the mean index value over the geometry
            # Use reduceRegion for polygons; scale should match sensor resolution (e.g., 10m for S2 NDVI)
            value = image.reduceRegion(
                reducer=ee.Reducer.mean(), # Use mean for polygon average
                geometry=_geometry,
                scale=10, # Scale in meters (10m for Sentinel-2 RGB/NIR)
                maxPixels=1e9 # Increase maxPixels for potentially larger geometries
            ).get(index_name)
            # Return a feature with the value and the image date
            return ee.Feature(None, {
                'date': image.date().format('YYYY-MM-dd'),
                index_name: value
            })

        # Map over the collection and remove features with null values
        ts_features = s2_sr_col.map(extract_value).filter(ee.Filter.notNull([index_name]))

        # Convert the FeatureCollection to a list of dictionaries
        ts_info = ts_features.getInfo()['features']

        if not ts_info:
            return pd.DataFrame(columns=['date', index_name]), "Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯."

        # Convert to Pandas DataFrame
        ts_data = [{'date': f['properties']['date'], index_name: f['properties'][index_name]} for f in ts_info]
        ts_df = pd.DataFrame(ts_data)
        ts_df['date'] = pd.to_datetime(ts_df['date'])
        ts_df = ts_df.sort_values('date').set_index('date')

        return ts_df, None # Return DataFrame and no error
    except ee.EEException as e:
        error_message = f"Ø®Ø·Ø§ÛŒ GEE Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ: {e}"
        st.error(error_message)
        return pd.DataFrame(columns=['date', index_name]), error_message
    except Exception as e:
        error_message = f"Ø®Ø·Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ: {e}\n{traceback.format_exc()}"
        st.error(error_message)
        return pd.DataFrame(columns=['date', index_name]), error_message


# ==============================================================================
# NEW: Function to get all relevant indices for a farm point for two periods
# ==============================================================================
@st.cache_data(show_spinner="Ø¯Ø± Ø­Ø§Ù„ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ÛŒ Ù†ÛŒØ§Ø²Ø³Ù†Ø¬ÛŒ...", persist=True)
def get_farm_needs_data(_geometry, start_curr, end_curr, start_prev, end_prev):
    """
    Calculates mean NDVI, NDMI, EVI for current and previous periods over a geometry.
     _geometry: ee.Geometry (Polygon, MultiPolygon, Point)
     """
    results = {
        'NDVI_curr': None, 'NDMI_curr': None, 'EVI_curr': None,
        'NDVI_prev': None, 'NDMI_prev': None, 'EVI_prev': None,
        'error': None
    }
    # Remove SAVI from the list of indices to get
    indices_to_get = ['NDVI', 'NDMI', 'EVI']

    def get_mean_values_for_period(start, end):
        period_values = {index: None for index in indices_to_get}
        error_msg = None
        try:
            # Get median composite image with all indices calculated
            s2_sr_col = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')
                         .filterBounds(_geometry) # Use polygon
                         .filterDate(start, end)
                         .map(maskS2clouds)
                         .map(add_indices)) # add_indices already updated to exclude SAVI

            count = s2_sr_col.size().getInfo()
            if count == 0:
                return period_values, f"Ù‡ÛŒÚ† ØªØµÙˆÛŒØ±ÛŒ Ø¯Ø± Ø¨Ø§Ø²Ù‡ {start}-{end} ÛŒØ§ÙØª Ù†Ø´Ø¯"

            median_image = s2_sr_col.median()

            # Reduce region to get the mean value at the point for all required indices
            mean_dict = median_image.select(indices_to_get).reduceRegion(
                reducer=ee.Reducer.mean(), # Use mean for polygon average
                geometry=_geometry, # Use polygon
                scale=10,  # Scale in meters
                maxPixels=1e9 # Increase maxPixels
            ).getInfo()

            if mean_dict:
                for index in indices_to_get:
                    period_values[index] = mean_dict.get(index)
            return period_values, None
        except ee.EEException as e:
            error_msg = f"Ø®Ø·Ø§ÛŒ GEE Ø¯Ø± Ø¨Ø§Ø²Ù‡ {start}-{end}: {e}"
            return period_values, error_msg
        except Exception as e:
            error_msg = f"Ø®Ø·Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡ Ø¯Ø± Ø¨Ø§Ø²Ù‡ {start}-{end}: {e}"
            return period_values, error_msg

    # Get data for current period
    curr_values, err_curr = get_mean_values_for_period(start_curr, end_curr)
    if err_curr:
        results['error'] = err_curr
    else:
        results['NDVI_curr'] = curr_values['NDVI']
        results['NDMI_curr'] = curr_values['NDMI']
        results['EVI_curr'] = curr_values['EVI']
        # Remove SAVI from results assignment
        # results['SAVI_curr'] = curr_values['SAVI']

    # Get data for previous period
    prev_values, err_prev = get_mean_values_for_period(start_prev, end_prev)
    if err_prev:
        results['error'] = f"{results.get('error', '')} | {err_prev}" # Append errors
    else:
        results['NDVI_prev'] = prev_values['NDVI']
        results['NDMI_prev'] = prev_values['NDMI']
        results['EVI_prev'] = prev_values['EVI']
        # Remove SAVI from results assignment
        # results['SAVI_prev'] = prev_values['SAVI']

    return results

# ==============================================================================
# NEW: Gemini AI Helper Functions
# ==============================================================================

# Configure Gemini API
@st.cache_resource
def configure_gemini():
    """Configures the Gemini API client using a hardcoded API key (NOT RECOMMENDED)."""
    try:
        # --- WARNING: Hardcoding API keys is insecure! Use Streamlit secrets instead. ---
        api_key = "AIzaSyC6ntMs3XDa3JTk07-6_BRRCduiQaRmQFQ" # <-- HARDCODED API KEY
        # ---------------------------------------------------------------------------

        if not api_key:
             st.error("âŒ Ú©Ù„ÛŒØ¯ API Ø¬Ù…ÛŒÙ†Ø§ÛŒ Ø¨Ù‡ ØµÙˆØ±Øª Ù…Ø³ØªÙ‚ÛŒÙ… Ø¯Ø± Ú©Ø¯ ÙˆØ§Ø±Ø¯ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª.")
             return None

        genai.configure(api_key=api_key)
        # Optional: Add safety settings configuration here if needed
        # safety_settings = [...]
        # model = genai.GenerativeModel('gemini-pro', safety_settings=safety_settings)
        model = genai.GenerativeModel('gemini-1.5-flash') # Use the latest flash model
        print("Gemini Configured Successfully (using hardcoded key).")
        return model
    # except KeyError: # No longer reading from secrets
    #     st.error("âŒ Ú©Ù„ÛŒØ¯ API Ø¬Ù…ÛŒÙ†Ø§ÛŒ (GEMINI_API_KEY) Ø¯Ø± ÙØ§ÛŒÙ„ secrets.toml ÛŒØ§ÙØª Ù†Ø´Ø¯.")
    #     st.info("Ù„Ø·ÙØ§Ù‹ ÙØ§ÛŒÙ„ .streamlit/secrets.toml Ø±Ø§ Ø§ÛŒØ¬Ø§Ø¯ Ú©Ø±Ø¯Ù‡ Ùˆ Ú©Ù„ÛŒØ¯ Ø®ÙˆØ¯ Ø±Ø§ Ø¯Ø± Ø¢Ù† Ù‚Ø±Ø§Ø± Ø¯Ù‡ÛŒØ¯.")
    #     return None
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªÙ†Ø¸ÛŒÙ… Gemini API: {e}")
        return None

# Function to get AI analysis
@st.cache_data(show_spinner="Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø±ÛŒØ§ÙØª ØªØ­Ù„ÛŒÙ„ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ...", persist=True)
def get_ai_analysis(_model, farm_name, index_data, recommendations):
    """Generates AI analysis for the farm's condition."""
    if _model is None:
        return "Ø³Ø±ÙˆÛŒØ³ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª."

    # Prepare data string
    data_str = ""
    if index_data['NDVI_curr'] is not None: data_str += f"NDVI ÙØ¹Ù„ÛŒ: {index_data['NDVI_curr']:.3f} (Ù‚Ø¨Ù„ÛŒ: {index_data.get('NDVI_prev', 'N/A'):.3f})\n"
    if index_data['NDMI_curr'] is not None: data_str += f"NDMI ÙØ¹Ù„ÛŒ: {index_data['NDMI_curr']:.3f} (Ù‚Ø¨Ù„ÛŒ: {index_data.get('NDMI_prev', 'N/A'):.3f})\n"
    if index_data['EVI_curr'] is not None: data_str += f"EVI ÙØ¹Ù„ÛŒ: {index_data['EVI_curr']:.3f} (Ù‚Ø¨Ù„ÛŒ: {index_data.get('EVI_prev', 'N/A'):.3f})\n"
    if index_data['SAVI_curr'] is not None: data_str += f"SAVI ÙØ¹Ù„ÛŒ: {index_data['SAVI_curr']:.3f} (Ù‚Ø¨Ù„ÛŒ: {index_data.get('SAVI_prev', 'N/A'):.3f})\n"

    prompt = f"""
    Ø´Ù…Ø§ ÛŒÚ© Ù…ØªØ®ØµØµ Ú©Ø´Ø§ÙˆØ±Ø²ÛŒ Ù†ÛŒØ´Ú©Ø± Ù‡Ø³ØªÛŒØ¯. Ù„Ø·ÙØ§Ù‹ ÙˆØ¶Ø¹ÛŒØª Ù…Ø²Ø±Ø¹Ù‡ '{farm_name}' Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ø§Ø®Øµ Ùˆ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø²ÛŒØ± ØªØ­Ù„ÛŒÙ„ Ú©Ù†ÛŒØ¯ Ùˆ ÛŒÚ© ØªÙˆØ¶ÛŒØ­ Ú©ÙˆØªØ§Ù‡ Ùˆ Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ Ø¨Ù‡ Ø²Ø¨Ø§Ù† ÙØ§Ø±Ø³ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯. ØªÙ…Ø±Ú©Ø² ØªØ­Ù„ÛŒÙ„ Ø¨Ø± Ù†ÛŒØ§Ø² Ø¢Ø¨ÛŒØ§Ø±ÛŒ Ùˆ Ú©ÙˆØ¯ÛŒ Ø¨Ø§Ø´Ø¯.

    Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ø§Ø®Øµ:
    {data_str}
    ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ‡:
    {', '.join(recommendations) if recommendations else 'Ù‡ÛŒÚ† ØªÙˆØµÛŒÙ‡â€ŒØ§ÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.'}

    ØªØ­Ù„ÛŒÙ„ Ø´Ù…Ø§:
    """

    try:
        response = _model.generate_content(prompt)
        # Accessing response text might differ slightly based on exact library version
        # Check response object structure if needed
        return response.text
    except Exception as e:
        st.warning(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Gemini API: {e}")
        return "Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª ØªØ­Ù„ÛŒÙ„ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ."



# ==============================================================================
# Main Application Layout (Using Tabs)
# ==============================================================================

# Configure Gemini Model at the start
gemini_model = configure_gemini()

tab1, tab3 = st.tabs(["ğŸ“Š Ù¾Ø§ÛŒØ´ Ù…Ø²Ø§Ø±Ø¹", "ğŸ’§Ú©ÙˆØ¯ Ùˆ Ø¢Ø¨ÛŒØ§Ø±ÛŒ"])

with tab1:
    # ==============================================================================
    # Main Panel Display
    # ==============================================================================

    # --- Get Selected Farm Geometry and Details ---
    selected_farm_details = None
    selected_farm_geom_ee = None # GEE geometry object
    selected_farm_geometry_shapely = None # Shapely geometry object (from GeoDataFrame)

    if selected_farm_name == "Ù‡Ù…Ù‡ Ù…Ø²Ø§Ø±Ø¹":
        # Create a FeatureCollection of all filtered farms for GEE processing/clipping
        try:
            # Convert DataFrame rows to ee.Features with geometry
            features = []
            for index, row in filtered_farms_df.iterrows():
                if row['ee_geometry']:
                    # Create feature with geometry and properties
                    feature = ee.Feature(row['ee_geometry'], row.drop(['ee_geometry']).to_dict())
                    features.append(feature)
            if features:
                 selected_farm_geom_ee = ee.FeatureCollection(features).geometry() # Use combined geometry
                 # For map extent, get bounds from the df coordinates
                 min_lon = filtered_farms_df[['lon1', 'lon2', 'lon3', 'lon4']].min().min()
                 min_lat = filtered_farms_df[['lat1', 'lat2', 'lat3', 'lat4']].min().min()
                 max_lon = filtered_farms_df[['lon1', 'lon2', 'lon3', 'lon4']].max().max()
                 max_lat = filtered_farms_df[['lat1', 'lat2', 'lat3', 'lat4']].max().max()
                 map_bounds = [[min_lat, min_lon], [max_lat, max_lon]]
            else:
                 st.warning("Ù‡Ù†Ø¯Ø³Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ú©Ù„ÛŒ Ù…Ø²Ø§Ø±Ø¹ ÛŒØ§ÙØª Ù†Ø´Ø¯.")
                 selected_farm_geom_ee = None
                 map_bounds = None
        except Exception as e:
            st.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§ÛŒØ¬Ø§Ø¯ Ù‡Ù†Ø¯Ø³Ù‡ ØªØ±Ú©ÛŒØ¨ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ù…Ø²Ø§Ø±Ø¹: {e}")
            selected_farm_geom_ee = None
            map_bounds = None

        # Store the DataFrame for map plotting
        selected_farms_df_for_map = filtered_farms_df
        st.subheader(f"Ù†Ù…Ø§ÛŒØ´ Ú©Ù„ÛŒ Ù…Ø²Ø§Ø±Ø¹ Ø¨Ø±Ø§ÛŒ Ø±ÙˆØ²: {selected_day}")
        st.info(f"ØªØ¹Ø¯Ø§Ø¯ Ù…Ø²Ø§Ø±Ø¹ Ø¯Ø± Ø§ÛŒÙ† Ø±ÙˆØ²: {len(filtered_farms_df)}")
    else:
        # Get the row for the selected farm
        selected_farm_details_series = filtered_farms_df[filtered_farms_df['Ù…Ø²Ø±Ø¹Ù‡'] == selected_farm_name.replace('\u202B', '').replace('\u202C', '')].iloc[0]
        selected_farm_details = selected_farm_details_series.to_dict() # Convert Series to Dict for easier access
        selected_farm_geom_ee = selected_farm_details['ee_geometry'] # Get the pre-calculated ee.Geometry

        if not selected_farm_geom_ee:
             st.error(f"Ù‡Ù†Ø¯Ø³Ù‡ GEE Ø¨Ø±Ø§ÛŒ Ù…Ø²Ø±Ø¹Ù‡ '{selected_farm_name}' ÛŒØ§ÙØª Ù†Ø´Ø¯ ÛŒØ§ Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø§Ø³Øª.")
             # Optionally try to recalculate it here if needed

        st.subheader(f"Ø¬Ø²Ø¦ÛŒØ§Øª Ù…Ø²Ø±Ø¹Ù‡: {fix_farm_name_display(selected_farm_name)} (Ø±ÙˆØ²: {selected_day})")
        # Display farm details (use .get for safety)
        details_cols = st.columns(3)
        with details_cols[0]:
            # Ù…Ø³Ø§Ø­Øª might not be in the new CSV, handle gracefully
            st.metric("Ù…Ø³Ø§Ø­Øª Ø¯Ø§Ø´Øª (Ù‡Ú©ØªØ§Ø±)", f"{selected_farm_details.get('Ù…Ø³Ø§Ø­Øª', 'N/A'):,.2f}" if pd.notna(selected_farm_details.get('Ù…Ø³Ø§Ø­Øª')) else "N/A")
            st.metric("ÙˆØ§Ø±ÛŒØªÙ‡", f"{selected_farm_details.get('ÙˆØ§Ø±ÛŒØªÙ‡', 'N/A')}")
        with details_cols[1]:
            st.metric("Ú¯Ø±ÙˆÙ‡", f"{selected_farm_details.get('Ú¯Ø±ÙˆÙ‡', 'N/A')}")
            st.metric("Ø³Ù†", f"{selected_farm_details.get('Ø³Ù†', 'N/A')}")
        with details_cols[2]:
             # Display centroid coordinates (calculate from ee_geometry if needed)
             try:
                 if selected_farm_geom_ee:
                    centroid_ee = selected_farm_geom_ee.centroid(maxError=1).coordinates().getInfo()
                    st.metric("Ù…Ø±Ú©Ø² Ù…Ø²Ø±Ø¹Ù‡", f"{centroid_ee[1]:.5f}, {centroid_ee[0]:.5f}")
                 else:
                    st.metric("Ù…Ø±Ú©Ø² Ù…Ø²Ø±Ø¹Ù‡", "N/A")
             except Exception:
                  st.metric("Ù…Ø±Ú©Ø² Ù…Ø²Ø±Ø¹Ù‡", "Ø®Ø·Ø§ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡")


    # --- Map Display ---
    st.markdown("---")
    st.subheader(" Ù†Ù‚Ø´Ù‡ ÙˆØ¶Ø¹ÛŒØª Ù…Ø²Ø§Ø±Ø¹")

    # Define visualization parameters based on the selected index
    vis_params = {
        'NDVI': {'min': 0, 'max': 1, 'palette': ['red', 'yellow', 'green']},
        'EVI': {'min': 0, 'max': 1, 'palette': ['red', 'yellow', 'green']},
        'NDMI': {'min': -1, 'max': 1, 'palette': ['brown', 'white', 'blue']},
        'LAI': {'min': 0, 'max': 6, 'palette': ['white', 'lightgreen', 'darkgreen']},
        'MSI': {'min': 0, 'max': 3, 'palette': ['blue', 'white', 'brown']},
        'CVI': {'min': 0, 'max': 20, 'palette': ['yellow', 'lightgreen', 'darkgreen']},
        'NI': {'min': -1, 'max': 1, 'palette': ['red', 'yellow', 'green']},  # Red indicates nitrogen deficiency
    }

    map_center_lat = 31.534442
    map_center_lon = 48.724416
    initial_zoom = 11

    # Create a geemap Map instance
    m = geemap.Map(
        location=[map_center_lat, map_center_lon],
        zoom=initial_zoom,
        add_google_map=False # Start clean
    )
    m.add_basemap("HYBRID") # Add Google Satellite Hybrid basemap

    # Get the processed image for the current week using ee.Geometry
    if selected_farm_geom_ee:
        gee_image_current, error_msg_current = get_processed_image(
            selected_farm_geom_ee, start_date_current_str, end_date_current_str, selected_index
        )

        if gee_image_current:
            # Add the GEE layer to the map
            try:
                m.addLayer(
                    gee_image_current.clip(selected_farm_geom_ee), # Clip the image to the farm boundary(ies)
                    vis_params.get(selected_index, {'min': 0, 'max': 1, 'palette': ['red', 'yellow', 'green']}), # Default vis
                    f"{selected_index} (Ù…Ø­Ø¯ÙˆØ¯Ù‡ Ù…Ø²Ø§Ø±Ø¹)"
                )

                # Remove the problematic add_legend call and replace with a custom legend
                # Create a custom legend using folium
                if selected_index in ['NDVI', 'EVI', 'LAI', 'CVI', 'NI']:
                    legend_html = '''
                    <div style="position: fixed; bottom: 50px; right: 50px; z-index: 1000; background-color: white; padding: 10px; border: 2px solid grey; border-radius: 5px;">
                        <p style="margin: 0;"><strong>{} Legend</strong></p>
                        <p style="margin: 0; color: red;">Ú©Ù…Ø¨ÙˆØ¯/Ø¨Ø­Ø±Ø§Ù†ÛŒ</p>
                        <p style="margin: 0; color: yellow;">Ù…ØªÙˆØ³Ø·</p>
                        <p style="margin: 0; color: green;">Ù…Ø·Ù„ÙˆØ¨/Ø¨Ø§Ù„Ø§</p>
                    </div>
                    '''.format(selected_index)
                elif selected_index in ['NDMI', 'MSI']:
                    legend_html = '''
                    <div style="position: fixed; bottom: 50px; right: 50px; z-index: 1000; background-color: white; padding: 10px; border: 2px solid grey; border-radius: 5px;">
                        <p style="margin: 0;"><strong>{} Legend</strong></p>
                        <p style="margin: 0; color: blue;">Ù…Ø±Ø·ÙˆØ¨/Ø¨Ø§Ù„Ø§</p>
                        <p style="margin: 0; color: white;">Ù…ØªÙˆØ³Ø·</p>
                        <p style="margin: 0; color: brown;">Ø®Ø´Ú©/Ù¾Ø§ÛŒÛŒÙ†</p>
                    </div>
                    '''.format(selected_index)
                else:
                    # Default legend for other indices
                    legend_html = '''
                    <div style="position: fixed; bottom: 50px; right: 50px; z-index: 1000; background-color: white; padding: 10px; border: 2px solid grey; border-radius: 5px;">
                        <p style="margin: 0;"><strong>{} Legend</strong></p>
                        <p style="margin: 0;">Low</p>
                        <p style="margin: 0;">Medium</p>
                        <p style="margin: 0;">High</p>
                    </div>
                    '''.format(selected_index)
                
                # Add the custom legend to the map
                m.get_root().html.add_child(folium.Element(legend_html))

                # Add farm boundaries to the map using folium.Polygon
                if selected_farm_name == "Ù‡Ù…Ù‡ Ù…Ø²Ø§Ø±Ø¹":
                     # Add all filtered farms as polygons
                     if not selected_farms_df_for_map.empty:
                         for idx, farm in selected_farms_df_for_map.iterrows():
                             coords = [
                                [farm['lat1'], farm['lon1']], [farm['lat2'], farm['lon2']],
                                [farm['lat3'], farm['lon3']], [farm['lat4'], farm['lon4']]
                                # No need to close the loop for folium.Polygon
                             ]
                             # Check for NaN coords before plotting
                             if not any(pd.isna(c) for point in coords for c in point):
                                 folium.Polygon(
                                     locations=coords,
                                     popup=f"Ù…Ø²Ø±Ø¹Ù‡: {farm['Ù…Ø²Ø±Ø¹Ù‡']}\nÚ¯Ø±ÙˆÙ‡: {farm.get('Ú¯Ø±ÙˆÙ‡', 'N/A')}",
                                     tooltip=farm['Ù…Ø²Ø±Ø¹Ù‡'],
                                     color='cyan',
                                     fill=True,
                                     fill_color='cyan',
                                     fill_opacity=0.1,
                                     weight=2
                                 ).add_to(m)
                         # Adjust map bounds to fit all farms
                         if map_bounds:
                             m.fit_bounds(map_bounds, padding=(30, 30))

                elif selected_farm_geom_ee:
                    # Add single selected farm polygon
                    try:
                        # Get coordinates from the ee.Geometry object
                        poly_coords_ee = selected_farm_geom_ee.coordinates().get(0).getInfo() # Get outer ring
                        # Convert [[lon, lat], ...] to [[lat, lon], ...] for folium
                        poly_coords_folium = [[lat, lon] for lon, lat in poly_coords_ee]
                        folium.Polygon(
                            locations=poly_coords_folium,
                            popup=f"Ù…Ø²Ø±Ø¹Ù‡: {selected_farm_name}",
                            tooltip=selected_farm_name,
                            color='red',
                            fill=True,
                            fill_color='red',
                            fill_opacity=0.2,
                            weight=3
                        ).add_to(m)
                        # Center map on the selected farm's centroid
                        centroid_ee = selected_farm_geom_ee.centroid(maxError=1).coordinates().getInfo()
                        m.location = [centroid_ee[1], centroid_ee[0]] # lat, lon
                        m.zoom = 15 # Zoom closer for a single polygon
                    except Exception as poly_err:
                        st.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø±Ø³Ù… Ú†Ù†Ø¯Ø¶Ù„Ø¹ÛŒ Ù…Ø²Ø±Ø¹Ù‡ {selected_farm_name} Ø±ÙˆÛŒ Ù†Ù‚Ø´Ù‡: {poly_err}")
                         # Fallback: use corner coordinates if ee geometry fails for folium
                        coords = [
                            [selected_farm_details['lat1'], selected_farm_details['lon1']],
                            [selected_farm_details['lat2'], selected_farm_details['lon2']],
                            [selected_farm_details['lat3'], selected_farm_details['lon3']],
                            [selected_farm_details['lat4'], selected_farm_details['lon4']]
                         ]
                        if not any(pd.isna(c) for point in coords for c in point):
                            folium.Polygon(locations=coords, color='red', fill=True, fill_color='red', fill_opacity=0.2, weight=3, tooltip=selected_farm_name).add_to(m)
                            # Center based on average coordinates
                            avg_lat = (selected_farm_details['lat1'] + selected_farm_details['lat2'] + selected_farm_details['lat3'] + selected_farm_details['lat4']) / 4
                            avg_lon = (selected_farm_details['lon1'] + selected_farm_details['lon2'] + selected_farm_details['lon3'] + selected_farm_details['lon4']) / 4
                            m.location = [avg_lat, avg_lon]
                            m.zoom = 15

                m.add_layer_control() # Add layer control to toggle base maps and layers

            except Exception as map_err:
                st.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§ÙØ²ÙˆØ¯Ù† Ù„Ø§ÛŒÙ‡ Ø¨Ù‡ Ù†Ù‚Ø´Ù‡: {map_err}")
                st.error(traceback.format_exc())
        else:
            st.warning(f"ØªØµÙˆÛŒØ±ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø±ÙˆÛŒ Ù†Ù‚Ø´Ù‡ ÛŒØ§ÙØª Ù†Ø´Ø¯. {error_msg_current}")

    # Display the map in Streamlit
    st_folium(m, width=None, height=500, use_container_width=True)
    st.caption("Ø¨Ø±Ø§ÛŒ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø¬Ø²Ø¦ÛŒØ§Øª Ø±ÙˆÛŒ Ù…Ø§Ø±Ú©Ø±Ù‡Ø§ Ú©Ù„ÛŒÚ© Ú©Ù†ÛŒØ¯. Ø§Ø² Ú©Ù†ØªØ±Ù„ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ ØªØºÛŒÛŒØ± Ù†Ù‚Ø´Ù‡ Ù¾Ø§ÛŒÙ‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.")
    # Note: Direct PNG download from st_folium/geemap isn't built-in easily.
    st.info("ğŸ’¡ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ù†Ù‚Ø´Ù‡ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² Ø§Ø¨Ø²Ø§Ø± Ø¹Ú©Ø³ Ú¯Ø±ÙØªÙ† Ø§Ø² ØµÙØ­Ù‡ (Screenshot) Ù…Ø±ÙˆØ±Ú¯Ø± ÛŒØ§ Ø³ÛŒØ³ØªÙ… Ø¹Ø§Ù…Ù„ Ø®ÙˆØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.")


    # --- Time Series Chart ---
    st.markdown("---")
    st.subheader(f"ğŸ“ˆ Ù†Ù…ÙˆØ¯Ø§Ø± Ø±ÙˆÙ†Ø¯ Ø²Ù…Ø§Ù†ÛŒ Ø´Ø§Ø®Øµ {selected_index}")

    if selected_farm_name == "Ù‡Ù…Ù‡ Ù…Ø²Ø§Ø±Ø¹":
        st.info("Ù„Ø·ÙØ§Ù‹ ÛŒÚ© Ù…Ø²Ø±Ø¹Ù‡ Ø®Ø§Øµ Ø±Ø§ Ø§Ø² Ù¾Ù†Ù„ Ú©Ù†Ø§Ø±ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯ ØªØ§ Ù†Ù…ÙˆØ¯Ø§Ø± Ø±ÙˆÙ†Ø¯ Ø²Ù…Ø§Ù†ÛŒ Ø¢Ù† Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆØ¯.")
    elif selected_farm_geom_ee: # Use the ee.Geometry object
        # Define a longer period for the time series chart (e.g., last 6 months)
        timeseries_end_date = today.strftime('%Y-%m-%d')
        timeseries_start_date = (today - datetime.timedelta(days=180)).strftime('%Y-%m-%d')

        # Pass the ee.Geometry (could be polygon) to the time series function
        ts_df, ts_error = get_index_time_series(
            selected_farm_geom_ee,
            selected_index,
            start_date=timeseries_start_date,
            end_date=timeseries_end_date
        )

        if ts_error:
            st.warning(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ: {ts_error}")
        elif not ts_df.empty:
            st.line_chart(ts_df[selected_index])
            st.caption(f"Ù†Ù…ÙˆØ¯Ø§Ø± ØªØºÛŒÛŒØ±Ø§Øª Ù…ÛŒØ§Ù†Ú¯ÛŒÙ† Ø´Ø§Ø®Øµ {selected_index} Ø¨Ø±Ø§ÛŒ Ú©Ù„ Ù…Ø³Ø§Ø­Øª Ù…Ø²Ø±Ø¹Ù‡ {selected_farm_name} Ø¯Ø± 6 Ù…Ø§Ù‡ Ú¯Ø°Ø´ØªÙ‡.")
        else:
            st.info(f"Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆØ¯Ø§Ø± Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ {selected_index} Ø¯Ø± Ø¨Ø§Ø²Ù‡ Ù…Ø´Ø®Øµ Ø´Ø¯Ù‡ ÛŒØ§ÙØª Ù†Ø´Ø¯.")
    # Remove the old geometry type check
    # else:
    #     st.warning("Ù†ÙˆØ¹ Ù‡Ù†Ø¯Ø³Ù‡ Ù…Ø²Ø±Ø¹Ù‡ Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆØ¯Ø§Ø± Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯ (ÙÙ‚Ø· Ù†Ù‚Ø·Ù‡).")
    else:
        st.warning("Ù‡Ù†Ø¯Ø³Ù‡ Ù…Ø²Ø±Ø¹Ù‡ Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆØ¯Ø§Ø± Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª.")


    # ==============================================================================
    # Helper Function for Status Determination
    # ==============================================================================

    def determine_status(row, index_name):
        """Determines the status based on change in index value."""
        if pd.isna(row['ØªØºÛŒÛŒØ±']) or pd.isna(row[f'{index_name} (Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ)']) or pd.isna(row[f'{index_name} (Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„)']):
            return "Ø¨Ø¯ÙˆÙ† Ø¯Ø§Ø¯Ù‡"

        change_val = row['ØªØºÛŒÛŒØ±']
        # Threshold for significant change
        threshold = 0.05

        # For indices where higher is better (NDVI, EVI, LAI, CVI, NDMI)
        if index_name in ['NDVI', 'EVI', 'LAI', 'CVI', 'NDMI']:
            if change_val > threshold:
                return "Ø±Ø´Ø¯ Ù…Ø«Ø¨Øª / Ø¨Ù‡Ø¨ÙˆØ¯"
            elif change_val < -threshold:
                return "ØªÙ†Ø´ / Ú©Ø§Ù‡Ø´"
            else:
                return "Ø«Ø§Ø¨Øª"
        # For indices where lower is better (MSI)
        elif index_name in ['MSI']:
            if change_val < -threshold: # Negative change means improvement (less stress)
                return "Ø¨Ù‡Ø¨ÙˆØ¯"
            elif change_val > threshold: # Positive change means deterioration (more stress)
                return "ØªÙ†Ø´ / Ø¨Ø¯ØªØ± Ø´Ø¯Ù†"
            else:
                return "Ø«Ø§Ø¨Øª"
        else:
            # Default case if index type is unknown
            return "Ù†Ø§Ù…Ø´Ø®Øµ"

    # ==============================================================================
    # Ranking Table
    # ==============================================================================
    st.markdown("---")
    st.subheader(f"ğŸ“Š Ø¬Ø¯ÙˆÙ„ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…Ø²Ø§Ø±Ø¹ Ø¨Ø± Ø§Ø³Ø§Ø³ {selected_index} (Ø±ÙˆØ²: {selected_day})")
    st.markdown("Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù…ØªÙˆØ³Ø· Ø´Ø§Ø®Øµ Ø¯Ø± Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ Ø¨Ø§ Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„.")

    @st.cache_data(show_spinner=f"Ø¯Ø± Ø­Ø§Ù„ Ù…Ø­Ø§Ø³Ø¨Ù‡ {selected_index} Ø¨Ø±Ø§ÛŒ Ù…Ø²Ø§Ø±Ø¹...", persist=True)
    def calculate_weekly_indices(_farms_df, index_name, start_curr, end_curr, start_prev, end_prev):
        """Calculates the average index value for the current and previous week for a list of farms using their ee.Geometry."""
        results = []
        errors = []
        total_farms = len(_farms_df)
        progress_bar = st.progress(0)

        # Geometries are already in 'ee_geometry' column

        for i, (idx, farm) in enumerate(_farms_df.iterrows()):
            farm_name = farm['Ù…Ø²Ø±Ø¹Ù‡']
            ee_geom = farm['ee_geometry'] # Get the ee.Geometry object

            if not ee_geom:
                # This should ideally be caught during loading, but double-check
                errors.append(f"Ù‡Ù†Ø¯Ø³Ù‡ Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø¨Ø±Ø§ÛŒ {farm_name} Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù‡ÙØªÚ¯ÛŒ.")
                progress_bar.progress((i + 1) / total_farms) # Update progress even on error
                continue # Skip this farm

            def get_mean_value(start, end):
                try:
                    image, error = get_processed_image(ee_geom, start, end, index_name)
                    if image:
                        # Reduce region to get the mean value over the polygon
                        mean_dict = image.reduceRegion(
                            reducer=ee.Reducer.mean(),
                            geometry=ee_geom,
                            scale=10,  # Scale in meters
                            maxPixels=1e9 # Increase maxPixels
                        ).getInfo()
                        # Handle potential null result from reduceRegion
                        if mean_dict is None:
                            return None, f"reduceRegion Ù†ØªÛŒØ¬Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ {farm_name} ({start}-{end}) Ø¨Ø±Ù†Ú¯Ø±Ø¯Ø§Ù†Ø¯."
                        return mean_dict.get(index_name), None
                    else:
                        return None, error
                except ee.EEException as e_ee:
                     error_msg = f"Ø®Ø·Ø§ÛŒ GEE Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ù‚Ø¯Ø§Ø± Ø¨Ø±Ø§ÛŒ {farm_name} ({start}-{end}): {e_ee}"
                     return None, error_msg
                except Exception as e_other:
                     # Catch other errors during reduceRegion or getInfo
                     error_msg = f"Ø®Ø·Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…Ù‚Ø¯Ø§Ø± Ø¨Ø±Ø§ÛŒ {farm_name} ({start}-{end}): {e_other}"
                     return None, error_msg

            # Calculate for current week
            current_val, err_curr = get_mean_value(start_curr, end_curr)
            if err_curr: errors.append(f"{farm_name} (Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ): {err_curr}")

            # Calculate for previous week
            previous_val, err_prev = get_mean_value(start_prev, end_prev)
            if err_prev: errors.append(f"{farm_name} (Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„): {err_prev}")


            # Calculate change
            change = None
            if current_val is not None and previous_val is not None:
                try:
                    change = current_val - previous_val
                except TypeError: # Handle cases where values might not be numeric unexpectedly
                    change = None

            results.append({
                'Ù…Ø²Ø±Ø¹Ù‡': farm_name,
                'Ú¯Ø±ÙˆÙ‡': farm.get('Ú¯Ø±ÙˆÙ‡', 'N/A'),
                f'{index_name} (Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ)': current_val,
                f'{index_name} (Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„)': previous_val,
                'ØªØºÛŒÛŒØ±': change
            })

            # Update progress bar
            progress_bar.progress((i + 1) / total_farms)

        progress_bar.empty() # Remove progress bar after completion
        return pd.DataFrame(results), errors

    # Calculate and display the ranking table
    ranking_df, calculation_errors = calculate_weekly_indices(
        filtered_farms_df, # Pass the filtered DataFrame
        selected_index,
        start_date_current_str,
        end_date_current_str,
        start_date_previous_str,
        end_date_previous_str
    )

    # Display any errors that occurred during calculation
    if calculation_errors:
        st.warning("âš ï¸ Ø¨Ø±Ø®ÛŒ Ø®Ø·Ø§Ù‡Ø§ Ø¯Ø± Ø­ÛŒÙ† Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ Ø±Ø® Ø¯Ø§Ø¯:")
        for error in calculation_errors[:10]: # Show first 10 errors
            st.warning(f"- {error}")
        if len(calculation_errors) > 10:
            st.warning(f"... Ùˆ {len(calculation_errors) - 10} Ø®Ø·Ø§ÛŒ Ø¯ÛŒÚ¯Ø±.")


    if not ranking_df.empty:
        # Sort by the current week's index value (descending for NDVI/EVI/LAI/CVI/NDMI, ascending for MSI)
        ascending_sort = selected_index not in ['MSI'] # Simpler logic: Ascending only if MSI
        ranking_df_sorted = ranking_df.sort_values(
            by=f'{selected_index} (Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ)',
            ascending=ascending_sort,
            na_position='last'
        ).reset_index(drop=True)

        # Add rank number
        ranking_df_sorted.index = ranking_df_sorted.index + 1
        ranking_df_sorted.index.name = 'Ø±ØªØ¨Ù‡'

        # Apply the determine_status function using .apply
        ranking_df_sorted['ÙˆØ¶Ø¹ÛŒØª'] = ranking_df_sorted.apply(
            lambda row: determine_status(row, selected_index), axis=1
        )

        # Format numbers for better readability
        cols_to_format = [f'{selected_index} (Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ)', f'{selected_index} (Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„)', 'ØªØºÛŒÛŒØ±']
        for col in cols_to_format:
            if col in ranking_df_sorted.columns:
                 # Check if column exists before formatting
                 ranking_df_sorted[col] = ranking_df_sorted[col].map(lambda x: f"{x:.3f}" if pd.notna(x) else "N/A")

        # Select columns to display, including 'Ú¯Ø±ÙˆÙ‡'
        display_columns = ['Ù…Ø²Ø±Ø¹Ù‡', 'Ú¯Ø±ÙˆÙ‡'] + cols_to_format + ['ÙˆØ¶Ø¹ÛŒØª']
        # Ensure only existing columns are selected
        display_columns = [col for col in display_columns if col in ranking_df_sorted.columns]

        # Display the table with color coding and selected columns
        st.dataframe(ranking_df_sorted[display_columns], use_container_width=True)
        
        # Add a summary of farm statuses
        st.subheader("ğŸ“Š Ø®Ù„Ø§ØµÙ‡ ÙˆØ¶Ø¹ÛŒØª Ù…Ø²Ø§Ø±Ø¹")
        
        # Display status counts with appropriate colors
        col1, col2, col3 = st.columns(3)
        
        # Dynamically find positive and negative status terms used
        status_counts = ranking_df_sorted['ÙˆØ¶Ø¹ÛŒØª'].value_counts()
        positive_terms = [s for s in status_counts.index if "Ø¨Ù‡Ø¨ÙˆØ¯" in s]
        negative_terms = [s for s in status_counts.index if any(sub in s for sub in ["ØªÙ†Ø´", "Ú©Ø§Ù‡Ø´", "Ø¨Ø¯ØªØ±"])]
        neutral_term = "Ø«Ø§Ø¨Øª"
        nodata_term = "Ø¨Ø¯ÙˆÙ† Ø¯Ø§Ø¯Ù‡"

        with col1:
            pos_count = sum(status_counts.get(term, 0) for term in positive_terms)
            if pos_count > 0:
                pos_label = positive_terms[0] if positive_terms else "Ø¨Ù‡Ø¨ÙˆØ¯"
                st.metric(f"ğŸŸ¢ {pos_label}", pos_count)
            else:
                 st.metric("ğŸŸ¢ Ø¨Ù‡Ø¨ÙˆØ¯", 0) # Show 0 if none

        with col2:
            neutral_count = status_counts.get(neutral_term, 0)
            st.metric(f"âšª {neutral_term}", neutral_count)

        with col3:
            neg_count = sum(status_counts.get(term, 0) for term in negative_terms)
            if neg_count > 0:
                neg_label = negative_terms[0] if negative_terms else "ØªÙ†Ø´"
                st.metric(f"ğŸ”´ {neg_label}", neg_count)
            else:
                st.metric("ğŸ”´ ØªÙ†Ø´", 0) # Show 0 if none

        # Add explanation
        st.info(f"""
        **ØªÙˆØ¶ÛŒØ­Ø§Øª:**
        - **ğŸŸ¢ Ø±Ø´Ø¯ Ù…Ø«Ø¨Øª / Ø¨Ù‡Ø¨ÙˆØ¯**: Ù…Ø²Ø§Ø±Ø¹ÛŒ Ú©Ù‡ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„ Ø¨Ù‡Ø¨ÙˆØ¯ Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡ÛŒ Ø¯Ø§Ø´ØªÙ‡â€ŒØ§Ù†Ø¯ (Ø§ÙØ²Ø§ÛŒØ´ NDVI/EVI/LAI/CVI/NDMI ÛŒØ§ Ú©Ø§Ù‡Ø´ MSI).
        - **âšª Ø«Ø§Ø¨Øª**: Ù…Ø²Ø§Ø±Ø¹ÛŒ Ú©Ù‡ ØªØºÛŒÛŒØ± Ù…Ø¹Ù†Ø§Ø¯Ø§Ø±ÛŒ Ù†Ø¯Ø§Ø´ØªÙ‡â€ŒØ§Ù†Ø¯.
        - **ğŸ”´ ØªÙ†Ø´ / Ú©Ø§Ù‡Ø´ / Ø¨Ø¯ØªØ± Ø´Ø¯Ù†**: Ù…Ø²Ø§Ø±Ø¹ÛŒ Ú©Ù‡ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„ ÙˆØ¶Ø¹ÛŒØª Ù†Ø§Ù…Ø·Ù„ÙˆØ¨â€ŒØªØ±ÛŒ Ø¯Ø§Ø´ØªÙ‡â€ŒØ§Ù†Ø¯ (Ú©Ø§Ù‡Ø´ NDVI/EVI/LAI/CVI/NDMI ÛŒØ§ Ø§ÙØ²Ø§ÛŒØ´ MSI).
        """)

        # Add download button for the table
        csv_data = ranking_df_sorted.to_csv(index=True).encode('utf-8')
        st.download_button(
            label="ğŸ“¥ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¬Ø¯ÙˆÙ„ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ (CSV)",
            data=csv_data,
            file_name=f'ranking_{selected_index}_{selected_day}_{end_date_current_str}.csv',
            mime='text/csv',
        )
    else:
        st.info(f"Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø¬Ø¯ÙˆÙ„ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ {selected_index} Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.")


    st.markdown("---")
    st.sidebar.markdown("---")
    st.sidebar.markdown("Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Streamlit, Google Earth Engine, Ùˆ geemap") # Removed geopandas


# --- New Tab for Needs Analysis ---
with tab3:
    st.header("ØªØ­Ù„ÛŒÙ„ Ù†ÛŒØ§Ø² Ø¢Ø¨ÛŒØ§Ø±ÛŒ Ùˆ Ú©ÙˆØ¯Ø¯Ù‡ÛŒ")

    if selected_farm_name == "Ù‡Ù…Ù‡ Ù…Ø²Ø§Ø±Ø¹":
        st.info("Ù„Ø·ÙØ§Ù‹ ÛŒÚ© Ù…Ø²Ø±Ø¹Ù‡ Ø®Ø§Øµ Ø±Ø§ Ø§Ø² Ù¾Ù†Ù„ Ú©Ù†Ø§Ø±ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯ ØªØ§ ØªØ­Ù„ÛŒÙ„ Ù†ÛŒØ§Ø²Ù‡Ø§ÛŒ Ø¢Ù† Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆØ¯.")
    elif selected_farm_geom_ee: # Use the ee.Geometry object
        st.subheader(f"ØªØ­Ù„ÛŒÙ„ Ø¨Ø±Ø§ÛŒ Ù…Ø²Ø±Ø¹Ù‡: {fix_farm_name_display(selected_farm_name)}")

        # Define thresholds (allow user adjustment)
        st.markdown("**ØªÙ†Ø¸ÛŒÙ… Ø¢Ø³ØªØ§Ù†Ù‡â€ŒÙ‡Ø§:**")
        ndmi_threshold = st.slider("Ø¢Ø³ØªØ§Ù†Ù‡ NDMI Ø¨Ø±Ø§ÛŒ Ù‡Ø´Ø¯Ø§Ø± Ø¢Ø¨ÛŒØ§Ø±ÛŒ:", 0.0, 0.5, 0.25, 0.01,
                                     help="Ø§Ú¯Ø± NDMI Ú©Ù…ØªØ± Ø§Ø² Ø§ÛŒÙ† Ù…Ù‚Ø¯Ø§Ø± Ø¨Ø§Ø´Ø¯ØŒ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¢Ø¨ÛŒØ§Ø±ÛŒ Ø§Ø¹Ù„Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯.")
        ndvi_drop_threshold = st.slider("Ø¢Ø³ØªØ§Ù†Ù‡ Ø§ÙØª NDVI Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ú©ÙˆØ¯Ø¯Ù‡ÛŒ (%):", 0.0, 20.0, 5.0, 0.5,
                                        help="Ø§Ú¯Ø± NDVI Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„ Ø¨ÛŒØ´ Ø§Ø² Ø§ÛŒÙ† Ø¯Ø±ØµØ¯ Ø§ÙØª Ú©Ù†Ø¯ØŒ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ø±Ø±Ø³ÛŒ Ú©ÙˆØ¯Ø¯Ù‡ÛŒ Ø§Ø¹Ù„Ø§Ù… Ù…ÛŒâ€ŒØ´ÙˆØ¯.")

        # Get the required index data for the selected farm using its ee.Geometry
        farm_needs_data = get_farm_needs_data(
            selected_farm_geom_ee, # Pass ee.Geometry
            start_date_current_str, end_date_current_str,
            start_date_previous_str, end_date_previous_str
        )

        if farm_needs_data['error']:
            st.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ø§Ø®Øµ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ù†ÛŒØ§Ø²Ù‡Ø§: {farm_needs_data['error']}")
        elif farm_needs_data['NDMI_curr'] is None or farm_needs_data['NDVI_curr'] is None:
            st.warning("Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ø§Ø®Øµ Ù„Ø§Ø²Ù… (NDMI/NDVI) Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¯Ø± Ø¯ÙˆØ±Ù‡ ÙØ¹Ù„ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.")
        else:
            # --- Display Current Indices ---
            st.markdown("**Ù…Ù‚Ø§Ø¯ÛŒØ± Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ (Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ):**")
            idx_cols = st.columns(3)
            with idx_cols[0]:
                st.metric("NDVI", f"{farm_needs_data['NDVI_curr']:.3f}")
            with idx_cols[1]:
                st.metric("NDMI", f"{farm_needs_data['NDMI_curr']:.3f}")
            with idx_cols[2]:
                st.metric("EVI", f"{farm_needs_data.get('EVI_curr', 'N/A'):.3f}" if farm_needs_data.get('EVI_curr') else "N/A")

            # --- Generate Recommendations ---
            recommendations = []
            # 1. Irrigation Check
            if farm_needs_data['NDMI_curr'] < ndmi_threshold:
                recommendations.append("ğŸ’§ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¢Ø¨ÛŒØ§Ø±ÛŒ")

            # 2. Fertilization Check (NDVI drop)
            if farm_needs_data['NDVI_prev'] is not None and farm_needs_data['NDVI_curr'] < farm_needs_data['NDVI_prev']:
                ndvi_change_percent = ((farm_needs_data['NDVI_prev'] - farm_needs_data['NDVI_curr']) / farm_needs_data['NDVI_prev']) * 100
                if ndvi_change_percent > ndvi_drop_threshold:
                    recommendations.append(f"âš ï¸ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ø±Ø±Ø³ÛŒ Ú©ÙˆØ¯Ø¯Ù‡ÛŒ (Ø§ÙØª {ndvi_change_percent:.1f}% Ø¯Ø± NDVI)")
            elif farm_needs_data['NDVI_prev'] is None:
                 st.caption("Ø¯Ø§Ø¯Ù‡ NDVI Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø§ÙØª Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª.")

            # 3. Default if no issues
            if not recommendations:
                recommendations.append("âœ… ÙˆØ¶Ø¹ÛŒØª ÙØ¹Ù„ÛŒ Ù…Ø·Ù„ÙˆØ¨ Ø¨Ù‡ Ù†Ø¸Ø± Ù…ÛŒâ€ŒØ±Ø³Ø¯.")

            # Display Recommendations
            st.markdown("**ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ‡:**")
            for rec in recommendations:
                if "Ø¢Ø¨ÛŒØ§Ø±ÛŒ" in rec: st.error(rec)
                elif "Ú©ÙˆØ¯Ø¯Ù‡ÛŒ" in rec: st.warning(rec)
                else: st.success(rec)

            # --- Get and Display AI Analysis ---
            if gemini_model:
                st.markdown("**ØªØ­Ù„ÛŒÙ„ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ:**")
                ai_explanation = get_ai_analysis(gemini_model, selected_farm_name, farm_needs_data, recommendations)
                st.markdown(ai_explanation)
            else:
                st.info("Ø³Ø±ÙˆÛŒØ³ ØªØ­Ù„ÛŒÙ„ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª.")

    else:
         st.info("Ø§Ø¨ØªØ¯Ø§ ÛŒÚ© Ù…Ø²Ø±Ø¹Ù‡ Ø±Ø§ Ø§Ø² Ù¾Ù†Ù„ Ú©Ù†Ø§Ø±ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯.")


st.markdown("---")
st.sidebar.markdown("---")
st.sidebar.markdown("Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Streamlit, Google Earth Engine, Ùˆ geemap")