import streamlit as st
import pandas as pd
import ee
import geemap.foliumap as geemap
import folium
import json
import datetime
import plotly.express as px
import plotly.graph_objects as go
import os
from io import BytesIO
import requests
import traceback
from streamlit_folium import st_folium
import base64
import google.generativeai as genai

# --- Custom CSS ---
st.set_page_config(
    page_title="Ø³Ø§Ù…Ø§Ù†Ù‡ Ù¾Ø§ÛŒØ´ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù†ÛŒØ´Ú©Ø±",
    page_icon="ğŸŒ¾",
    layout="wide"
)

# Modern CSS with dark mode and color palette
st.markdown("""
    <style>
        @import url('https://fonts.googleapis.com/css2?family=Vazirmatn:wght@400;700&display=swap');
        @import url('https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css');
        html, body, .main, .stApp {
            font-family: 'Vazirmatn', sans-serif !important;
            background: linear-gradient(135deg, #e0f7fa 0%, #f8fafc 100%);
        }
        /* Modern card style */
        .modern-card {
            background: linear-gradient(135deg, #43cea2 0%, #185a9d 100%);
            color: white;
            border-radius: 18px;
            padding: 24px 18px;
            margin: 10px 0;
            box-shadow: 0 4px 16px rgba(30,60,114,0.08);
            text-align: center;
            transition: transform 0.2s;
        }
        .modern-card:hover {
            transform: translateY(-4px) scale(1.02);
        }
        /* Sidebar logo */
        .sidebar-logo {
            display: flex;
            align-items: center;
            justify-content: center;
            margin-bottom: 1.5rem;
        }
        .sidebar-logo img {
            width: 90px;
            height: 90px;
            border-radius: 18px;
            box-shadow: 0 2px 8px rgba(30,60,114,0.12);
        }
        /* Main header logo */
        .main-logo {
            width: 48px;
            height: 48px;
            border-radius: 12px;
            margin-left: 12px;
            vertical-align: middle;
        }
        /* Dark mode support */
        @media (prefers-color-scheme: dark) {
            html, body, .main, .stApp {
                background: linear-gradient(135deg, #232526 0%, #414345 100%);
                color: #f8fafc;
            }
        }
        /* Status badges */
        .status-badge {
            display: inline-block;
            padding: 0.25em 0.5em;
            font-size: 0.75em;
            font-weight: bold;
            line-height: 1;
            text-align: center;
            white-space: nowrap;
            vertical-align: baseline;
            border-radius: 0.25rem;
            color: #fff;
        }
        .status-positive {
            background-color: #28a745; /* Green */
        }
        .status-negative {
            background-color: #dc3545; /* Red */
        }
        .status-neutral {
            background-color: #6c757d; /* Grey */
             color: #fff; /* Ensure text is white */
        }
        .status-nodata {
            background-color: #ffc107; /* Yellow */
            color: #212529; /* Dark text */
        }
    </style>
""", unsafe_allow_html=True)

# --- Helper for Status Badges ---
def status_badge(status: str) -> str:
    """Returns HTML for a status badge with color."""
    if "Ø¨Ù‡Ø¨ÙˆØ¯" in status or "Ø±Ø´Ø¯ Ù…Ø«Ø¨Øª" in status:
        badge_class = "status-positive"
    elif "ØªÙ†Ø´" in status or "Ú©Ø§Ù‡Ø´" in status or "Ø¨Ø¯ØªØ± Ø´Ø¯Ù†" in status:
        badge_class = "status-negative"
    elif "Ø«Ø§Ø¨Øª" in status:
        badge_class = "status-neutral"
    elif "Ø¨Ø¯ÙˆÙ† Ø¯Ø§Ø¯Ù‡" in status:
         badge_class = "status-nodata"
    else:
        badge_class = "status-neutral" # Default

    return f'<span class="status-badge {badge_class}">{status}</span>'


# --- Helper for Modern Metric Card ---
def modern_metric_card(title: str, value: str, icon: str, color: str) -> str:
    """
    Returns a modern styled HTML card for displaying a metric.
    :param title: Title of the metric
    :param value: Value of the metric
    :param icon: FontAwesome icon class (e.g., 'fa-leaf')
    :param color: Accent color for the card background gradient
    :return: HTML string
    """
    return f'''
    <div class="modern-card" style="background: linear-gradient(135deg, {color} 0%, #185a9d 100%);">
        <div style="font-size: 0.9em; opacity: 0.8;">{title} <i class="fa {icon}"></i></div>
        <div style="font-size: 1.8em; font-weight: bold; margin-top: 5px;">{value}</div>
    </div>
    '''

# --- Modern Progress Bar (HTML) ---
# Moved this function definition BEFORE its usage in calculate_weekly_indices
def modern_progress_bar(progress: float) -> str:
    """
    Returns a modern styled HTML progress bar for Streamlit.
    :param progress: float between 0 and 1
    :return: HTML string
    """
    percent = int(progress * 100)
    # Adjust color gradient based on progress
    color_start = '#43cea2' # Greenish
    color_end = '#185a9d'   # Bluish
    intermediate_color = '#ffc107' # Yellowish for middle
    if percent < 50:
         current_color_end = f"rgba({int(0x43 + (0xff-0x43)*(percent/50))},{int(0xce + (0xc1-0xce)*(percent/50))},{int(0xa2 + (0x07-0xa2)*(percent/50))},1)"
         background_gradient = f"linear-gradient(90deg, {color_start} 0%, {current_color_end} 100%)"
    elif percent < 100:
         current_color_start = f"rgba({int(0xff + (0x18-0xff)*((percent-50)/50))},{int(0xc1 + (0x5a-0xc1)*((percent-50)/50))},{int(0x07 + (0x9d-0x07)*((percent-50)/50))},1)"
         background_gradient = f"linear-gradient(90deg, {intermediate_color} 0%, {current_color_start} 100%)"
    else:
         background_gradient = f"linear-gradient(90deg, {color_start} 0%, {color_end} 100%)"


    return f'''
    <div style="width: 100%; background: #e0f7fa; border-radius: 12px; height: 22px; margin: 8px 0; box-shadow: 0 2px 8px rgba(30,60,114,0.08);">
      <div style="width: {percent}%; background: {background_gradient}; height: 100%; border-radius: 12px; transition: width 0.3s;"></div>
      <span style="position: absolute; left: 50%; top: 0; transform: translateX(-50%); color: #185a9d; font-weight: bold; line-height: 22px;">{percent}%</span>
    </div>
    '''


# --- Sidebar Logo ---
st.sidebar.markdown(
    """
    <div class='sidebar-logo'>
        <img src='https://raw.githubusercontent.com/esmaeilkiani13877/MonitoringSugarcane-13/main/logo%20(1).png' alt='Ù„ÙˆÚ¯Ùˆ Ø³Ø§Ù…Ø§Ù†Ù‡' />
    </div>
    """,
    unsafe_allow_html=True
)

# --- Main Header with Logo ---
st.markdown(
    """
    <div style='display: flex; align-items: center; gap: 16px; margin-bottom: 0.5rem;'>
        <img src='https://raw.githubusercontent.com/esmaeilkiani13877/MonitoringSugarcane-13/main/logo%20(1).png' class='main-logo' alt='Ù„ÙˆÚ¯Ùˆ' />
        <h1 style='font-family: Vazirmatn, sans-serif; color: #185a9d; margin: 0;'>Ø³Ø§Ù…Ø§Ù†Ù‡ Ù¾Ø§ÛŒØ´ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù†ÛŒØ´Ú©Ø±</h1>
    </div>
    <h4 style='color: #43cea2; margin-top: 0;'>Ù…Ø·Ø§Ù„Ø¹Ø§Øª Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ Ø´Ø±Ú©Øª Ú©Ø´Øª Ùˆ ØµÙ†Ø¹Øª Ø¯Ù‡Ø®Ø¯Ø§</h4>
    """,
    unsafe_allow_html=True
)

# --- Configuration ---
APP_TITLE = "Ø³Ø§Ù…Ø§Ù†Ù‡ Ù¾Ø§ÛŒØ´ Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù†ÛŒØ´Ú©Ø±"
APP_SUBTITLE = "Ù…Ø·Ø§Ù„Ø¹Ø§Øª Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ Ø´Ø±Ú©Øª Ú©Ø´Øª Ùˆ ØµÙ†Ø¹Øª Ø¯Ù‡Ø®Ø¯Ø§"
INITIAL_LAT = 31.534442
INITIAL_LON = 48.724416
INITIAL_ZOOM = 12

# --- File Paths (Relative to the script location or accessible via URL) ---
# Assuming the files are in the same repo or accessible path
CSV_FILE_PATH = 'Ø¨Ø±Ù†Ø§Ù…Ù‡_Ø±ÛŒØ²ÛŒ_Ø¨Ø§_Ù…Ø®ØªØµØ§Øª (1).csv' # Not used in the provided code logic, kept for reference
SERVICE_ACCOUNT_FILE = 'ee-esmaeilkiani13877-cfdea6eaf411 (4).json'
FARM_GEOJSON_PATH = 'farm_geodata_fixed.geojson'
ANALYSIS_CSV_PATH = 'Ù…Ø­Ø§Ø³Ø¨Ø§Øª 2.csv'


# --- GEE Authentication ---
@st.cache_resource # Cache the GEE initialization
def initialize_gee():
    """Initializes Google Earth Engine using the Service Account."""
    try:
        # Check if running locally or in a deployment environment
        if os.path.exists(SERVICE_ACCOUNT_FILE):
            credentials = ee.ServiceAccountCredentials(None, key_file=SERVICE_ACCOUNT_FILE)
            print("GEE Initialized Successfully using Service Account File.")
        else:
            # Attempt to use environment variables if service account file is not found (for deployment)
            # Ensure EE_ACCOUNT and EE_PRIVATE_KEY are set in your environment or secrets
            # For Streamlit Cloud, use secrets: https://docs.streamlit.io/streamlit-cloud/get-started/secrets-management
            try:
                # Assuming EE_ACCOUNT and EE_PRIVATE_KEY env vars are set
                # This part might need adjustment based on how secrets are exposed in the deployment env
                # In Streamlit Cloud secrets are typically accessed via st.secrets
                account_id = st.secrets["EE_ACCOUNT"] if "EE_ACCOUNT" in st.secrets else os.environ.get("EE_ACCOUNT")
                private_key_data = st.secrets["EE_PRIVATE_KEY"] if "EE_PRIVATE_KEY" in st.secrets else os.environ.get("EE_PRIVATE_KEY")

                if not account_id or not private_key_data:
                     raise ValueError("EE_ACCOUNT or EE_PRIVATE_KEY not found in secrets or environment variables.")

                # Need to write the private key data to a temporary file or use a direct method if GEE supports it
                # Writing to a temporary file is safer than hardcoding/exposing directly in memory for some GEE methods
                # A more robust approach would be using gee.auth.ServiceAccountCredentials.from_service_account_info
                # if available, but that requires the info in a dictionary format.
                # For simplicity, let's assume the file method is preferred or required by the current GEE library version.
                # THIS IS A SIMPLIFIED EXAMPLE - Secure handling of private keys in deployment is crucial.
                # A better way in Streamlit Cloud: save the JSON content in a single secret and load it.
                # Example: st.secrets["gee_auth_json"] containing the full JSON as a string.
                if "gee_auth_json" in st.secrets:
                    auth_info = json.loads(st.secrets["gee_auth_json"])
                    credentials = ee.ServiceAccountCredentials(auth_info['client_email'], None, private_key_id=auth_info['private_key_id'], private_key=auth_info['private_key'], token_uri=auth_info['token_uri'])
                    print("GEE Initialized Successfully using Streamlit Secrets JSON.")
                else:
                    # Fallback if separate EE_ACCOUNT and EE_PRIVATE_KEY are used (less common)
                    st.error("âŒ Secret 'gee_auth_json' not found. Please configure GEE credentials using the full JSON in a single secret.")
                    st.stop()
                    # This path is less likely to work directly with ServiceAccountCredentials(None, key_file)
                    # If using EE_ACCOUNT and EE_PRIVATE_KEY env vars, you might need to configure GEE differently.
                    # ee.Authenticate() or similar interactive auth might be needed if service account file isn't used.
                    # The current code relies on key_file or equivalent from secrets.
                    # For a production app on Streamlit Cloud, using a single JSON secret is the recommended way.
                    # credentials = ee.ServiceAccountCredentials(account_id, private_key_data) # This line is illustrative, might not work directly

            except Exception as e:
                 st.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø§Ø·Ù„Ø§Ø¹Ø§Øª Service Account Ø§Ø² Secrets ÛŒØ§ Environment Variables: {e}")
                 st.info("Ù„Ø·ÙØ§Ù‹ Ø§Ø² ØªÙ†Ø¸ÛŒÙ… ØµØ­ÛŒØ­ Secrets ÛŒØ§ Environment Variables Ø¨Ø±Ø§ÛŒ Google Earth Engine Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø­Ø§ØµÙ„ Ú©Ù†ÛŒØ¯.")
                 st.stop()


        ee.Initialize(credentials=credentials, opt_url='https://earthengine-highvolume.googleapis.com')
        print("GEE Initialized Successfully.")
        return True
    except ee.EEException as e:
        st.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§ØªØµØ§Ù„ Ø¨Ù‡ Google Earth Engine: {e}")
        st.error("Ù„Ø·ÙØ§Ù‹ Ø§Ø² ØµØ­Øª ÙØ§ÛŒÙ„ Service Account ÛŒØ§ ØªÙ†Ø¸ÛŒÙ…Ø§Øª Secrets/Environment Variables Ùˆ ÙØ¹Ø§Ù„ Ø¨ÙˆØ¯Ù† Ø¢Ù† Ø¯Ø± Ù¾Ø±ÙˆÚ˜Ù‡ GEE Ø§Ø·Ù…ÛŒÙ†Ø§Ù† Ø­Ø§ØµÙ„ Ú©Ù†ÛŒØ¯.")
        st.stop()
    except Exception as e:
        st.error(f"Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡ Ù‡Ù†Ú¯Ø§Ù… Ø§ØªØµØ§Ù„ Ø¨Ù‡ GEE: {e}")
        st.error(traceback.format_exc())
        st.stop()

# --- Load Farm Data from GeoJSON ---
@st.cache_data(show_spinner="Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø²Ø§Ø±Ø¹...")
def load_farm_data_from_geojson(geojson_path=FARM_GEOJSON_PATH):
    """Loads farm data from the specified GeoJSON file."""
    try:
        # Attempt to load from local path first
        if os.path.exists(geojson_path):
            with open(geojson_path, 'r', encoding='utf-8') as f:
                gj = json.load(f)
            print(f"Loaded GeoJSON from local path: {geojson_path}")
        else:
            # Fallback to fetching from a URL if specified (e.g., GitHub raw file)
            # Replace with your actual raw file URL if hosting elsewhere
            github_raw_url = f'https://raw.githubusercontent.com/esmaeilkiani13877/MonitoringSugarcane-13/main/{geojson_path}'
            try:
                response = requests.get(github_raw_url)
                response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)
                gj = response.json()
                print(f"Loaded GeoJSON from URL: {github_raw_url}")
            except requests.exceptions.RequestException as e:
                 st.error(f"âŒ ÙØ§ÛŒÙ„ '{geojson_path}' Ø¯Ø± Ù…Ø³ÛŒØ± Ù…Ø­Ù„ÛŒ ÛŒØ§ Ø§Ø² URL Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨ '{github_raw_url}' ÛŒØ§ÙØª Ù†Ø´Ø¯ ÛŒØ§ Ù‚Ø§Ø¨Ù„ Ø¯Ø³ØªØ±Ø³ÛŒ Ù†ÛŒØ³Øª: {e}")
                 st.stop()
            except json.JSONDecodeError:
                 st.error(f"âŒ ÙØ§ÛŒÙ„ '{geojson_path}' Ø§Ø² URL Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨ ÛŒÚ© ÙØ§ÛŒÙ„ GeoJSON Ù…Ø¹ØªØ¨Ø± Ù†ÛŒØ³Øª.")
                 st.stop()


        features = gj['features']
        # Extract properties and geometry
        records = []
        for feat in features:
            props = feat['properties']
            geom = feat['geometry']
            # For polygons, calculate centroid for display/analysis
            centroid_lon, centroid_lat = None, None
            if geom and geom['type'] == 'Polygon' and geom['coordinates']:
                try:
                    # Use ee.Geometry to calculate centroid - more robust
                    ee_geom = ee.Geometry(geom)
                    centroid = ee_geom.centroid(maxError=1).getInfo()['coordinates'] # Use a small maxError
                    centroid_lon, centroid_lat = centroid[0], centroid[1]
                except ee.EEException as e:
                    print(f"Warning: Could not calculate centroid for a polygon: {e}")
                    # Fallback to simple average if GEE centroid calculation fails
                    try:
                        coords = geom['coordinates'][0] # Assuming outer ring
                        lons = [pt[0] for pt in coords]
                        lats = [pt[1] for pt in coords]
                        centroid_lon = sum(lons) / len(lons) if lons else None
                        centroid_lat = sum(lats) / len(lats) if lats else None
                    except Exception as avg_e:
                        print(f"Warning: Simple average centroid calculation failed: {avg_e}")
                except Exception as general_e:
                     print(f"Warning: General error calculating centroid: {general_e}")

            record = {
                **props,
                'geometry_type': geom['type'] if geom else None,
                'coordinates': geom['coordinates'] if geom else None,
                'centroid_lon': centroid_lon,
                'centroid_lat': centroid_lat
            }
            records.append(record)
        df = pd.DataFrame(records)

        # Basic validation
        required_cols = ['Ù…Ø²Ø±Ø¹Ù‡', 'Ø±ÙˆØ²']
        if not all(col in df.columns for col in required_cols):
            st.error(f"âŒ ÙØ§ÛŒÙ„ GeoJSON Ø¨Ø§ÛŒØ¯ Ø´Ø§Ù…Ù„ Ø³ØªÙˆÙ†â€ŒÙ‡Ø§ÛŒ Ø¶Ø±ÙˆØ±ÛŒ Ø¨Ø§Ø´Ø¯: {', '.join(required_cols)}")
            st.stop()
        # Ensure centroid columns exist even if initially None
        if 'centroid_lon' not in df.columns:
             df['centroid_lon'] = None
        if 'centroid_lat' not in df.columns:
             df['centroid_lat'] = None

        # Drop rows where essential data for processing is missing
        initial_count = len(df)
        df = df.dropna(subset=['Ù…Ø²Ø±Ø¹Ù‡', 'centroid_lon', 'centroid_lat', 'Ø±ÙˆØ²'])
        dropped_count = initial_count - len(df)
        if dropped_count > 0:
            st.warning(f"âš ï¸ {dropped_count} Ø±Ú©ÙˆØ±Ø¯ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù†Ø§Ù…Ø¹ØªØ¨Ø± ÛŒØ§ Ø®Ø§Ù„ÛŒ Ø¯Ø± Ù†Ø§Ù… Ù…Ø²Ø±Ø¹Ù‡ØŒ Ù…Ø®ØªØµØ§Øª ÛŒØ§ Ø±ÙˆØ² Ø­Ø°Ù Ø´Ø¯Ù†Ø¯.")

        if df.empty:
            st.warning("âš ï¸ Ø¯Ø§Ø¯Ù‡ Ù…Ø¹ØªØ¨Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø²Ø§Ø±Ø¹ ÛŒØ§ÙØª Ù†Ø´Ø¯ (Ù¾Ø³ Ø§Ø² Ø­Ø°Ù Ø±Ú©ÙˆØ±Ø¯Ù‡Ø§ÛŒ Ø¨Ø¯ÙˆÙ† Ù†Ø§Ù… Ù…Ø²Ø±Ø¹Ù‡ØŒ Ù…Ø®ØªØµØ§Øª ÛŒØ§ Ø±ÙˆØ²).")
            st.stop()

        df['Ø±ÙˆØ²'] = df['Ø±ÙˆØ²'].astype(str).str.replace(r'\s+', ' ', regex=True).str.strip()
        # Handle potential non-string types in 'Ú¯Ø±ÙˆÙ‡' and 'ÙˆØ§Ø±ÛŒØªÙ‡' before strip
        df['Ú¯Ø±ÙˆÙ‡'] = df.get('Ú¯Ø±ÙˆÙ‡', pd.Series()).astype(str).str.strip()
        df['ÙˆØ§Ø±ÛŒØªÙ‡'] = df.get('ÙˆØ§Ø±ÛŒØªÙ‡', pd.Series()).astype(str).str.strip()


        st.success(f"âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ {len(df)} Ù…Ø²Ø±Ø¹Ù‡ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ø² GeoJSON Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯.")
        return df
    except FileNotFoundError:
        st.error(f"âŒ ÙØ§ÛŒÙ„ '{geojson_path}' ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ ÙØ§ÛŒÙ„ GeoJSON Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø²Ø§Ø±Ø¹ Ø±Ø§ Ø¯Ø± Ù…Ø³ÛŒØ± ØµØ­ÛŒØ­ Ù‚Ø±Ø§Ø± Ø¯Ù‡ÛŒØ¯.")
        st.stop()
        return pd.DataFrame() # Return empty DataFrame on error
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÛŒØ§ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ GeoJSON: {e}")
        st.error(traceback.format_exc())
        st.stop()
        return pd.DataFrame() # Return empty DataFrame on error


# --- Load Analysis Data ---
@st.cache_data(show_spinner="Ø¯Ø± Ø­Ø§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ø§Øª...")
def load_analysis_data(csv_path=ANALYSIS_CSV_PATH):
    """Loads and preprocesses data from the analysis CSV file."""
    try:
        # Attempt to load from local path first
        if os.path.exists(csv_path):
            with open(csv_path, 'r', encoding='utf-8') as f:
                lines = f.readlines()
            print(f"Loaded Analysis CSV from local path: {csv_path}")
        else:
             # Fallback to fetching from a URL (e.g., GitHub raw file)
             github_raw_url = f'https://raw.githubusercontent.com/esmaeilkiani13877/MonitoringSugarcane-13/main/{csv_path}'
             try:
                 response = requests.get(github_raw_url)
                 response.raise_for_status()
                 lines = response.text.splitlines()
                 print(f"Loaded Analysis CSV from URL: {github_raw_url}")
             except requests.exceptions.RequestException as e:
                 st.error(f"âŒ ÙØ§ÛŒÙ„ '{csv_path}' Ø¯Ø± Ù…Ø³ÛŒØ± Ù…Ø­Ù„ÛŒ ÛŒØ§ Ø§Ø² URL Ú¯ÛŒØªâ€ŒÙ‡Ø§Ø¨ '{github_raw_url}' ÛŒØ§ÙØª Ù†Ø´Ø¯ ÛŒØ§ Ù‚Ø§Ø¨Ù„ Ø¯Ø³ØªØ±Ø³ÛŒ Ù†ÛŒØ³Øª: {e}")
                 st.stop()
                 return None, None


        # Find the headers and split points
        headers_indices = [i for i, line in enumerate(lines) if 'Ø§Ø¯Ø§Ø±Ù‡,Ø³Ù†,' in line or 'ØªÙˆÙ„ÛŒØ¯,Ø³Ù†,' in line]
        if len(headers_indices) < 1: # Must find at least one header
            st.error(f"âŒ Ø³Ø§Ø®ØªØ§Ø± ÙØ§ÛŒÙ„ '{csv_path}' Ù‚Ø§Ø¨Ù„ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†ÛŒØ³Øª. Ù‡Ø¯Ø±Ù‡Ø§ÛŒ Ù…ÙˆØ±Ø¯ Ø§Ù†ØªØ¸Ø§Ø± ('Ø§Ø¯Ø§Ø±Ù‡,Ø³Ù†,' ÛŒØ§ 'ØªÙˆÙ„ÛŒØ¯,Ø³Ù†,') ÛŒØ§ÙØª Ù†Ø´Ø¯.")
            st.stop()
            return None, None

        section1_start = headers_indices[0] + 1
        section2_start = None
        if len(headers_indices) > 1:
            section2_start = headers_indices[1] + 1

        # Read the first section (Area)
        # Determine nrows by looking for the next header or 'Grand Total' or end of file
        end_row_area = len(lines)
        if section2_start:
            end_row_area = section2_start - 2 # Stop before the next header line
        else:
             # If no second section, look for 'Grand Total' or a mostly empty line after section1_start
             for i in range(section1_start, len(lines)):
                 if "Grand Total" in lines[i] or len(lines[i].strip()) < 5:
                     end_row_area = i
                     break

        nrows_area = (end_row_area - section1_start) if end_row_area > section1_start else 0
        df_area = None
        if nrows_area > 0:
             df_area = pd.read_csv(BytesIO("\n".join(lines[headers_indices[0]:end_row_area]).encode('utf-8')), encoding='utf-8')


        # Read the second section (Production) if found
        df_prod = None
        if section2_start:
            # Find the end of the second section (look for 'Grand Total' or end of file)
            end_row_prod = len(lines)
            for i in range(section2_start, len(lines)):
                if "Grand Total" in lines[i] or len(lines[i].strip()) < 5:
                    end_row_prod = i
                    break
            nrows_prod = (end_row_prod - section2_start) if end_row_prod > section2_start else 0
            if nrows_prod > 0:
                 df_prod = pd.read_csv(BytesIO("\n".join(lines[headers_indices[1]:end_row_prod]).encode('utf-8')), encoding='utf-8')


        # --- Preprocessing Function ---
        def preprocess_df(df, section_name):
            if df is None or df.empty:
                return None
            # Ensure 'Ø§Ø¯Ø§Ø±Ù‡' is the first column if it got misplaced or is unnamed
            if df.columns.tolist() and 'Ø§Ø¯Ø§Ø±Ù‡' not in df.columns:
                df.rename(columns={df.columns[0]: 'Ø§Ø¯Ø§Ø±Ù‡'}, inplace=True)
                # Clean column names (remove leading/trailing spaces, potential BOM)
                df.columns = df.columns.str.strip()

            # Check for required columns after potential renaming
            if not all(col in df.columns for col in ['Ø§Ø¯Ø§Ø±Ù‡', 'Ø³Ù†']):
                 st.warning(f"âš ï¸ Ø³ØªÙˆÙ† Ù‡Ø§ÛŒ Ø¶Ø±ÙˆØ±ÛŒ 'Ø§Ø¯Ø§Ø±Ù‡' ÛŒØ§ 'Ø³Ù†' Ø¯Ø± Ø¨Ø®Ø´ '{section_name}' ÛŒØ§ÙØª Ù†Ø´Ø¯.")
                 return None

            # Forward fill 'Ø§Ø¯Ø§Ø±Ù‡' to fill down merged cells
            df['Ø§Ø¯Ø§Ø±Ù‡'] = df['Ø§Ø¯Ø§Ø±Ù‡'].ffill()

            # Filter out 'total' and 'Grand Total' rows in 'Ø³Ù†' and 'Ø§Ø¯Ø§Ø±Ù‡' columns
            df = df[~df['Ø³Ù†'].astype(str).str.contains('total', case=False, na=False)]
            # Also filter rows in 'Ø§Ø¯Ø§Ø±Ù‡' that contain 'total' or 'Ø¯Ù‡Ø®Ø¯Ø§' (case-insensitive)
            df = df[~df['Ø§Ø¯Ø§Ø±Ù‡'].astype(str).str.contains('total|Ø¯Ù‡Ø®Ø¯Ø§', case=False, na=False)]

            # Remove rows where 'Ø§Ø¯Ø§Ø±Ù‡' is NaN after ffill (first rows before a number)
            df = df.dropna(subset=['Ø§Ø¯Ø§Ø±Ù‡'])

             # Convert 'Ø§Ø¯Ø§Ø±Ù‡' to integer where possible, coercing errors
            df['Ø§Ø¯Ø§Ø±Ù‡'] = pd.to_numeric(df['Ø§Ø¯Ø§Ø±Ù‡'], errors='coerce').astype('Int64') # Use nullable integer type
            df = df.dropna(subset=['Ø§Ø¯Ø§Ø±Ù‡']) # Drop if conversion resulted in NaN

            # Convert numeric columns, coerce errors to NaN
            # Exclude 'Ø§Ø¯Ø§Ø±Ù‡' and 'Ø³Ù†' and any potential 'Ø¯Ø±ØµØ¯' or 'Grand Total' columns that survived
            value_cols = [col for col in df.columns if col not in ['Ø§Ø¯Ø§Ø±Ù‡', 'Ø³Ù†', 'Ø¯Ø±ØµØ¯', 'Grand Total']]
            for col in value_cols:
                # Attempt to convert to float, coercing errors
                df[col] = pd.to_numeric(df[col], errors='coerce')

            # Drop columns that are mostly NaN after conversion (e.g., text columns mistaken as value cols)
            # Adjust threshold based on expected data structure
            df = df.dropna(axis=1, thresh=len(df)*0.5) # Drop columns with more than 50% NaNs

            # Drop Grand Total and Ø¯Ø±ØµØ¯ columns if they exist and weren't dropped by NA threshold
            df = df.drop(columns=['Grand Total', 'Ø¯Ø±ØµØ¯'], errors='ignore')

            # Set multi-index for easier access
            if 'Ø§Ø¯Ø§Ø±Ù‡' in df.columns and 'Ø³Ù†' in df.columns:
                try:
                    df = df.set_index(['Ø§Ø¯Ø§Ø±Ù‡', 'Ø³Ù†'])
                except ValueError as e:
                     st.warning(f"âš ï¸ Ø®Ø·Ø§ÛŒ ØªÙ†Ø¸ÛŒÙ… Ø§ÛŒÙ†Ø¯Ú©Ø³ Ú†Ù†Ø¯Ú¯Ø§Ù†Ù‡ Ø¯Ø± Ø¨Ø®Ø´ '{section_name}': {e}. Ø§Ø¯Ø§Ù…Ù‡ Ø¨Ø¯ÙˆÙ† Ø§ÛŒÙ†Ø¯Ú©Ø³.")
                     # If setting index fails (e.g., duplicate index combinations), return without index
            else:
                 st.warning(f"âš ï¸ Ø³ØªÙˆÙ† Ù‡Ø§ÛŒ 'Ø§Ø¯Ø§Ø±Ù‡' ÛŒØ§ 'Ø³Ù†' Ø¨Ø±Ø§ÛŒ ØªÙ†Ø¸ÛŒÙ… Ø§ÛŒÙ†Ø¯Ú©Ø³ Ú†Ù†Ø¯Ú¯Ø§Ù†Ù‡ Ø¯Ø± Ø¨Ø®Ø´ '{section_name}' ÛŒØ§ÙØª Ù†Ø´Ø¯.")


            return df

        df_area_processed = preprocess_df(df_area, "Ù…Ø³Ø§Ø­Øª")
        df_prod_processed = preprocess_df(df_prod, "ØªÙˆÙ„ÛŒØ¯")

        if df_area_processed is not None or df_prod_processed is not None:
             st.success(f"âœ… Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ùˆ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯.")
        else:
             st.warning("âš ï¸ Ù‡ÛŒÚ† Ø¯Ø§Ø¯Ù‡ Ù…Ø¹ØªØ¨Ø±ÛŒ Ø§Ø² ÙØ§ÛŒÙ„ Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÛŒØ§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù†Ø´Ø¯.")

        return df_area_processed, df_prod_processed

    except FileNotFoundError:
        st.error(f"âŒ ÙØ§ÛŒÙ„ '{csv_path}' ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ ÙØ§ÛŒÙ„ CSV Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ø§Øª Ø±Ø§ Ø¯Ø± Ù…Ø³ÛŒØ± ØµØ­ÛŒØ­ Ù‚Ø±Ø§Ø± Ø¯Ù‡ÛŒØ¯.")
        return None, None
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÛŒØ§ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ Ù…Ø­Ø§Ø³Ø¨Ø§Øª CSV: {e}")
        st.error(traceback.format_exc())
        return None, None


# Initialize GEE and Load Data
if initialize_gee():
    farm_data_df = load_farm_data_from_geojson()
    # Load Analysis Data - Load this regardless of selected farm/day
    analysis_area_df, analysis_prod_df = load_analysis_data()
else:
    st.stop() # Stop if GEE initialization failed


# ==============================================================================
# Sidebar Filters
# ==============================================================================
st.sidebar.header("ØªÙ†Ø¸ÛŒÙ…Ø§Øª Ù†Ù…Ø§ÛŒØ´")

# --- Day of the Week Selection ---
if not farm_data_df.empty:
    available_days = sorted(farm_data_df['Ø±ÙˆØ²'].unique())
    if not available_days:
         st.sidebar.warning("Ù‡ÛŒÚ† Ø±ÙˆØ² Ù‡ÙØªÙ‡â€ŒØ§ÛŒ Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø²Ø§Ø±Ø¹ ÛŒØ§ÙØª Ù†Ø´Ø¯.")
         selected_day = None
    else:
        selected_day = st.sidebar.selectbox(
            "ğŸ“… Ø±ÙˆØ² Ù‡ÙØªÙ‡ Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯:",
            options=available_days,
            index=0, # Default to the first day
            help="Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø²Ø§Ø±Ø¹ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø§ÛŒÙ† Ø±ÙˆØ² ÙÛŒÙ„ØªØ± Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯."
        )
else:
     st.sidebar.warning("Ø¯Ø§Ø¯Ù‡ Ù…Ø²Ø§Ø±Ø¹ Ø¨Ø±Ø§ÛŒ ÙÛŒÙ„ØªØ± Ø±ÙˆØ² Ù‡ÙØªÙ‡ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª.")
     selected_day = None


# --- Filter Data Based on Selected Day ---
filtered_farms_df = pd.DataFrame()
if selected_day:
    filtered_farms_df = farm_data_df[farm_data_df['Ø±ÙˆØ²'] == selected_day].copy()

if filtered_farms_df.empty:
    st.warning(f"âš ï¸ Ù‡ÛŒÚ† Ù…Ø²Ø±Ø¹Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø±ÙˆØ² '{selected_day}' ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ Ø±ÙˆØ² Ø¯ÛŒÚ¯Ø±ÛŒ Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯ ÛŒØ§ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø²Ø§Ø±Ø¹ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.")
    # Prevent further processing if no farms
    selected_farm_name = "Ù‡Ù…Ù‡ Ù…Ø²Ø§Ø±Ø¹" # Default selection
    available_farms = []
else:
    # --- Farm Selection ---
    available_farms = sorted(filtered_farms_df['Ù…Ø²Ø±Ø¹Ù‡'].unique())
    # Add an option for "All Farms"
    farm_options = ["Ù‡Ù…Ù‡ Ù…Ø²Ø§Ø±Ø¹"] + available_farms
    selected_farm_name = st.sidebar.selectbox(
        "ğŸŒ¾ Ù…Ø²Ø±Ø¹Ù‡ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯:",
        options=farm_options,
        index=0, # Default to "All Farms"
        help="Ù…Ø²Ø±Ø¹Ù‡â€ŒØ§ÛŒ Ú©Ù‡ Ù…ÛŒâ€ŒØ®ÙˆØ§Ù‡ÛŒØ¯ Ø¬Ø²Ø¦ÛŒØ§Øª Ø¢Ù† Ø±Ø§ Ø¨Ø¨ÛŒÙ†ÛŒØ¯ ÛŒØ§ 'Ù‡Ù…Ù‡ Ù…Ø²Ø§Ø±Ø¹' Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ú©Ù„ÛŒ."
    )

# --- Index Selection ---
index_options = {
    "NDVI": "Ø´Ø§Ø®Øµ Ù¾ÙˆØ´Ø´ Ú¯ÛŒØ§Ù‡ÛŒ ØªÙØ§Ø¶Ù„ÛŒ Ù†Ø±Ù…Ø§Ù„ Ø´Ø¯Ù‡",
    "EVI": "Ø´Ø§Ø®Øµ Ù¾ÙˆØ´Ø´ Ú¯ÛŒØ§Ù‡ÛŒ Ø¨Ù‡Ø¨ÙˆØ¯ ÛŒØ§ÙØªÙ‡",
    "NDMI": "Ø´Ø§Ø®Øµ Ø±Ø·ÙˆØ¨Øª ØªÙØ§Ø¶Ù„ÛŒ Ù†Ø±Ù…Ø§Ù„ Ø´Ø¯Ù‡ (ÙˆØ¶Ø¹ÛŒØª Ø¢Ø¨ÛŒ)",
    "LAI": "Ø´Ø§Ø®Øµ Ø³Ø·Ø­ Ø¨Ø±Ú¯ (ØªØ®Ù…ÛŒÙ†ÛŒ)",
    "MSI": "Ø´Ø§Ø®Øµ ØªÙ†Ø´ Ø±Ø·ÙˆØ¨ØªÛŒ",
    "CVI": "Ø´Ø§Ø®Øµ Ú©Ù„Ø±ÙˆÙÛŒÙ„ (ØªØ®Ù…ÛŒÙ†ÛŒ)",
    "SAVI": "Ø´Ø§Ø®Øµ Ù¾ÙˆØ´Ø´ Ú¯ÛŒØ§Ù‡ÛŒ ØªØ¹Ø¯ÛŒÙ„ Ø´Ø¯Ù‡ Ø¨Ø§ Ø®Ø§Ú©",
}
selected_index = st.sidebar.selectbox(
    "ğŸ“ˆ Ø´Ø§Ø®Øµ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø±ÙˆÛŒ Ù†Ù‚Ø´Ù‡ Ùˆ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ:",
    options=list(index_options.keys()),
    format_func=lambda x: f"{x} ({index_options[x]})",
    index=0 # Default to NDVI
)

# --- Date Range Calculation ---
today = datetime.date.today()
# Find the most recent date corresponding to the selected day of the week
# Map Persian day names to Python's weekday() (Monday=0, Sunday=6)
persian_to_weekday = {
    "Ø´Ù†Ø¨Ù‡": 5,
    "ÛŒÚ©Ø´Ù†Ø¨Ù‡": 6,
    "Ø¯ÙˆØ´Ù†Ø¨Ù‡": 0,
    "Ø³Ù‡ Ø´Ù†Ø¨Ù‡": 1,
    "Ú†Ù‡Ø§Ø±Ø´Ù†Ø¨Ù‡": 2,
    "Ù¾Ù†Ø¬Ø´Ù†Ø¨Ù‡": 3,
    "Ø¬Ù…Ø¹Ù‡": 4,
}

start_date_current_str = None
end_date_current_str = None
start_date_previous_str = None
end_date_previous_str = None

if selected_day:
    try:
        target_weekday = persian_to_weekday[selected_day.strip()] # Ensure no leading/trailing spaces
        today_weekday = today.weekday() # Monday is 0, Sunday is 6

        # Calculate days difference to get to the most recent selected day
        days_ago = (today_weekday - target_weekday + 7) % 7
        # If today is the target day, days_ago is 0. If the target day was yesterday, days_ago is 1, etc.

        end_date_current = today - datetime.timedelta(days=days_ago)

        # Ensure the current week starts correctly based on the calculated end date
        start_date_current = end_date_current - datetime.timedelta(days=6)

        end_date_previous = start_date_current - datetime.timedelta(days=1)
        start_date_previous = end_date_previous - datetime.timedelta(days=6)

        # Convert to strings for GEE
        start_date_current_str = start_date_current.strftime('%Y-%m-%d')
        end_date_current_str = end_date_current.strftime('%Y-%m-%d')
        start_date_previous_str = start_date_previous.strftime('%Y-%m-%d')
        end_date_previous_str = end_date_previous.strftime('%Y-%m-%d')

        st.sidebar.info(f"**Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ ÙØ¹Ù„ÛŒ:** {start_date_current_str} ØªØ§ {end_date_current_str}")
        st.sidebar.info(f"**Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ù‚Ø¨Ù„ÛŒ:** {start_date_previous_str} ØªØ§ {end_date_previous_str}")

    except KeyError:
        st.sidebar.error(f"Ù†Ø§Ù… Ø±ÙˆØ² Ù‡ÙØªÙ‡ '{selected_day}' Ù‚Ø§Ø¨Ù„ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ù†ÛŒØ³Øª.")
        # selected_day = None # Reset selected_day to prevent further errors
    except Exception as e:
        st.sidebar.error(f"Ø®Ø·Ø§ Ø¯Ø± Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ: {e}")
        st.error(traceback.format_exc())
        # selected_day = None # Reset selected_day to prevent further errors


# ==============================================================================
# Google Earth Engine Functions
# ==============================================================================

# --- Cloud Masking Function for Sentinel-2 ---
def maskS2clouds(image):
    """Masks clouds in a Sentinel-2 SR image using the QA band and SCL band."""
    qa = image.select('QA60')
    # Bits 10 and 11 are clouds and cirrus, respectively.
    cloudBitMask = 1 << 10
    cirrusBitMask = 1 << 11
    # Both flags should be set to zero, indicating clear conditions.
    mask_qa = qa.bitwiseAnd(cloudBitMask).eq(0).And(
             qa.bitwiseAnd(cirrusBitMask).eq(0))

    # SCL (Scene Classification Layer) band - more detailed masking
    scl = image.select('SCL')
    # Classes to mask out:
    # 3 = Cloud medium probability
    # 8 = Cloud high probability
    # 9 = Cirrus
    # 10 = Snow/Ice (unless relevant to crop) - Masking for sugarcane
    # 11 = Cloud shadow
    # Good classes to keep:
    # 4 = Vegetation
    # 5 = Not Vegetated
    # 6 = Water (could be field puddles or irrigation canals, maybe keep?) - Keep for now
    # 7 = Unclassified (potentially risky, maybe mask?) - Mask for safety
    # 1 = Saturated or Defective - Mask
    # 2 = Dark Area Pixels - Mask
    # 0 = No Data - Mask

    masked_classes = [0, 1, 2, 3, 7, 8, 9, 10, 11]
    mask_scl = scl.remap(masked_classes, [0] * len(masked_classes), 1) # Map bad classes to 0, others to 1

    # Combine masks
    final_mask = mask_qa.And(mask_scl)

    # Scale and offset factors for Sentinel-2 SR bands
    opticalBands = image.select('B.*').multiply(0.0001)

    # Apply the final mask and add scaled bands
    return image.addBands(opticalBands, None, True)\
                .updateMask(final_mask)


# --- Index Calculation Functions ---
def add_indices(image):
    """Calculates and adds various indices as bands to an image."""
    # Use scaled bands for calculations
    red = image.select('B4')
    nir = image.select('B8')
    blue = image.select('B2')
    green = image.select('B3')
    swir1 = image.select('B11')

    # NDVI: (NIR - Red) / (NIR + Red)
    ndvi = image.normalizedDifference(['B8', 'B4']).rename('NDVI')

    # EVI: 2.5 * (NIR - Red) / (NIR + 6 * Red - 7.5 * Blue + 1)
    evi = image.expression(
        '2.5 * (NIR - RED) / (NIR + 6 * RED - 7.5 * BLUE + 1)', {
            'NIR': nir,
            'RED': red,
            'BLUE': blue
        }).rename('EVI')

    # NDMI (Normalized Difference Moisture Index): (NIR - SWIR1) / (NIR + SWIR1)
    ndmi = image.normalizedDifference(['B8', 'B11']).rename('NDMI')

    # SAVI (Soil-Adjusted Vegetation Index): ((NIR - Red) / (NIR + Red + L)) * (1 + L) | L=0.5
    savi = image.expression(
        '((NIR - RED) / (NIR + RED + L)) * (1 + L)',
        {
            'NIR': nir,
            'RED': red,
            'L': 0.5
        }
    ).rename('SAVI')

    # MSI (Moisture Stress Index): SWIR1 / NIR
    # Handle potential division by zero if NIR is 0
    nir_safe = nir.max(ee.Image(0.0001))
    msi = image.expression('SWIR1 / NIR', {
        'SWIR1': swir1,
        'NIR': nir_safe
    }).rename('MSI')

    # LAI (Leaf Area Index) - Simple estimation using EVI (Needs calibration for accuracy)
    # Using the formula: LAI = 3.618 * EVI - 0.118 (from literature, may need local calibration)
    lai = evi.multiply(3.618).subtract(0.118).rename('LAI').reproject(crs=image.projection().crs(), scale=10) # Reproject after calculation

    # CVI (Chlorophyll Vegetation Index) - (NIR / Green) * (Red / Green)
    # Handle potential division by zero if Green band is 0
    green_safe = green.max(ee.Image(0.0001))
    cvi = image.expression('(NIR / GREEN) * (RED / GREEN)', {
        'NIR': nir,
        'GREEN': green_safe,
        'RED': red
    }).rename('CVI').reproject(crs=image.projection().crs(), scale=10) # Reproject after calculation


    return image.addBands([ndvi, evi, ndmi, msi, lai, cvi, savi]) # Add calculated indices

# --- Function to get processed image for a date range and geometry ---
# Increased cache persistence to reduce re-computation
@st.cache_data(show_spinner="Ø¯Ø± Ø­Ø§Ù„ Ù¾Ø±Ø¯Ø§Ø²Ø´ ØªØµØ§ÙˆÛŒØ± Ù…Ø§Ù‡ÙˆØ§Ø±Ù‡â€ŒØ§ÛŒ...", persist="ì•±") # Persist across app restarts
def get_processed_image(_geometry, start_date, end_date, index_name):
    """
    Gets cloud-masked, index-calculated Sentinel-2 median composite for a given geometry and date range.
    _geometry: ee.Geometry (Point or Polygon)
    start_date, end_date: YYYY-MM-DD strings
    index_name: Name of the primary index band to return (e.g., 'NDVI')
    """
    if _geometry is None:
        return None, "Ù‡Ù†Ø¯Ø³Ù‡ Ù…Ø¹ØªØ¨Ø± Ø¨Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ GEE ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯."
    try:
        s2_sr_col = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')
                     .filterBounds(_geometry)
                     .filterDate(start_date, end_date)
                     .map(maskS2clouds))

        # Check if any images are available after filtering
        # Use aggregate_array and getInfo() - more robust than size().getInfo() in some cases
        image_list = s2_sr_col.aggregate_array('system:index').getInfo()
        if not image_list:
             return None, f"Ù‡ÛŒÚ† ØªØµÙˆÛŒØ± Sentinel-2 Ø¨Ø¯ÙˆÙ† Ø§Ø¨Ø± Ø¯Ø± Ø¨Ø§Ø²Ù‡ {start_date} ØªØ§ {end_date} ÛŒØ§ÙØª Ù†Ø´Ø¯."

        # Calculate indices for each image in the collection
        indexed_col = s2_sr_col.map(add_indices)

        # Create a median composite image
        median_image = indexed_col.median() # Use median to reduce noise/outliers

        # Select the desired index band
        # Ensure the selected index band exists
        available_bands = median_image.bandNames().getInfo()
        if index_name not in available_bands:
             return None, f"Ø´Ø§Ø®Øµ '{index_name}' Ø¯Ø± ØªØµØ§ÙˆÛŒØ± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ø¨Ø§Ù†Ø¯Ù‡Ø§ÛŒ Ù…ÙˆØ¬ÙˆØ¯: {', '.join(available_bands)}"

        output_image = median_image.select(index_name)

        return output_image, None # Return the image and no error message
    except ee.EEException as e:
        # Handle GEE specific errors
        error_message = f"Ø®Ø·Ø§ÛŒ Google Earth Engine: {e}"
        st.error(error_message)
        # Try to extract more details if available
        try:
            # GEE errors sometimes have details nested
            error_details = e.args[0] if e.args else str(e)
            if isinstance(error_details, str) and 'computation timed out' in error_details.lower():
                 error_message += "\n(Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ø­Ø¬Ù… Ø¨Ø§Ù„Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ ÛŒØ§ Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ Ø·ÙˆÙ„Ø§Ù†ÛŒ)"
            elif isinstance(error_details, str) and 'user memory limit exceeded' in error_details.lower():
                 error_message += "\n(Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ù…Ù†Ø·Ù‚Ù‡ Ø¨Ø²Ø±Ú¯ ÛŒØ§ Ø¹Ù…Ù„ÛŒØ§Øª Ù¾ÛŒÚ†ÛŒØ¯Ù‡)"
        except Exception:
            pass # Ignore errors during error detail extraction
        return None, error_message
    except Exception as e:
        error_message = f"Ø®Ø·Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ GEE: {e}\n{traceback.format_exc()}"
        st.error(error_message)
        return None, error_message

# --- Function to get time series data for a point ---
# Increased cache persistence
@st.cache_data(show_spinner="Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø±ÛŒØ§ÙØª Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ Ø´Ø§Ø®Øµ...", persist="ì•±") # Persist across app restarts
def get_index_time_series(_point_geom, index_name, start_date='2023-01-01', end_date=today.strftime('%Y-%m-%d')):
    """Gets a time series of a specified index for a point geometry."""
    if _point_geom is None:
        return pd.DataFrame(columns=['date', index_name]), "Ù‡Ù†Ø¯Ø³Ù‡ Ù†Ù‚Ø·Ù‡â€ŒØ§ÛŒ Ù…Ø¹ØªØ¨Ø± Ø¨Ø±Ø§ÛŒ Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯."
    try:
        s2_sr_col = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')
                     .filterBounds(_point_geom)
                     .filterDate(start_date, end_date)
                     .map(maskS2clouds)
                     .map(add_indices))

        # Check if the index band exists in the first image (assuming consistency)
        # Or check after mapping add_indices
        # Example check (requires fetching info):
        # first_image = s2_sr_col.first()
        # if first_image:
        #      available_bands = first_image.bandNames().getInfo()
        #      if index_name not in available_bands:
        #          return pd.DataFrame(columns=['date', index_name]), f"Ø´Ø§Ø®Øµ '{index_name}' Ø¯Ø± ØªØµØ§ÙˆÛŒØ± ÛŒØ§ÙØª Ù†Ø´Ø¯."

        def extract_value(image):
            # Ensure the index band is selected to avoid issues with missing bands
            # Use reduceRegion for points; scale should match sensor resolution (e.g., 10m for S2)
            # Check if the band exists before reducing
            bands = image.bandNames()
            if bands.contains(index_name):
                value = image.select(index_name).reduceRegion(
                    reducer=ee.Reducer.first(), # Use 'first' or 'mean' if point covers multiple pixels
                    geometry=_point_geom,
                    scale=10, # Scale in meters (10m for Sentinel-2 RGB/NIR)
                    bestEffort=True # Use bestEffort for potentially complex geometries
                ).get(index_name)
                # Return a feature with the value and the image date
                return ee.Feature(None, {
                    'date': image.date().format('YYYY-MM-dd'),
                    index_name: value
                })
            else:
                 return ee.Feature(None, {
                    'date': image.date().format('YYYY-MM-dd'),
                    index_name: None # Return None if band is missing for this image
                })


        # Map over the collection and remove features with null values for the index
        ts_features = s2_sr_col.map(extract_value).filter(ee.Filter.notNull([index_name]))

        # Convert the FeatureCollection to a list of dictionaries
        ts_info = ts_features.getInfo()['features']

        if not ts_info:
            return pd.DataFrame(columns=['date', index_name]), "Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ Ø¯Ø± Ø¨Ø§Ø²Ù‡ Ù…Ø´Ø®Øµ Ø´Ø¯Ù‡ ÛŒØ§ÙØª Ù†Ø´Ø¯."

        # Convert to Pandas DataFrame
        ts_data = []
        for f in ts_info:
            properties = f['properties']
            # Ensure the index_name key exists before accessing
            if index_name in properties:
                 ts_data.append({'date': properties['date'], index_name: properties[index_name]})
            else:
                 # This case should ideally be caught by filter, but adding safety
                 print(f"Warning: Index '{index_name}' not found in properties for date {properties.get('date', 'N/A')}")


        ts_df = pd.DataFrame(ts_data)
        if ts_df.empty:
             return pd.DataFrame(columns=['date', index_name]), "Ø¯Ø§Ø¯Ù‡ Ù…Ø¹ØªØ¨Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯ (Ù¾Ø³ Ø§Ø² Ø­Ø°Ù Ù…Ù‚Ø§Ø¯ÛŒØ± Ø®Ø§Ù„ÛŒ)."

        ts_df['date'] = pd.to_datetime(ts_df['date'])
        ts_df = ts_df.sort_values('date').set_index('date')

        return ts_df, None # Return DataFrame and no error
    except ee.EEException as e:
        error_message = f"Ø®Ø·Ø§ÛŒ GEE Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ: {e}"
        st.error(error_message)
        return pd.DataFrame(columns=['date', index_name]), error_message
    except Exception as e:
        error_message = f"Ø®Ø·Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ: {e}\n{traceback.format_exc()}"
        st.error(error_message)
        return pd.DataFrame(columns=['date', index_name]), error_message


# ==============================================================================
# Function to get all relevant indices for a farm point for two periods
# ==============================================================================
# Increased cache persistence
@st.cache_data(show_spinner="Ø¯Ø± Ø­Ø§Ù„ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ÛŒ Ù†ÛŒØ§Ø²Ø³Ù†Ø¬ÛŒ...", persist="ì•±") # Persist across app restarts
def get_farm_needs_data(_point_geom, start_curr, end_curr, start_prev, end_prev):
    """Calculates mean NDVI, NDMI, EVI, SAVI for current and previous periods."""
    results = {
        'NDVI_curr': None, 'NDMI_curr': None, 'EVI_curr': None, 'SAVI_curr': None,
        'NDVI_prev': None, 'NDMI_prev': None, 'EVI_prev': None, 'SAVI_prev': None,
        'error': None
    }
    indices_to_get = ['NDVI', 'NDMI', 'EVI', 'SAVI']

    def get_mean_values_for_period(start, end):
        period_values = {index: None for index in indices_to_get}
        error_msg = None
        if _point_geom is None:
             return period_values, "Ù‡Ù†Ø¯Ø³Ù‡ Ù†Ù‚Ø·Ù‡â€ŒØ§ÛŒ Ù…Ø¹ØªØ¨Ø± ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯."
        try:
            # Get median composite image with all indices calculated
            s2_sr_col = (ee.ImageCollection('COPERNICUS/S2_SR_HARMONIZED')
                         .filterBounds(_point_geom)
                         .filterDate(start, end)
                         .map(maskS2clouds)
                         .map(add_indices))

            # Check if any images are available
            image_list = s2_sr_col.aggregate_array('system:index').getInfo()
            if not image_list:
                return period_values, f"Ù‡ÛŒÚ† ØªØµÙˆÛŒØ±ÛŒ Ø¯Ø± Ø¨Ø§Ø²Ù‡ {start}-{end} ÛŒØ§ÙØª Ù†Ø´Ø¯"

            median_image = s2_sr_col.median()

            # Reduce region to get the mean value at the point for all desired indices
            # Select only the indices we want to get values for
            selected_bands = median_image.select(indices_to_get)
            mean_dict = selected_bands.reduceRegion(
                reducer=ee.Reducer.mean(),
                geometry=_point_geom,
                scale=10,  # Scale in meters
                bestEffort=True # Use bestEffort
            ).getInfo()

            if mean_dict:
                for index in indices_to_get:
                    # Check if the index exists and is not None in the result
                    if index in mean_dict and mean_dict[index] is not None:
                         period_values[index] = mean_dict[index]
                    # else: print(f"Warning: {index} not found or is None in mean_dict for {start}-{end}")

            return period_values, None
        except ee.EEException as e:
            error_msg = f"Ø®Ø·Ø§ÛŒ GEE Ø¯Ø± Ø¨Ø§Ø²Ù‡ {start}-{end}: {e}"
            st.error(error_msg) # Display GEE error
            return period_values, error_msg
        except Exception as e:
            error_msg = f"Ø®Ø·Ø§ÛŒ Ù†Ø§Ø´Ù†Ø§Ø®ØªÙ‡ Ø¯Ø± Ø¨Ø§Ø²Ù‡ {start}-{end}: {e}\n{traceback.format_exc()}"
            st.error(error_msg) # Display unknown error
            return period_values, error_msg

    # Get data for current period
    curr_values, err_curr = get_mean_values_for_period(start_curr, end_curr)
    if err_curr:
        results['error'] = f"Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø²Ù‡ Ø¬Ø§Ø±ÛŒ: {err_curr}"
    results['NDVI_curr'] = curr_values['NDVI']
    results['NDMI_curr'] = curr_values['NDMI']
    results['EVI_curr'] = curr_values['EVI']
    results['SAVI_curr'] = curr_values['SAVI']


    # Get data for previous period
    prev_values, err_prev = get_mean_values_for_period(start_prev, end_prev)
    if err_prev:
        results['error'] = f"{results.get('error', '')}\nØ®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø²Ù‡ Ù‚Ø¨Ù„ÛŒ: {err_prev}".strip() # Append errors with separator
    results['NDVI_prev'] = prev_values['NDVI']
    results['NDMI_prev'] = prev_values['NDMI']
    results['EVI_prev'] = prev_values['EVI']
    results['SAVI_prev'] = prev_values['SAVI']

    return results

# ==============================================================================
# Gemini AI Helper Functions
# ==============================================================================

# Configure Gemini API
@st.cache_resource
def configure_gemini():
    """Configures the Gemini API client using Streamlit secrets."""
    try:
        # Recommended: Use Streamlit secrets
        # In your .streamlit/secrets.toml file, add:
        # GEMINI_API_KEY = "YOUR_API_KEY"
        # Access like: st.secrets["GEMINI_API_KEY"]

        # --- WARNING: Hardcoding API keys is HIGHLY INSECURE! ---
        # api_key = "YOUR_HARDCODED_API_KEY" # <--- REMOVE THIS LINE IN PRODUCTION
        # ----------------------------------------------------

        # Attempt to get API key from Streamlit secrets first
        api_key = st.secrets.get("GEMINI_API_KEY")
        if not api_key:
            # Fallback to environment variable (less recommended than secrets for Streamlit Cloud)
            api_key = os.environ.get("GEMINI_API_KEY")

        if not api_key:
             st.error("âŒ Ú©Ù„ÛŒØ¯ API Ø¬Ù…ÛŒÙ†Ø§ÛŒ (GEMINI_API_KEY) Ø¯Ø± ÙØ§ÛŒÙ„ secrets.toml ÛŒØ§ Environment Variables ÛŒØ§ÙØª Ù†Ø´Ø¯.")
             st.info("Ù„Ø·ÙØ§Ù‹ ÙØ§ÛŒÙ„ .streamlit/secrets.toml Ø±Ø§ Ø§ÛŒØ¬Ø§Ø¯ Ú©Ø±Ø¯Ù‡ Ùˆ Ú©Ù„ÛŒØ¯ Ø®ÙˆØ¯ Ø±Ø§ Ø¯Ø± Ø¢Ù† Ù‚Ø±Ø§Ø± Ø¯Ù‡ÛŒØ¯ ÛŒØ§ Environment Variable Ù…Ø±Ø¨ÙˆØ·Ù‡ Ø±Ø§ ØªÙ†Ø¸ÛŒÙ… Ú©Ù†ÛŒØ¯.")
             return None

        genai.configure(api_key=api_key)

        # Define safety settings - Adjust as needed
        safety_settings = [
            {
                "category": "HARM_CATEGORY_HARASSMENT",
                "threshold": "BLOCK_NONE", # Or BLOCK_LOW_AND_ABOVE
            },
            {
                "category": "HARM_CATEGORY_HATE_SPEECH",
                "threshold": "BLOCK_NONE",
            },
            {
                "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                "threshold": "BLOCK_NONE",
            },
            {
                "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                "threshold": "BLOCK_NONE",
            },
        ]

        # Use a suitable model, e.g., 'gemini-1.5-flash' for lower latency/cost
        model = genai.GenerativeModel('gemini-1.5-flash', safety_settings=safety_settings)
        print("Gemini Configured Successfully.")
        return model
    except Exception as e:
        st.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± ØªÙ†Ø¸ÛŒÙ… Gemini API: {e}")
        st.error(traceback.format_exc())
        return None

# Function to get AI analysis for needs
# Increased cache persistence
@st.cache_data(show_spinner="Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø±ÛŒØ§ÙØª ØªØ­Ù„ÛŒÙ„ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ...", persist="ì•±") # Persist across app restarts
def get_ai_needs_analysis(_model, farm_name, index_data, recommendations):
    """Generates AI analysis for the farm's condition related to needs."""
    if _model is None:
        return "Ø³Ø±ÙˆÛŒØ³ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª."

    # Prepare data string - Handle None values gracefully
    data_str_parts = []
    if pd.notna(index_data.get('NDVI_curr')):
        data_str_parts.append(f"NDVI ÙØ¹Ù„ÛŒ: {index_data['NDVI_curr']:.3f}")
        if pd.notna(index_data.get('NDVI_prev')):
            data_str_parts.append(f"(Ù‚Ø¨Ù„ÛŒ: {index_data['NDVI_prev']:.3f})")
    if pd.notna(index_data.get('NDMI_curr')):
         data_str_parts.append(f"\nNDMI ÙØ¹Ù„ÛŒ: {index_data['NDMI_curr']:.3f}")
         if pd.notna(index_data.get('NDMI_prev')):
              data_str_parts.append(f"(Ù‚Ø¨Ù„ÛŒ: {index_data['NDMI_prev']:.3f})")
    if pd.notna(index_data.get('EVI_curr')):
         data_str_parts.append(f"\nEVI ÙØ¹Ù„ÛŒ: {index_data['EVI_curr']:.3f}")
         if pd.notna(index_data.get('EVI_prev')):
              data_str_parts.append(f"(Ù‚Ø¨Ù„ÛŒ: {index_data['EVI_prev']:.3f})")
    if pd.notna(index_data.get('SAVI_curr')):
         data_str_parts.append(f"\nSAVI ÙØ¹Ù„ÛŒ: {index_data['SAVI_curr']:.3f}")
         if pd.notna(index_data.get('SAVI_prev')):
              data_str_parts.append(f"(Ù‚Ø¨Ù„ÛŒ: {index_data['SAVI_prev']:.3f})")

    data_str = " ".join(data_str_parts) if data_str_parts else "Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ø§Ø®Øµ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª."

    prompt = f"""
    Ø´Ù…Ø§ ÛŒÚ© Ù…ØªØ®ØµØµ Ú©Ø´Ø§ÙˆØ±Ø²ÛŒ Ù†ÛŒØ´Ú©Ø± Ù‡Ø³ØªÛŒØ¯. Ù„Ø·ÙØ§Ù‹ ÙˆØ¶Ø¹ÛŒØª Ù…Ø²Ø±Ø¹Ù‡ '{farm_name}' Ø±Ø§ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ø§Ø®Øµ Ù…Ø§Ù‡ÙˆØ§Ø±Ù‡â€ŒØ§ÛŒ Ùˆ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø²ÛŒØ± ØªØ­Ù„ÛŒÙ„ Ú©Ù†ÛŒØ¯ Ùˆ ÛŒÚ© ØªÙˆØ¶ÛŒØ­ Ú©ÙˆØªØ§Ù‡ØŒ Ú©Ø§Ø±Ø¨Ø±Ø¯ÛŒ Ùˆ ÙØ§Ø±Ø³ÛŒ Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯. ØªÙ…Ø±Ú©Ø² ØªØ­Ù„ÛŒÙ„ Ø¨Ø± Ù†ÛŒØ§Ø² Ø¢Ø¨ÛŒØ§Ø±ÛŒ Ùˆ Ú©ÙˆØ¯ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØºÛŒÛŒØ±Ø§Øª Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ Ø¨Ø§Ø´Ø¯. ØªÙˆØ¶ÛŒØ­ Ø´Ù…Ø§ Ø¨Ø§ÛŒØ¯ Ø¨Ù‡ Ø²Ø¨Ø§Ù† Ø³Ø§Ø¯Ù‡ Ùˆ Ù‚Ø§Ø¨Ù„ ÙÙ‡Ù… Ø¨Ø±Ø§ÛŒ Ú©Ø´Ø§ÙˆØ±Ø² Ø¨Ø§Ø´Ø¯.

    Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ø§Ø®Øµ:
    {data_str}

    ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ (Ø¨Ø± Ø§Ø³Ø§Ø³ Ù‚ÙˆØ§Ù†ÛŒÙ† Ø³Ø§Ø¯Ù‡):
    {', '.join(recommendations) if recommendations else 'Ù‡ÛŒÚ† ØªÙˆØµÛŒÙ‡â€ŒØ§ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù‚ÙˆØ§Ù†ÛŒÙ† Ø§ÙˆÙ„ÛŒÙ‡ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯.'}

    ØªØ­Ù„ÛŒÙ„ Ø´Ù…Ø§:
    """

    try:
        response = _model.generate_content(prompt)
        # Accessing response text
        if hasattr(response, 'text'):
            return response.text
        else:
             st.warning("âš ï¸ Ù¾Ø§Ø³Ø® Ù…Ø¹ØªØ¨Ø±ÛŒ Ø§Ø² Gemini Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯ (Ø´Ø§ÛŒØ¯ Ù…Ø­ØªÙˆØ§ÛŒ Ù…Ø³Ø¯ÙˆØ¯ Ø´Ø¯Ù‡).")
             # Optionally inspect response.prompt_feedback or response.candidates
             return "Ù¾Ø§Ø³Ø® Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª."
    except Exception as e:
        st.warning(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Gemini API Ù‡Ù†Ú¯Ø§Ù… ØªØ­Ù„ÛŒÙ„ Ù†ÛŒØ§Ø²Ù‡Ø§: {e}")
        st.warning(traceback.format_exc())
        return "Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª ØªØ­Ù„ÛŒÙ„ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ."


# --- Function to get AI summary for the map (Optional, based on data) ---
# This function was mentioned in the description but not fully implemented.
# To implement this, you would need to feed the AI a summary of the ranking_df_sorted,
# perhaps focusing on the farms with 'ØªÙ†Ø´' status.
# For now, providing a placeholder.
@st.cache_data(show_spinner="Ø¯Ø± Ø­Ø§Ù„ Ø¯Ø±ÛŒØ§ÙØª Ø®Ù„Ø§ØµÙ‡ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù†Ù‚Ø´Ù‡...", persist="ì•±")
def get_ai_map_summary(_model, ranking_df_sorted, selected_index, selected_day):
    """Generates AI summary for the overall map/ranking status."""
    if _model is None:
        return "Ø³Ø±ÙˆÛŒØ³ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª."

    if ranking_df_sorted.empty:
        return "Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù†Ù‚Ø´Ù‡ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯."

    # Prepare a summary of the ranking data for the AI
    # Focus on problematic farms or overall trends
    negative_status_farms = ranking_df_sorted[ranking_df_sorted['ÙˆØ¶Ø¹ÛŒØª'].astype(str).str.contains("ØªÙ†Ø´|Ú©Ø§Ù‡Ø´|Ø¨Ø¯ØªØ±", case=False, na=False)]
    positive_status_farms = ranking_df_sorted[ranking_df_sorted['ÙˆØ¶Ø¹ÛŒØª'].astype(str).str.contains("Ø¨Ù‡Ø¨ÙˆØ¯|Ø±Ø´Ø¯ Ù…Ø«Ø¨Øª", case=False, na=False)]

    summary_text = f"Ø®Ù„Ø§ØµÙ‡ ÙˆØ¶Ø¹ÛŒØª Ù…Ø²Ø§Ø±Ø¹ Ø¨Ø±Ø§ÛŒ Ø±ÙˆØ² {selected_day} Ø¨Ø± Ø§Ø³Ø§Ø³ Ø´Ø§Ø®Øµ {selected_index}:\n"
    summary_text += f"ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù…Ø²Ø§Ø±Ø¹ Ø¨Ø±Ø±Ø³ÛŒ Ø´Ø¯Ù‡: {len(ranking_df_sorted)}\n"
    summary_text += f"ØªØ¹Ø¯Ø§Ø¯ Ù…Ø²Ø§Ø±Ø¹ Ø¨Ø§ ÙˆØ¶Ø¹ÛŒØª 'ØªÙ†Ø´/Ú©Ø§Ù‡Ø´': {len(negative_status_farms)}\n"
    summary_text += f"ØªØ¹Ø¯Ø§Ø¯ Ù…Ø²Ø§Ø±Ø¹ Ø¨Ø§ ÙˆØ¶Ø¹ÛŒØª 'Ø¨Ù‡Ø¨ÙˆØ¯/Ø±Ø´Ø¯ Ù…Ø«Ø¨Øª': {len(positive_status_farms)}\n"
    summary_text += f"ØªØ¹Ø¯Ø§Ø¯ Ù…Ø²Ø§Ø±Ø¹ Ø¨Ø§ ÙˆØ¶Ø¹ÛŒØª 'Ø«Ø§Ø¨Øª': {len(ranking_df_sorted) - len(negative_status_farms) - len(positive_status_farms)}\n\n"

    if not negative_status_farms.empty:
        summary_text += "Ù…Ø²Ø§Ø±Ø¹ Ø¨Ø§ Ø¨ÛŒØ´ØªØ±ÛŒÙ† ØªÙ†Ø´/Ú©Ø§Ù‡Ø´ (Ø¨Ø± Ø§Ø³Ø§Ø³ Ø±ØªØ¨Ù‡):\n"
        # Get top 5 problematic farms
        top_problem_farms = negative_status_farms.head(5)
        for idx, row in top_problem_farms.iterrows():
            summary_text += f"- Ù…Ø²Ø±Ø¹Ù‡ {row['Ù…Ø²Ø±Ø¹Ù‡']}: ÙˆØ¶Ø¹ÛŒØª {row['ÙˆØ¶Ø¹ÛŒØª'].replace('<span class="status-badge status-negative">', '').replace('</span>', '')}, Ø´Ø§Ø®Øµ Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ: {row[f'{selected_index} (Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ)']}, ØªØºÛŒÛŒØ±: {row['ØªØºÛŒÛŒØ±']}\n"

    if not positive_status_farms.empty and len(positive_status_farms) > 0:
         summary_text += "\nÙ…Ø²Ø§Ø±Ø¹ÛŒ Ø¨Ø§ Ø¨Ù‡ØªØ±ÛŒÙ† Ø¨Ù‡Ø¨ÙˆØ¯/Ø±Ø´Ø¯ (Ø¨Ø± Ø§Ø³Ø§Ø³ Ø±ØªØ¨Ù‡):\n"
         # Get top 5 improving farms
         top_improving_farms = positive_status_farms.head(5) # Head after sorting by index value if ascending
         if selected_index in ['MSI']: # For MSI, better is lower value, so last after ascending sort
              top_improving_farms = positive_status_farms.tail(5) # Tail after sorting by index value if ascending

         for idx, row in top_improving_farms.iterrows():
              summary_text += f"- Ù…Ø²Ø±Ø¹Ù‡ {row['Ù…Ø²Ø±Ø¹Ù‡']}: ÙˆØ¶Ø¹ÛŒØª {row['ÙˆØ¶Ø¹ÛŒØª'].replace('<span class="status-badge status-positive">', '').replace('</span>', '')}, Ø´Ø§Ø®Øµ Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ: {row[f'{selected_index} (Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ)']}, ØªØºÛŒÛŒØ±: {row['ØªØºÛŒÛŒØ±']}\n"


    prompt = f"""
    Ø´Ù…Ø§ ÛŒÚ© ØªØ­Ù„ÛŒÙ„Ú¯Ø± Ú©Ø´Ø§ÙˆØ±Ø²ÛŒ Ù‡Ø³ØªÛŒØ¯. Ù„Ø·ÙØ§Ù‹ ÛŒÚ© Ø®Ù„Ø§ØµÙ‡ ÙØ§Ø±Ø³ÛŒ Ú©ÙˆØªØ§Ù‡ Ùˆ Ù…ÙÛŒØ¯ Ø§Ø² ÙˆØ¶Ø¹ÛŒØª Ú©Ù„ÛŒ Ù…Ø²Ø§Ø±Ø¹ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø²ÛŒØ± Ø§Ø±Ø§Ø¦Ù‡ Ø¯Ù‡ÛŒØ¯. Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ù…Ø²Ø§Ø±Ø¹ Ø¯Ø± Ù‡Ø± ÙˆØ¶Ø¹ÛŒØª (ØªÙ†Ø´ØŒ Ø¨Ù‡Ø¨ÙˆØ¯ØŒ Ø«Ø§Ø¨Øª) Ùˆ Ø¯Ø± ØµÙˆØ±Øª ÙˆØ¬ÙˆØ¯ØŒ Ø¨Ù‡ Ù…Ø²Ø§Ø±Ø¹ Ø¨Ø§ Ø¨ÛŒØ´ØªØ±ÛŒÙ† ØªÙ†Ø´ Ø§Ø´Ø§Ø±Ù‡ Ú©Ù†ÛŒØ¯.

    Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø®Ù„Ø§ØµÙ‡ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ:
    {summary_text}

    Ø®Ù„Ø§ØµÙ‡ ØªØ­Ù„ÛŒÙ„ Ø´Ù…Ø§:
    """
    try:
        response = _model.generate_content(prompt)
        if hasattr(response, 'text'):
            return response.text
        else:
             st.warning("âš ï¸ Ù¾Ø§Ø³Ø® Ù…Ø¹ØªØ¨Ø±ÛŒ Ø§Ø² Gemini Ø¨Ø±Ø§ÛŒ Ø®Ù„Ø§ØµÙ‡ Ù†Ù‚Ø´Ù‡ Ø¯Ø±ÛŒØ§ÙØª Ù†Ø´Ø¯.")
             return "Ø®Ù„Ø§ØµÙ‡ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª."
    except Exception as e:
        st.warning(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø§Ø±ØªØ¨Ø§Ø· Ø¨Ø§ Gemini API Ù‡Ù†Ú¯Ø§Ù… Ø®Ù„Ø§ØµÙ‡â€ŒØ³Ø§Ø²ÛŒ Ù†Ù‚Ø´Ù‡: {e}")
        st.warning(traceback.format_exc())
        return "Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø®Ù„Ø§ØµÙ‡ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù†Ù‚Ø´Ù‡."


# ==============================================================================
# Helper Function for Status Determination
# ==============================================================================

def determine_status(row, index_name):
    """Determines the status based on change in index value."""
    # Use pd.notna for robust NaN check
    if pd.notna(row.get('ØªØºÛŒÛŒØ±')) and pd.notna(row.get(f'{index_name} (Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ)')) and pd.notna(row.get(f'{index_name} (Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„)')):
        change_val = row['ØªØºÛŒÛŒØ±']
        # Threshold for significant change - Can be adjusted
        threshold = 0.03 # Lowering threshold slightly for potentially more sensitive detection

        current_val = row[f'{index_name} (Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ)']
        previous_val = row[f'{index_name} (Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„)']

        # For indices where higher is better (NDVI, EVI, LAI, CVI, NDMI)
        # Consider the absolute values too, not just change, maybe relative change
        if index_name in ['NDVI', 'EVI', 'LAI', 'CVI', 'NDMI']:
            # Check for significant positive or negative change
            if change_val > threshold:
                return "Ø±Ø´Ø¯ Ù…Ø«Ø¨Øª / Ø¨Ù‡Ø¨ÙˆØ¯"
            elif change_val < -threshold:
                return "ØªÙ†Ø´ / Ú©Ø§Ù‡Ø´"
            else:
                # If change is within threshold, check if values are consistently low/high
                # This part is more complex and depends on what "fixed" means for different indices
                # For simplicity, let's stick to change for now unless specific fixed thresholds are defined
                 return "Ø«Ø§Ø¨Øª"

        # For indices where lower is better (MSI)
        elif index_name in ['MSI']:
            # Negative change means improvement (less stress)
            if change_val < -threshold:
                return "Ø¨Ù‡Ø¨ÙˆØ¯"
            # Positive change means deterioration (more stress)
            elif change_val > threshold:
                return "ØªÙ†Ø´ / Ø¨Ø¯ØªØ± Ø´Ø¯Ù†"
            else:
                return "Ø«Ø§Ø¨Øª"
        else:
            # Default case if index type is unknown or not handled
            return "Ù†Ø§Ù…Ø´Ø®Øµ"
    elif pd.notna(row.get(f'{index_name} (Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ)')) and pd.isna(row.get(f'{index_name} (Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„)')):
         return "Ø¨Ø¯ÙˆÙ† Ø¯Ø§Ø¯Ù‡ Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„"
    elif pd.isna(row.get(f'{index_name} (Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ)')) and pd.notna(row.get(f'{index_name} (Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„)')):
         return "Ø¨Ø¯ÙˆÙ† Ø¯Ø§Ø¯Ù‡ Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ"
    else:
        return "Ø¨Ø¯ÙˆÙ† Ø¯Ø§Ø¯Ù‡"


# ==============================================================================
# Main Application Layout (Using Tabs)
# ==============================================================================

# Configure Gemini Model at the start
gemini_model = configure_gemini()

# Define tabs
tab1, tab2, tab3 = st.tabs(["ğŸ“Š Ù¾Ø§ÛŒØ´ Ù…Ø²Ø§Ø±Ø¹ (Ù†Ù‚Ø´Ù‡ Ùˆ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ)", "ğŸ“ˆ ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ø§Øª", "ğŸ’§ ØªØ­Ù„ÛŒÙ„ Ù†ÛŒØ§Ø² Ø¢Ø¨ÛŒØ§Ø±ÛŒ Ùˆ Ú©ÙˆØ¯Ø¯Ù‡ÛŒ"])

# --- Tab 1: Map and Ranking ---
with tab1:
    st.header("ğŸ“Š Ù¾Ø§ÛŒØ´ Ù…Ø²Ø§Ø±Ø¹ (Ù†Ù‚Ø´Ù‡ Ùˆ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ)")

    if filtered_farms_df.empty:
        st.warning("âš ï¸ Ù‡ÛŒÚ† Ù…Ø²Ø±Ø¹Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø±ÙˆØ² Ùˆ ÙÛŒÙ„ØªØ±Ù‡Ø§ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù†Ù…Ø§ÛŒØ´ Ù†Ù‚Ø´Ù‡ Ùˆ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø§Ù…Ú©Ø§Ù†â€ŒÙ¾Ø°ÛŒØ± Ù†ÛŒØ³Øª.")
    else:
        # --- Get Selected Farm Geometry and Details ---
        selected_farm_details = None
        selected_farm_geom = None
        center_lat = INITIAL_LAT
        center_lon = INITIAL_LON
        zoom_level = INITIAL_ZOOM

        if selected_farm_name == "Ù‡Ù…Ù‡ Ù…Ø²Ø§Ø±Ø¹":
            # Use the bounding box of all filtered farms for the map view
            min_lon, min_lat = filtered_farms_df['centroid_lon'].min(), filtered_farms_df['centroid_lat'].min()
            max_lon, max_lat = filtered_farms_df['centroid_lon'].max(), filtered_farms_df['centroid_lat'].max()
            # Create a bounding box geometry
            try:
                 # Ensure coordinates are valid before creating ee.Geometry
                 if pd.notna(min_lon) and pd.notna(min_lat) and pd.notna(max_lon) and pd.notna(max_lat):
                     selected_farm_geom = ee.Geometry.Rectangle([min_lon, min_lat, max_lon, max_lat])
                 else:
                     st.warning("âš ï¸ Ù…Ø®ØªØµØ§Øª Ù…Ø¹ØªØ¨Ø± Ø¨Ø±Ø§ÛŒ Ø§ÛŒØ¬Ø§Ø¯ Ù†Ø§Ø­ÛŒÙ‡ Ù…Ø±Ø²ÛŒ Ù‡Ù…Ù‡ Ù…Ø²Ø§Ø±Ø¹ ÛŒØ§ÙØª Ù†Ø´Ø¯.")
            except Exception as e:
                 st.warning(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø§ÛŒØ¬Ø§Ø¯ Ù†Ø§Ø­ÛŒÙ‡ Ù…Ø±Ø²ÛŒ GEE Ø¨Ø±Ø§ÛŒ Ù‡Ù…Ù‡ Ù…Ø²Ø§Ø±Ø¹: {e}")
                 selected_farm_geom = None # Set to None if geometry creation fails

            st.subheader(f"Ù†Ù…Ø§ÛŒØ´ Ú©Ù„ÛŒ Ù…Ø²Ø§Ø±Ø¹ Ø¨Ø±Ø§ÛŒ Ø±ÙˆØ²: {selected_day}")
            # --- ØªØ¹Ø¯Ø§Ø¯ Ù…Ø²Ø§Ø±Ø¹ Ø¯Ø± Ø§ÛŒÙ† Ø±ÙˆØ² (Ú©Ø§Ø±Øª Ø²ÛŒØ¨Ø§) ---
            st.markdown(modern_metric_card("ØªØ¹Ø¯Ø§Ø¯ Ù…Ø²Ø§Ø±Ø¹ Ø¯Ø± Ø§ÛŒÙ† Ø±ÙˆØ²", f"{len(filtered_farms_df):,}", icon="fa-leaf", color="#185a9d"), unsafe_allow_html=True)
            st.caption("ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„ Ù…Ø²Ø§Ø±Ø¹ Ø«Ø¨Øª Ø´Ø¯Ù‡ Ø¨Ø±Ø§ÛŒ Ø±ÙˆØ² Ø§Ù†ØªØ®Ø§Ø¨ Ø´Ø¯Ù‡.")

            # --- Ù†Ù…ÙˆØ¯Ø§Ø± Ø³Ù‡ Ø¨Ø¹Ø¯ÛŒ Ù…ØªØ­Ø±Ú© Ø¯Ø±ØµØ¯ Ù‡Ø± ÙˆØ§Ø±ÛŒØªÙ‡ Ø¯Ø± Ø§ÛŒÙ† Ø±ÙˆØ² ---
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¯Ø±ØµØ¯ Ù‡Ø± ÙˆØ§Ø±ÛŒØªÙ‡ Ø¯Ø± Ø¨ÛŒÙ† Ù…Ø²Ø§Ø±Ø¹ Ø§ÛŒÙ† Ø±ÙˆØ²
            if 'ÙˆØ§Ø±ÛŒØªÙ‡' in filtered_farms_df.columns and not filtered_farms_df['ÙˆØ§Ø±ÛŒØªÙ‡'].isna().all():
                # Filter out 'nan' strings if any survived
                variety_counts = filtered_farms_df[filtered_farms_df['ÙˆØ§Ø±ÛŒØªÙ‡'].astype(str).str.lower() != 'nan']['ÙˆØ§Ø±ÛŒØªÙ‡'].value_counts().sort_values(ascending=False)
                if not variety_counts.empty:
                     variety_percent = 100 * variety_counts / variety_counts.sum()
                     # Ø³Ø§Ø®Øª Ø¯ÛŒØªØ§ÙØ±ÛŒÙ… Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆØ¯Ø§Ø±
                     pie_df = pd.DataFrame({
                         'ÙˆØ§Ø±ÛŒØªÙ‡': variety_percent.index,
                         'Ø¯Ø±ØµØ¯': variety_percent.values
                     })
                     # Ù†Ù…ÙˆØ¯Ø§Ø± Ø³Ù‡ Ø¨Ø¹Ø¯ÛŒ Ù…ØªØ­Ø±Ú© (Pie 3D Animated)
                     fig_pie = go.Figure(
                         data=[go.Pie(
                             labels=pie_df['ÙˆØ§Ø±ÛŒØªÙ‡'],
                             values=pie_df['Ø¯Ø±ØµØ¯'],
                             hole=0.3,
                             pull=[0.08]*len(pie_df),
                             marker=dict(line=dict(color='#fff', width=2)),
                             textinfo='label+percent',
                             insidetextorientation='radial',
                             rotation=90,
                         )]
                     )
                     fig_pie.update_traces(textfont_size=18)
                     fig_pie.update_layout(
                         title=f"Ø¯Ø±ØµØ¯ ÙˆØ§Ø±ÛŒØªÙ‡â€ŒÙ‡Ø§ Ø¯Ø± Ù…Ø²Ø§Ø±Ø¹ Ø±ÙˆØ² {selected_day} (Ø¯Ø§ÛŒÙ†Ø§Ù…ÛŒÚ©)",
                         showlegend=True,
                         height=450,
                         margin=dict(l=20, r=20, t=60, b=20),
                         paper_bgcolor='rgba(0,0,0,0)',
                         plot_bgcolor='rgba(0,0,0,0)'
                     )
                     st.plotly_chart(fig_pie, use_container_width=True)
                     st.caption("Ø¯Ø±ØµØ¯ Ù‡Ø± ÙˆØ§Ø±ÛŒØªÙ‡ Ø§Ø² Ú©Ù„ Ù…Ø²Ø§Ø±Ø¹ Ø«Ø¨Øª Ø´Ø¯Ù‡ Ø¯Ø± Ø§ÛŒÙ† Ø±ÙˆØ².")
                else:
                    st.info("Ø¯Ø§Ø¯Ù‡ ÙˆØ§Ø±ÛŒØªÙ‡ Ù…Ø¹ØªØ¨Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆØ¯Ø§Ø± Ø¯Ø± Ø§ÛŒÙ† Ø±ÙˆØ² ÛŒØ§ÙØª Ù†Ø´Ø¯.")
            else:
                st.info("Ø³ØªÙˆÙ† ÙˆØ§Ø±ÛŒØªÙ‡ Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§ÛŒÙ† Ø±ÙˆØ² ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯ ÛŒØ§ Ø®Ø§Ù„ÛŒ Ø§Ø³Øª.")


        else:
            selected_farm_details = filtered_farms_df[filtered_farms_df['Ù…Ø²Ø±Ø¹Ù‡'] == selected_farm_name].iloc[0]
            lat = selected_farm_details['centroid_lat']
            lon = selected_farm_details['centroid_lon']
            # Ensure coordinates are valid numbers before creating GEE Point
            if pd.notna(lat) and pd.notna(lon):
                selected_farm_geom = ee.Geometry.Point([lon, lat])
                center_lat = lat
                center_lon = lon
                zoom_level = 14 # Zoom closer for a single farm
            else:
                 st.warning(f"âš ï¸ Ù…Ø®ØªØµØ§Øª Ù…Ø¹ØªØ¨Ø± Ø¨Ø±Ø§ÛŒ Ù…Ø²Ø±Ø¹Ù‡ '{selected_farm_name}' ÛŒØ§ÙØª Ù†Ø´Ø¯.")
                 selected_farm_geom = None


            st.subheader(f"Ø¬Ø²Ø¦ÛŒØ§Øª Ù…Ø²Ø±Ø¹Ù‡: {selected_farm_name} (Ø±ÙˆØ²: {selected_day})")
            # Display farm details using modern cards
            details_cols = st.columns(3)
            with details_cols[0]:
                st.markdown(modern_metric_card("Ù…Ø³Ø§Ø­Øª Ø¯Ø§Ø´Øª (Ù‡Ú©ØªØ§Ø±)", f"{selected_farm_details.get('Ù…Ø³Ø§Ø­Øª', 'N/A'):,.2f}" if pd.notna(selected_farm_details.get('Ù…Ø³Ø§Ø­Øª')) else "N/A", icon="fa-ruler-combined", color="#43cea2"), unsafe_allow_html=True)
                st.markdown(modern_metric_card("ÙˆØ§Ø±ÛŒØªÙ‡", f"{selected_farm_details.get('ÙˆØ§Ø±ÛŒØªÙ‡', 'N/A')}", icon="fa-seedling", color="#43cea2"), unsafe_allow_html=True)
            with details_cols[1]:
                st.markdown(modern_metric_card("Ú¯Ø±ÙˆÙ‡", f"{selected_farm_details.get('Ú¯Ø±ÙˆÙ‡', 'N/A')}", icon="fa-users", color="#43cea2"), unsafe_allow_html=True)
                st.markdown(modern_metric_card("Ø³Ù†", f"{selected_farm_details.get('Ø³Ù†', 'N/A')}", icon="fa-hourglass-half", color="#43cea2"), unsafe_allow_html=True)
            with details_cols[2]:
                st.markdown(modern_metric_card("Ù…Ø®ØªØµØ§Øª", f"{lat:.5f}, {lon:.5f}" if pd.notna(lat) and pd.notna(lon) else "N/A", icon="fa-map-marker-alt", color="#43cea2"), unsafe_allow_html=True)


        # --- Map Display ---
        st.markdown("---")
        st.subheader("ğŸ—ºï¸ Ù†Ù‚Ø´Ù‡ ÙˆØ¶Ø¹ÛŒØª Ù…Ø²Ø§Ø±Ø¹")

        # Define visualization parameters based on the selected index
        vis_params = {
            'NDVI': {'min': 0, 'max': 0.9, 'palette': ['red', 'yellow', 'green']},
            'EVI': {'min': 0, 'max': 0.8, 'palette': ['red', 'yellow', 'green']},
            'NDMI': {'min': -0.5, 'max': 0.5, 'palette': ['brown', 'white', 'blue']}, # Adjusted range
            'LAI': {'min': 0, 'max': 5, 'palette': ['white', 'lightgreen', 'darkgreen']}, # Adjusted max based on expected values
            'MSI': {'min': 0.5, 'max': 2, 'palette': ['blue', 'white', 'brown']}, # Lower values = more moisture, adjusted range
            'CVI': {'min': 0, 'max': 15, 'palette': ['yellow', 'lightgreen', 'darkgreen']}, # Adjusted max based on expected values
            'SAVI': {'min': 0, 'max': 0.8, 'palette': ['red', 'yellow', 'green']},
        }

        # Create a geemap Map instance
        m = geemap.Map(
            location=[center_lat, center_lon],
            zoom=zoom_level,
            add_google_map=False # Start clean
        )
        m.add_basemap("HYBRID") # Add Google Satellite Hybrid basemap

        # Get the processed image for the current week
        gee_image_current = None
        error_msg_current = None
        if selected_farm_geom and start_date_current_str and end_date_current_str:
            gee_image_current, error_msg_current = get_processed_image(
                selected_farm_geom, start_date_current_str, end_date_current_str, selected_index
            )

        if gee_image_current:
            # Add the GEE layer to the map
            try:
                # Ensure vis_params has default if selected_index is not in the dict
                index_vis_params = vis_params.get(selected_index, {'min': 0, 'max': 1, 'palette': ['red', 'yellow', 'green']})
                m.addLayer(
                    gee_image_current,
                    index_vis_params,
                    f"{selected_index} ({start_date_current_str} ØªØ§ {end_date_current_str})"
                )

                # Create a custom legend using folium
                # Simplified legend based on color gradient meaning
                legend_title = f"Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒ Ø´Ø§Ø®Øµ {selected_index}"
                if selected_index in ['NDVI', 'EVI', 'LAI', 'CVI', 'SAVI']:
                    legend_text = '''
                    <p style="margin: 0;"><span style="color: red;">Low Value</span> / <span style="color: yellow;">Medium Value</span> / <span style="color: green;">High Value</span></p>
                    <p style="margin: 0; font-size: 0.8em;">(Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ù‡ ÙˆØ¶Ø¹ÛŒØª Ù¾ÙˆØ´Ø´ Ú¯ÛŒØ§Ù‡ÛŒ/Ø³Ù„Ø§Ù…Øª)</p>
                    '''
                elif selected_index in ['NDMI']:
                    legend_text = '''
                    <p style="margin: 0;"><span style="color: brown;">Low Value</span> / <span style="color: white;">Medium Value</span> / <span style="color: blue;">High Value</span></p>
                    <p style="margin: 0; font-size: 0.8em;">(Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ù‡ Ø±Ø·ÙˆØ¨Øª)</p>
                    '''
                elif selected_index in ['MSI']: # Lower MSI is better (more moisture)
                     legend_text = '''
                    <p style="margin: 0;"><span style="color: blue;">Low Value</span> / <span style="color: white;">Medium Value</span> / <span style="color: brown;">High Value</span></p>
                    <p style="margin: 0; font-size: 0.8em;">(Ù†Ø´Ø§Ù†â€ŒØ¯Ù‡Ù†Ø¯Ù‡ ØªÙ†Ø´ Ø±Ø·ÙˆØ¨ØªÛŒØŒ Ù¾Ø§ÛŒÛŒÙ†â€ŒØªØ± Ø¨Ù‡ØªØ± Ø§Ø³Øª)</p>
                    '''
                else:
                    legend_text = '''
                    <p style="margin: 0;">Custom Index Legend</p>
                    '''

                legend_html = f'''
                <div style="position: fixed; bottom: 50px; right: 10px; z-index: 1000; background-color: white; padding: 10px; border: 2px solid grey; border-radius: 5px; font-family: Vazirmatn, sans-serif;">
                    <p style="margin: 0;"><strong>{legend_title}</strong></p>
                    {legend_text}
                </div>
                '''
                # Add the custom legend to the map
                m.get_root().html.add_child(folium.Element(legend_html))

                # --- Add Farm Markers with Popups ---
                # This section needs to be updated to show relevant info including current/previous indices
                # It might require calculating indices for each farm point beforehand or fetching them here.
                # To show index values in popups, we need the results from calculate_weekly_indices.
                # Let's calculate ranking_df_map here if selected_farm_name is "Ù‡Ù…Ù‡ Ù…Ø²Ø§Ø±Ø¹"
                # If selected_farm_name is a single farm, we can use the details we already have or fetch its index value.

                # --- Calculate Ranking Data for Map Popups if "Ù‡Ù…Ù‡ Ù…Ø²Ø§Ø±Ø¹" ---
                ranking_df_map_popups = pd.DataFrame()
                if selected_farm_name == "Ù‡Ù…Ù‡ Ù…Ø²Ø§Ø±Ø¹" and start_date_current_str and end_date_current_str and start_date_previous_str and end_date_previous_str:
                     # Reuse calculate_weekly_indices for the map popups
                     # Note: This might recalculate if the cache key is different from the table's call
                     # To optimize, we could store the results of the first calculation.
                     # For simplicity now, let's call it, cache should help.
                     st.info("Ø¯Ø± Ø­Ø§Ù„ Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ù…Ø²Ø§Ø±Ø¹ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø±ÙˆÛŒ Ù†Ù‚Ø´Ù‡...")
                     ranking_df_map_popups, popup_calculation_errors = calculate_weekly_indices(
                          filtered_farms_df,
                          selected_index,
                          start_date_current_str,
                          end_date_current_str,
                          start_date_previous_str,
                          end_date_previous_str
                     )
                     if popup_calculation_errors:
                         st.warning("âš ï¸ Ø¨Ø±Ø®ÛŒ Ø®Ø·Ø§Ù‡Ø§ Ø¯Ø± Ø­ÛŒÙ† Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ù¾â€ŒØ¢Ù¾â€ŒÙ‡Ø§ÛŒ Ù†Ù‚Ø´Ù‡ Ø±Ø® Ø¯Ø§Ø¯:")
                         for error in popup_calculation_errors[:5]: st.warning(f"- {error}") # Show first 5

                # Add markers for farms
                if selected_farm_name == "Ù‡Ù…Ù‡ Ù…Ø²Ø§Ø±Ø¹":
                     # Add markers for all filtered farms using data from ranking_df_map_popups
                     if not ranking_df_map_popups.empty:
                         # Merge with original farm data to get other properties like age/variety
                         map_data_with_indices = pd.merge(
                             filtered_farms_df,
                             ranking_df_map_popups,
                             on='Ù…Ø²Ø±Ø¹Ù‡',
                             how='left' # Keep all farms from filtered_farms_df
                         )

                         for idx, farm in map_data_with_indices.iterrows():
                              if pd.notna(farm['centroid_lat']) and pd.notna(farm['centroid_lon']):
                                  popup_html = f"""
                                  <strong>Ù…Ø²Ø±Ø¹Ù‡:</strong> {farm['Ù…Ø²Ø±Ø¹Ù‡']}<br>
                                  <strong>Ú¯Ø±ÙˆÙ‡:</strong> {farm.get('Ú¯Ø±ÙˆÙ‡', 'N/A')}<br>
                                  <strong>Ø³Ù†:</strong> {farm.get('Ø³Ù†', 'N/A')}<br>
                                  <strong>ÙˆØ§Ø±ÛŒØªÙ‡:</strong> {farm.get('ÙˆØ§Ø±ÛŒØªÙ‡', 'N/A')}<br>
                                  ---<br>
                                  <strong>{selected_index} (Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ):</strong> {farm.get(f'{selected_index} (Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ)', 'N/A'):.3f} <br>
                                  <strong>{selected_index} (Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„):</strong> {farm.get(f'{selected_index} (Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„)', 'N/A'):.3f} <br>
                                  <strong>ØªØºÛŒÛŒØ±:</strong> {farm.get('ØªØºÛŒÛŒØ±', 'N/A'):.3f}
                                  """
                                  # Add status badge to popup if available
                                  # Need to determine status for each farm here based on change
                                  if pd.notna(farm.get('ØªØºÛŒÛŒØ±')):
                                       status_text = determine_status(farm, selected_index)
                                       popup_html += f"<br><strong>ÙˆØ¶Ø¹ÛŒØª:</strong> {status_text}" # Add plain text status for popup

                                  folium.Marker(
                                      location=[farm['centroid_lat'], farm['centroid_lon']],
                                      popup=folium.Popup(popup_html, max_width=300),
                                      tooltip=farm['Ù…Ø²Ø±Ø¹Ù‡'],
                                      icon=folium.Icon(color='blue', icon='info-sign')
                                  ).add_to(m)
                         # Center on the bounding box of the filtered farms
                         if selected_farm_geom:
                              m.center_object(selected_farm_geom, zoom=zoom_level) # Center on the bounding box
                     else:
                         st.info("Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ø§Ø®Øµ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¯Ø± Ù¾Ø§Ù¾â€ŒØ¢Ù¾â€ŒÙ‡Ø§ÛŒ Ù†Ù‚Ø´Ù‡ Ù‡Ù…Ù‡ Ù…Ø²Ø§Ø±Ø¹ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª.")

                else:
                     # Add marker for the single selected farm
                     if selected_farm_details is not None and pd.notna(selected_farm_details['centroid_lat']) and pd.notna(selected_farm_details['centroid_lon']):
                         # For a single farm, fetch its specific index values if needed for the popup
                         # Or ideally, use the results from calculate_weekly_indices if already run for the table.
                         # Let's assume calculate_weekly_indices has been run or will be.
                         # Find the farm in the potential ranking_df results
                         farm_ranking_info = None
                         if 'ranking_df_sorted' in locals() and not ranking_df_sorted.empty:
                             # Use ranking_df_sorted from the table calculation if available
                             farm_ranking_info = ranking_df_sorted[ranking_df_sorted['Ù…Ø²Ø±Ø¹Ù‡'] == selected_farm_name].iloc[0] if selected_farm_name in ranking_df_sorted['Ù…Ø²Ø±Ø¹Ù‡'].values else None
                         elif start_date_current_str and end_date_current_str and start_date_previous_str and end_date_previous_str:
                              # If ranking_df_sorted is not available, perform a quick calculation for this single farm
                              # This might be redundant if calculate_weekly_indices is called later for the table.
                              # To avoid redundancy, maybe only fetch index value for the popup here.
                              # Let's fetch values for the popup directly if not in ranking_df_sorted
                              point_geom_single = ee.Geometry.Point([selected_farm_details['centroid_lon'], selected_farm_details['centroid_lat']])
                              curr_val, err_curr = get_farm_needs_data(point_geom_single, start_date_current_str, end_date_current_str, start_date_previous_str, end_date_previous_str)
                              if not err_curr:
                                   farm_ranking_info = {
                                       'Ù…Ø²Ø±Ø¹Ù‡': selected_farm_name,
                                       f'{selected_index} (Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ)': curr_val.get(f'{selected_index}_curr'),
                                       f'{selected_index} (Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„)': curr_val.get(f'{selected_index}_prev')
                                   }
                                   # Calculate change if values exist
                                   if pd.notna(farm_ranking_info[f'{selected_index} (Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ)']) and pd.notna(farm_ranking_info[f'{selected_index} (Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„)']):
                                        farm_ranking_info['ØªØºÛŒÛŒØ±'] = farm_ranking_info[f'{selected_index} (Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ)'] - farm_ranking_info[f'{selected_index} (Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„)']
                                   else:
                                        farm_ranking_info['ØªØºÛŒÛŒØ±'] = None
                                   # Determine status
                                   farm_ranking_info['ÙˆØ¶Ø¹ÛŒØª'] = determine_status(farm_ranking_info, selected_index) if pd.notna(farm_ranking_info.get('ØªØºÛŒÛŒØ±')) else "Ø¨Ø¯ÙˆÙ† Ø¯Ø§Ø¯Ù‡"


                         popup_html = f"""
                         <strong>Ù…Ø²Ø±Ø¹Ù‡:</strong> {selected_farm_name}<br>
                         <strong>Ú¯Ø±ÙˆÙ‡:</strong> {selected_farm_details.get('Ú¯Ø±ÙˆÙ‡', 'N/A')}<br>
                         <strong>Ø³Ù†:</strong> {selected_farm_details.get('Ø³Ù†', 'N/A')}<br>
                         <strong>ÙˆØ§Ø±ÛŒØªÙ‡:</strong> {selected_farm_details.get('ÙˆØ§Ø±ÛŒØªÙ‡', 'N/A')}<br>
                         ---<br>
                         """
                         if farm_ranking_info:
                              popup_html += f"<strong>{selected_index} (Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ):</strong> {farm_ranking_info.get(f'{selected_index} (Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ)', 'N/A'):.3f} <br>"
                              popup_html += f"<strong>{selected_index} (Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„):</strong> {farm_ranking_info.get(f'{selected_index} (Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„)', 'N/A'):.3f} <br>"
                              popup_html += f"<strong>ØªØºÛŒÛŒØ±:</strong> {farm_ranking_info.get('ØªØºÛŒÛŒØ±', 'N/A'):.3f} <br>"
                              popup_html += f"<strong>ÙˆØ¶Ø¹ÛŒØª:</strong> {farm_ranking_info.get('ÙˆØ¶Ø¹ÛŒØª', 'N/A')}" # Add plain text status for popup
                         else:
                              popup_html += f"<strong>{selected_index} (Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ):</strong> N/A <br>"
                              popup_html += f"<strong>{selected_index} (Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„):</strong> N/A <br>"
                              popup_html += f"<strong>ØªØºÛŒÛŒØ±:</strong> N/A <br>"
                              popup_html += f"<strong>ÙˆØ¶Ø¹ÛŒØª:</strong> Ø¨Ø¯ÙˆÙ† Ø¯Ø§Ø¯Ù‡"


                         folium.Marker(
                             location=[lat, lon],
                             popup=folium.Popup(popup_html, max_width=300),
                             tooltip=selected_farm_name,
                             icon=folium.Icon(color='red', icon='star')
                         ).add_to(m)


                m.add_layer_control() # Add layer control to toggle base maps and layers

                # --- Add Age, Variety, Status Layers (Based on Description - NOT FULLY IMPLEMENTED YET) ---
                # The description mentions adding layers for Age, Variety, and Status based on ranking_df_map.
                # Implementing this requires converting the Pandas DataFrame (with calculated statuses)
                # into a GEE FeatureCollection and styling it for display on the map.
                # This is a significant development task and is not included in this fix.
                # Placeholder comment to acknowledge this missing feature as per the description.
                # if 'ranking_df_map' is calculated and available:
                #     try:
                #         # Convert ranking_df_map to GEE FeatureCollection
                #         # Define visualization and styling based on 'ÙˆØ¶Ø¹ÛŒØª', 'Ø³Ù†', 'ÙˆØ§Ø±ÛŒØªÙ‡'
                #         # Add these as separate layers to the map (m.addLayer)
                #         pass # Placeholder for future implementation
                #     except Exception as layer_err:
                #         st.warning(f"âš ï¸ Ø®Ø·Ø§ Ø¯Ø± Ø§ÙØ²ÙˆØ¯Ù† Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø³Ù†ØŒ ÙˆØ§Ø±ÛŒØªÙ‡ ÛŒØ§ ÙˆØ¶Ø¹ÛŒØª Ø¨Ù‡ Ù†Ù‚Ø´Ù‡: {layer_err}")


            except Exception as map_err:
                st.error(f"Ø®Ø·Ø§ Ø¯Ø± Ø§ÙØ²ÙˆØ¯Ù† Ù„Ø§ÛŒÙ‡ GEE Ø¨Ù‡ Ù†Ù‚Ø´Ù‡: {map_err}")
                st.error(traceback.format_exc())
        else:
            st.warning(f"ØªØµÙˆÛŒØ±ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø´Ø§Ø®Øµ {selected_index} Ø±ÙˆÛŒ Ù†Ù‚Ø´Ù‡ Ø¯Ø± Ø¨Ø§Ø²Ù‡ ÙØ¹Ù„ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯. {error_msg_current}")


        # Display the map in Streamlit
        st_folium(m, width=None, height=500, use_container_width=True)
        st.caption("Ø¨Ø±Ø§ÛŒ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ø¬Ø²Ø¦ÛŒØ§Øª Ø±ÙˆÛŒ Ù…Ø§Ø±Ú©Ø±Ù‡Ø§ Ú©Ù„ÛŒÚ© Ú©Ù†ÛŒØ¯. Ø§Ø² Ú©Ù†ØªØ±Ù„ Ù„Ø§ÛŒÙ‡â€ŒÙ‡Ø§ Ø¯Ø± Ø³Ù…Øª Ø±Ø§Ø³Øª Ø¨Ø§Ù„Ø§ Ø¨Ø±Ø§ÛŒ ØªØºÛŒÛŒØ± Ù†Ù‚Ø´Ù‡ Ù¾Ø§ÛŒÙ‡ Ùˆ Ù„Ø§ÛŒÙ‡ Ø´Ø§Ø®Øµ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.")
        st.info("ğŸ’¡ Ø¨Ø±Ø§ÛŒ Ø°Ø®ÛŒØ±Ù‡ Ù†Ù‚Ø´Ù‡ØŒ Ù…ÛŒâ€ŒØªÙˆØ§Ù†ÛŒØ¯ Ø§Ø² Ø§Ø¨Ø²Ø§Ø± Ø¹Ú©Ø³ Ú¯Ø±ÙØªÙ† Ø§Ø² ØµÙØ­Ù‡ (Screenshot) Ù…Ø±ÙˆØ±Ú¯Ø± ÛŒØ§ Ø³ÛŒØ³ØªÙ… Ø¹Ø§Ù…Ù„ Ø®ÙˆØ¯ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†ÛŒØ¯.")


        # --- Time Series Chart ---
        st.markdown("---")
        st.subheader(f"ğŸ“ˆ Ù†Ù…ÙˆØ¯Ø§Ø± Ø±ÙˆÙ†Ø¯ Ø²Ù…Ø§Ù†ÛŒ Ø´Ø§Ø®Øµ {selected_index}")

        if selected_farm_name == "Ù‡Ù…Ù‡ Ù…Ø²Ø§Ø±Ø¹":
            st.info("Ù„Ø·ÙØ§Ù‹ ÛŒÚ© Ù…Ø²Ø±Ø¹Ù‡ Ø®Ø§Øµ Ø±Ø§ Ø§Ø² Ù¾Ù†Ù„ Ú©Ù†Ø§Ø±ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯ ØªØ§ Ù†Ù…ÙˆØ¯Ø§Ø± Ø±ÙˆÙ†Ø¯ Ø²Ù…Ø§Ù†ÛŒ Ø¢Ù† Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆØ¯.")
        elif selected_farm_geom and selected_farm_details is not None:
             # Check if the geometry type is Point by checking the stored property
             is_point = selected_farm_details.get('geometry_type') == 'Point'

             if is_point:
                 # Define a longer period for the time series chart (e.g., last 1 year)
                 timeseries_end_date = today.strftime('%Y-%m-%d')
                 timeseries_start_date = (today - datetime.timedelta(days=365)).strftime('%Y-%m-%d')

                 ts_df, ts_error = get_index_time_series(
                     selected_farm_geom,
                     selected_index,
                     start_date=timeseries_start_date,
                     end_date=timeseries_end_date
                 )

                 if ts_error:
                     st.warning(f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ: {ts_error}")
                 elif not ts_df.empty:
                     # Plotting with Plotly for better interactivity and date handling
                     fig_ts = px.line(ts_df, y=selected_index, title=f'Ø±ÙˆÙ†Ø¯ Ø²Ù…Ø§Ù†ÛŒ Ø´Ø§Ø®Øµ {selected_index} Ø¨Ø±Ø§ÛŒ Ù…Ø²Ø±Ø¹Ù‡ {selected_farm_name}')
                     fig_ts.update_layout(
                         xaxis_title="ØªØ§Ø±ÛŒØ®",
                         yaxis_title=selected_index,
                         hovermode="x unified", # Show tooltip for nearest data point across all lines
                         margin=dict(l=20, r=20, t=40, b=20)
                     )
                     st.plotly_chart(fig_ts, use_container_width=True)
                     st.caption(f"Ù†Ù…ÙˆØ¯Ø§Ø± ØªØºÛŒÛŒØ±Ø§Øª Ø´Ø§Ø®Øµ {selected_index} Ø¨Ø±Ø§ÛŒ Ù…Ø²Ø±Ø¹Ù‡ {selected_farm_name} Ø¯Ø± ÛŒÚ© Ø³Ø§Ù„ Ú¯Ø°Ø´ØªÙ‡ (ØªØµØ§ÙˆÛŒØ± Ù…Ø§Ù‡ÙˆØ§Ø±Ù‡â€ŒØ§ÛŒ Ø¨Ø¯ÙˆÙ† Ø§Ø¨Ø±).")
                 else:
                     st.info(f"Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ù†Ù…ÙˆØ¯Ø§Ø± Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ Ø´Ø§Ø®Øµ {selected_index} Ø¯Ø± Ø¨Ø§Ø²Ù‡ Ù…Ø´Ø®Øµ Ø´Ø¯Ù‡ ÛŒØ§ÙØª Ù†Ø´Ø¯ (Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ù¾ÙˆØ´Ø´ Ø§Ø¨Ø±ÛŒ).")
             else:
                 st.warning("Ù†ÙˆØ¹ Ù‡Ù†Ø¯Ø³Ù‡ Ù…Ø²Ø±Ø¹Ù‡ Ø¨Ø±Ø§ÛŒ Ù†Ù…ÙˆØ¯Ø§Ø± Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ù†Ù…ÛŒâ€ŒØ´ÙˆØ¯ (ÙÙ‚Ø· Ù…Ø®ØªØµØ§Øª Ù†Ù‚Ø·Ù‡â€ŒØ§ÛŒ).")
        else:
            st.info("Ù„Ø·ÙØ§Ù‹ ÛŒÚ© Ù…Ø²Ø±Ø¹Ù‡ Ù…Ø¹ØªØ¨Ø± Ø¨Ø§ Ù…Ø®ØªØµØ§Øª Ù†Ù‚Ø·Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ù…Ø´Ø§Ù‡Ø¯Ù‡ Ù†Ù…ÙˆØ¯Ø§Ø± Ø³Ø±ÛŒ Ø²Ù…Ø§Ù†ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯.")


        # ==============================================================================
        # Ranking Table
        # ==============================================================================
        st.markdown("---")
        st.subheader(f"ğŸ“Š Ø¬Ø¯ÙˆÙ„ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ù…Ø²Ø§Ø±Ø¹ Ø¨Ø± Ø§Ø³Ø§Ø³ {selected_index} (Ø±ÙˆØ²: {selected_day})")
        st.markdown("Ù…Ù‚Ø§ÛŒØ³Ù‡ Ù…Ù‚Ø§Ø¯ÛŒØ± Ù…ØªÙˆØ³Ø· Ø´Ø§Ø®Øµ Ø¯Ø± Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ Ø¨Ø§ Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„.")

        @st.cache_data(show_spinner=f"Ø¯Ø± Ø­Ø§Ù„ Ù…Ø­Ø§Ø³Ø¨Ù‡ {selected_index} Ø¨Ø±Ø§ÛŒ Ù…Ø²Ø§Ø±Ø¹ Ø¬Ù‡Øª Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ...", persist="ì•±") # Increased cache persistence
        def calculate_weekly_indices(
            _farms_df, index_name, start_curr, end_curr, start_prev, end_prev
        ):
            """Calculates the average index value for the current and previous week for a list of farms."""
            results = []
            errors = []
            total_farms = len(_farms_df)
            # Use a placeholder to update the progress bar within the loop
            progress_placeholder = st.empty()


            for i, (idx, farm) in enumerate(_farms_df.iterrows()):
                farm_name = farm['Ù…Ø²Ø±Ø¹Ù‡']
                lat = farm['centroid_lat']
                lon = farm['centroid_lon']

                if pd.isna(lat) or pd.isna(lon):
                    errors.append(f"Ù…Ø®ØªØµØ§Øª Ù†Ø§Ù…Ø¹ØªØ¨Ø± Ø¨Ø±Ø§ÛŒ Ù…Ø²Ø±Ø¹Ù‡ '{farm_name}'. Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´Ø¯.")
                    continue # Skip this farm

                point_geom = ee.Geometry.Point([lon, lat])

                # Use get_farm_needs_data which already fetches for two periods
                # Although it fetches more indices, caching helps.
                # A dedicated function might be slightly more efficient if only the selected index is needed.
                # Let's stick to get_farm_needs_data for simplicity and cache leverage.
                # Note: get_farm_needs_data gets NDVI, NDMI, EVI, SAVI. We only need the selected_index.
                # We can call get_mean_values_for_period twice here for just the selected index.

                def get_mean_value_single_index(start, end, index):
                     try:
                          image, error = get_processed_image(point_geom, start, end, index)
                          if image:
                              # Reduce region to get the mean value at the point
                              mean_dict = image.reduceRegion(
                                  reducer=ee.Reducer.mean(),
                                  geometry=point_geom,
                                  scale=10,  # Scale in meters
                                  bestEffort=True # Use bestEffort
                              ).getInfo()
                              # Return the value for the specific index, handle potential None or key error
                              return mean_dict.get(index) if mean_dict and index in mean_dict else None, None
                          else:
                              return None, error # Return the error from get_processed_image
                     except ee.EEException as e:
                          return None, f"GEE Error for {farm_name} ({start}-{end}): {e}"
                     except Exception as e:
                          return None, f"Unknown Error for {farm_name} ({start}-{end}): {e}"


                # Calculate for current week (only the selected index)
                current_val, err_curr = get_mean_value_single_index(start_curr, end_curr, index_name)
                if err_curr: errors.append(f"Ù…Ø²Ø±Ø¹Ù‡ '{farm_name}' (Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ): {err_curr}")

                # Calculate for previous week (only the selected index)
                previous_val, err_prev = get_mean_value_single_index(start_prev, end_prev, index_name)
                if err_prev: errors.append(f"Ù…Ø²Ø±Ø¹Ù‡ '{farm_name}' (Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„): {err_prev}")

                # Calculate change
                change = None
                # Use pd.notna for robust check of numeric values
                if pd.notna(current_val) and pd.notna(previous_val):
                    try:
                        change = current_val - previous_val
                    except TypeError: # Handle cases where values might not be numeric unexpectedly
                        change = None


                results.append({
                    'Ù…Ø²Ø±Ø¹Ù‡': farm_name,
                    'Ú¯Ø±ÙˆÙ‡': farm.get('Ú¯Ø±ÙˆÙ‡', 'N/A'), # Use .get for safety
                    f'{index_name} (Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ)': current_val,
                    f'{index_name} (Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„)': previous_val,
                    'ØªØºÛŒÛŒØ±': change,
                    'Ø³Ù†': farm.get('Ø³Ù†', 'N/A'), # Add Ø³Ù† to results
                    'ÙˆØ§Ø±ÛŒØªÙ‡': farm.get('ÙˆØ§Ø±ÛŒØªÙ‡', 'N/A'), # Add ÙˆØ§Ø±ÛŒØªÙ‡ to results
                })

                # Update progress bar
                progress = (i + 1) / total_farms
                progress_placeholder.markdown(modern_progress_bar(progress), unsafe_allow_html=True)

            progress_placeholder.empty() # Remove progress bar after completion
            return pd.DataFrame(results), errors

        # Calculate and display the ranking table
        # Only run calculation if date ranges are valid
        ranking_df = pd.DataFrame()
        calculation_errors = []
        if start_date_current_str and end_date_current_str and start_date_previous_str and end_date_previous_str:
             ranking_df, calculation_errors = calculate_weekly_indices(
                 filtered_farms_df,
                 selected_index,
                 start_date_current_str,
                 end_date_current_str,
                 start_date_previous_str,
                 end_date_previous_str
             )
        else:
             st.warning("âš ï¸ Ø¨Ø§Ø²Ù‡â€ŒÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ Ù…Ø¹ØªØ¨Ø± Ø¨Ø±Ø§ÛŒ Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª.")


        # Display any errors that occurred during calculation
        if calculation_errors:
            st.warning("âš ï¸ Ø¨Ø±Ø®ÛŒ Ø®Ø·Ø§Ù‡Ø§ Ø¯Ø± Ø­ÛŒÙ† Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø±Ø® Ø¯Ø§Ø¯:")
            for error in calculation_errors[:10]: # Show first 10 errors
                st.warning(f"- {error}")
            if len(calculation_errors) > 10:
                st.warning(f"... Ùˆ {len(calculation_errors) - 10} Ø®Ø·Ø§ÛŒ Ø¯ÛŒÚ¯Ø±.")


        if not ranking_df.empty:
            # Sort by the current week's index value
            # Sorting order depends on whether higher index means better (NDVI, EVI, LAI, CVI, NDMI) or worse (MSI)
            ascending_sort = selected_index == 'MSI' # True if MSI (lower is better), False otherwise

            # Add a temporary column for sorting to handle potential non-numeric values robustly
            # Fill non-numeric with a value that places them last (e.g., large number for ascending, small for descending)
            sort_col_name = f'{selected_index} (Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ)'
            temp_sort_col = f'{sort_col_name}_sortable'

            if ascending_sort: # MSI: lower is better -> ascending
                 ranking_df[temp_sort_col] = pd.to_numeric(ranking_df[sort_col_name], errors='coerce').fillna(1e9) # Large number places NaNs last
            else: # Others: higher is better -> descending
                 ranking_df[temp_sort_col] = pd.to_numeric(ranking_df[sort_col_name], errors='coerce').fillna(-1e9) # Small number places NaNs last


            ranking_df_sorted = ranking_df.sort_values(
                by=temp_sort_col,
                ascending=ascending_sort,
            ).drop(columns=[temp_sort_col]).reset_index(drop=True)


            # Add rank number
            ranking_df_sorted.index = ranking_df_sorted.index + 1
            ranking_df_sorted.index.name = 'Ø±ØªØ¨Ù‡'

            # Apply the determine_status function using .apply
            ranking_df_sorted['ÙˆØ¶Ø¹ÛŒØª'] = ranking_df_sorted.apply(
                lambda row: determine_status(row, selected_index), axis=1
            )

            # Apply status badge HTML
            ranking_df_sorted['ÙˆØ¶Ø¹ÛŒØª_Ù†Ù…Ø§ÛŒØ´'] = ranking_df_sorted['ÙˆØ¶Ø¹ÛŒØª'].apply(lambda s: status_badge(s))

            # Format numbers for better readability
            cols_to_format = [f'{selected_index} (Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ)', f'{selected_index} (Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„)', 'ØªØºÛŒÛŒØ±']
            for col in cols_to_format:
                if col in ranking_df_sorted.columns:
                     # Check if column exists before formatting and use pd.notna
                     ranking_df_sorted[col] = ranking_df_sorted[col].map(lambda x: f"{x:.3f}" if pd.notna(x) else "N/A")

            # Select columns to display, including 'Ú¯Ø±ÙˆÙ‡', 'Ø³Ù†', 'ÙˆØ§Ø±ÛŒØªÙ‡'
            display_columns = ['Ù…Ø²Ø±Ø¹Ù‡', 'Ú¯Ø±ÙˆÙ‡', 'Ø³Ù†', 'ÙˆØ§Ø±ÛŒØªÙ‡'] + cols_to_format + ['ÙˆØ¶Ø¹ÛŒØª_Ù†Ù…Ø§ÛŒØ´']
            # Ensure only existing columns are selected and in the correct order
            # Prioritize the order specified in display_columns
            final_display_columns = [col for col in display_columns if col in ranking_df_sorted.columns]

            # Rename the 'ÙˆØ¶Ø¹ÛŒØª_Ù†Ù…Ø§ÛŒØ´' column header for display
            ranking_df_display = ranking_df_sorted[final_display_columns].rename(columns={'ÙˆØ¶Ø¹ÛŒØª_Ù†Ù…Ø§ÛŒØ´': 'ÙˆØ¶Ø¹ÛŒØª'})


            # Display the table with color coding and selected columns
            st.write("<style>td {vertical-align: middle !important;}</style>", unsafe_allow_html=True)
            # Use to_html with escape=False because 'ÙˆØ¶Ø¹ÛŒØª_Ù†Ù…Ø§ÛŒØ´' already contains HTML
            st.write(ranking_df_display.to_html(escape=False, index=True), unsafe_allow_html=True)

            # Add a summary of farm statuses below the table
            st.subheader("ğŸ“Š Ø®Ù„Ø§ØµÙ‡ ÙˆØ¶Ø¹ÛŒØª Ù…Ø²Ø§Ø±Ø¹ (Ø¨Ø± Ø§Ø³Ø§Ø³ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ)")

            # Calculate status counts from the raw 'ÙˆØ¶Ø¹ÛŒØª' column (not the HTML one)
            status_counts = ranking_df_sorted['ÙˆØ¶Ø¹ÛŒØª'].value_counts()

            # Dynamically find positive and negative status terms used
            positive_terms = [s for s in status_counts.index if "Ø¨Ù‡Ø¨ÙˆØ¯" in s or "Ø±Ø´Ø¯ Ù…Ø«Ø¨Øª" in s]
            negative_terms = [s for s in status_counts.index if any(sub in s for sub in ["ØªÙ†Ø´", "Ú©Ø§Ù‡Ø´", "Ø¨Ø¯ØªØ±"])]
            neutral_term = "Ø«Ø§Ø¨Øª"
            nodata_term = "Ø¨Ø¯ÙˆÙ† Ø¯Ø§Ø¯Ù‡"

            col1, col2, col3, col4 = st.columns(4) # Added column for No Data

            with col1:
                pos_count = sum(status_counts.get(term, 0) for term in positive_terms)
                pos_label = "ğŸŸ¢ Ø¨Ù‡Ø¨ÙˆØ¯" if positive_terms else "ğŸŸ¢ Ø¨Ù‡Ø¨ÙˆØ¯" # Use generic label if specific not found
                st.metric(pos_label, pos_count)

            with col2:
                neutral_count = status_counts.get(neutral_term, 0)
                st.metric(f"âšª {neutral_term}", neutral_count)

            with col3:
                neg_count = sum(status_counts.get(term, 0) for term in negative_terms)
                neg_label = "ğŸ”´ ØªÙ†Ø´" if negative_terms else "ğŸ”´ ØªÙ†Ø´" # Use generic label if specific not found
                st.metric(neg_label, neg_count)

            with col4:
                 nodata_count = status_counts.get(nodata_term, 0)
                 st.metric(f"ğŸŸ¡ {nodata_term}", nodata_count) # Use yellow for no data

            # Add explanation for statuses
            st.info(f"""
            **ØªÙˆØ¶ÛŒØ­Ø§Øª ÙˆØ¶Ø¹ÛŒØª:**
            - **ğŸŸ¢ Ø¨Ù‡Ø¨ÙˆØ¯/Ø±Ø´Ø¯ Ù…Ø«Ø¨Øª**: Ù…Ø²Ø§Ø±Ø¹ÛŒ Ú©Ù‡ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„ Ø¨Ù‡Ø¨ÙˆØ¯ Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡ÛŒ Ø¯Ø§Ø´ØªÙ‡â€ŒØ§Ù†Ø¯ (Ø§ÙØ²Ø§ÛŒØ´ Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ÛŒÛŒ Ù…Ø§Ù†Ù†Ø¯ NDVI ÛŒØ§ Ú©Ø§Ù‡Ø´ Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ÛŒÛŒ Ù…Ø§Ù†Ù†Ø¯ MSI).
            - **âšª Ø«Ø§Ø¨Øª**: Ù…Ø²Ø§Ø±Ø¹ÛŒ Ú©Ù‡ ØªØºÛŒÛŒØ± Ù…Ø¹Ù†Ø§Ø¯Ø§Ø±ÛŒ Ø¯Ø± Ø´Ø§Ø®Øµ Ù†Ø¯Ø§Ø´ØªÙ‡â€ŒØ§Ù†Ø¯ (Ø¯Ø±ÙˆÙ† Ø¢Ø³ØªØ§Ù†Ù‡ ØªØºÛŒÛŒØ±).
            - **ğŸ”´ ØªÙ†Ø´/Ú©Ø§Ù‡Ø´/Ø¨Ø¯ØªØ± Ø´Ø¯Ù†**: Ù…Ø²Ø§Ø±Ø¹ÛŒ Ú©Ù‡ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„ ÙˆØ¶Ø¹ÛŒØª Ù†Ø§Ù…Ø·Ù„ÙˆØ¨â€ŒØªØ±ÛŒ Ø¯Ø§Ø´ØªÙ‡â€ŒØ§Ù†Ø¯ (Ú©Ø§Ù‡Ø´ Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ÛŒÛŒ Ù…Ø§Ù†Ù†Ø¯ NDVI ÛŒØ§ Ø§ÙØ²Ø§ÛŒØ´ Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ÛŒÛŒ Ù…Ø§Ù†Ù†Ø¯ MSI).
            - **ğŸŸ¡ Ø¨Ø¯ÙˆÙ† Ø¯Ø§Ø¯Ù‡**: Ù…Ø²Ø§Ø±Ø¹ÛŒ Ú©Ù‡ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ø¹Ø¯Ù… Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ ØªØµØ§ÙˆÛŒØ± Ù…Ø§Ù‡ÙˆØ§Ø±Ù‡â€ŒØ§ÛŒ Ø¨Ø¯ÙˆÙ† Ø§Ø¨Ø± Ø¯Ø± ÛŒÚ© ÛŒØ§ Ù‡Ø± Ø¯Ùˆ Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒØŒ Ø§Ù…Ú©Ø§Ù† Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØºÛŒÛŒØ±Ø§Øª ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø´ØªÙ‡ Ø§Ø³Øª.
            """)

            # Add AI summary for the ranking/map
            st.markdown("---")
            st.subheader("ğŸ¤– Ø®Ù„Ø§ØµÙ‡ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ø§Ø² ÙˆØ¶Ø¹ÛŒØª Ù…Ø²Ø§Ø±Ø¹")
            if gemini_model:
                 # Call the AI map summary function
                 ai_map_summary = get_ai_map_summary(gemini_model, ranking_df_sorted, selected_index, selected_day)
                 st.markdown(ai_map_summary)
            else:
                 st.info("Ø³Ø±ÙˆÛŒØ³ ØªØ­Ù„ÛŒÙ„ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ù†Ø´Ø¯Ù‡ ÛŒØ§ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª.")


            # Add download button for the table
            # Provide the clean DataFrame for download, not the one with HTML badges
            ranking_df_clean = ranking_df_sorted.drop(columns=['ÙˆØ¶Ø¹ÛŒØª_Ù†Ù…Ø§ÛŒØ´'])
            # Ensure original 'ÙˆØ¶Ø¹ÛŒØª' column is included in clean data
            ranking_df_clean['ÙˆØ¶Ø¹ÛŒØª'] = ranking_df_sorted['ÙˆØ¶Ø¹ÛŒØª']

            csv_data = ranking_df_clean.to_csv(index=True).encode('utf-8')
            st.download_button(
                label="ğŸ“¥ Ø¯Ø§Ù†Ù„ÙˆØ¯ Ø¬Ø¯ÙˆÙ„ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ (CSV)",
                data=csv_data,
                file_name=f'ranking_{selected_index}_{selected_day}_{end_date_current_str}.csv',
                mime='text/csv',
            )
        else:
            st.info(f"Ø¯Ø§Ø¯Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ø¬Ø¯ÙˆÙ„ Ø±ØªØ¨Ù‡â€ŒØ¨Ù†Ø¯ÛŒ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø´Ø§Ø®Øµ {selected_index} Ø¯Ø± Ø§ÛŒÙ† Ø¨Ø§Ø²Ù‡ Ø²Ù…Ø§Ù†ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯ (Ø§Ø­ØªÙ…Ø§Ù„Ø§Ù‹ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ø¹Ø¯Ù… Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ ØªØµØ§ÙˆÛŒØ± Ù…Ø§Ù‡ÙˆØ§Ø±Ù‡â€ŒØ§ÛŒ Ø¨Ø¯ÙˆÙ† Ø§Ø¨Ø±).")


    st.markdown("---")
    st.sidebar.markdown("---")
    st.sidebar.markdown("Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Streamlit, Google Earth Engine, Ùˆ geemap")


# --- Tab 2: Analysis Data Visualization ---
with tab2:
    st.header("ğŸ“ˆ ØªØ­Ù„ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ÙØ§ÛŒÙ„ Ù…Ø­Ø§Ø³Ø¨Ø§Øª")
    st.markdown("Ù†Ù…Ø§ÛŒØ´ Ú¯Ø±Ø§ÙÛŒÚ©ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ù…Ø³Ø§Ø­Øª Ùˆ ØªÙˆÙ„ÛŒØ¯ Ø¨Ù‡ ØªÙÚ©ÛŒÚ© Ø§Ø¯Ø§Ø±Ù‡ Ùˆ Ø³Ù† Ø§Ø² ÙØ§ÛŒÙ„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯Ù‡.")

    if analysis_area_df is not None or analysis_prod_df is not None:

        # Get unique 'Ø§Ø¯Ø§Ø±Ù‡' values from both dataframes if they exist
        available_edareh = []
        if analysis_area_df is not None and 'Ø§Ø¯Ø§Ø±Ù‡' in analysis_area_df.index.names:
            available_edareh.extend(analysis_area_df.index.get_level_values('Ø§Ø¯Ø§Ø±Ù‡').unique().tolist())
        if analysis_prod_df is not None and 'Ø§Ø¯Ø§Ø±Ù‡' in analysis_prod_df.index.names:
            available_edareh.extend(analysis_prod_df.index.get_level_values('Ø§Ø¯Ø§Ø±Ù‡').unique().tolist())

        # Ensure unique and sorted list
        available_edareh = sorted(list(set(available_edareh)))

        if not available_edareh:
            st.warning("âš ï¸ Ù‡ÛŒÚ† Ø§Ø¯Ø§Ø±Ù‡â€ŒØ§ÛŒ Ø¨Ø±Ø§ÛŒ Ù†Ù…Ø§ÛŒØ´ Ø¯Ø± Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯. Ù„Ø·ÙØ§Ù‹ Ø³Ø§Ø®ØªØ§Ø± ÙØ§ÛŒÙ„ 'Ù…Ø­Ø§Ø³Ø¨Ø§Øª 2.csv' Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.")
        else:
            selected_edareh = st.selectbox(
                "Ø§Ø¯Ø§Ø±Ù‡ Ù…ÙˆØ±Ø¯ Ù†Ø¸Ø± Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯:",
                options=available_edareh,
                key='analysis_edareh_select' # Unique key for this widget
            )

            # --- Display Data for Selected Edareh ---
            st.subheader(f"Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø§Ø¯Ø§Ø±Ù‡: {selected_edareh}")

            col1, col2 = st.columns(2)

            # --- Area Data Visualization ---
            with col1:
                st.markdown("#### Ù…Ø³Ø§Ø­Øª (Ù‡Ú©ØªØ§Ø±)")
                # Check if analysis_area_df exists and the selected_edareh is in its index
                if analysis_area_df is not None and selected_edareh in analysis_area_df.index.get_level_values('Ø§Ø¯Ø§Ø±Ù‡').unique():
                    try:
                        df_area_selected = analysis_area_df.loc[selected_edareh].copy()

                        # Prepare data for 3D surface plot
                        # X = Ø³Ù†, Y = ÙˆØ§Ø±ÛŒØªÙ‡ (Ø³ØªÙˆÙ† Ù‡Ø§), Z = Ù…Ù‚Ø¯Ø§Ø±
                        # Ensure index 'Ø³Ù†' and columns (varieties) are suitable for plotting
                        ages = df_area_selected.index.tolist()
                        varieties = df_area_selected.columns.tolist()
                        z_data = df_area_selected.values

                        # Check if enough data points for a meaningful surface plot
                        # Requires at least 2 unique ages and 2 unique varieties (columns)
                        if len(ages) > 1 and len(varieties) > 1 and z_data.shape[0] > 1 and z_data.shape[1] > 1:
                             try:
                                 fig_3d_area = go.Figure(data=[go.Surface(z=z_data, x=ages, y=varieties, colorscale='Viridis')])
                                 fig_3d_area.update_layout(
                                     title=f'Ù†Ù…ÙˆØ¯Ø§Ø± Ø³Ø·Ø­ Ù…Ø³Ø§Ø­Øª - Ø§Ø¯Ø§Ø±Ù‡ {selected_edareh}',
                                     scene=dict(
                                         xaxis_title='Ø³Ù†',
                                         yaxis_title='ÙˆØ§Ø±ÛŒØªÙ‡',
                                         zaxis_title='Ù…Ø³Ø§Ø­Øª (Ù‡Ú©ØªØ§Ø±)'),
                                     autosize=True, height=500)
                                 st.plotly_chart(fig_3d_area, use_container_width=True)
                                 st.caption("Ù†Ù…Ø§ÛŒØ´ ØªÙˆØ²ÛŒØ¹ Ù…Ø³Ø§Ø­Øª Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³Ù† Ùˆ ÙˆØ§Ø±ÛŒØªÙ‡ Ø¯Ø± ÛŒÚ© Ø³Ø·Ø­ Ø³Ù‡ Ø¨Ø¹Ø¯ÛŒ.")
                             except Exception as e:
                                 st.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆØ¯Ø§Ø± Surface Plot Ù…Ø³Ø§Ø­Øª: {e}")
                                 st.dataframe(df_area_selected) # Show table as fallback
                        else:
                             st.info("âš ï¸ Ø¯Ø§Ø¯Ù‡ Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø± Surface Plot Ù…Ø³Ø§Ø­Øª ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯ (Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨ÛŒØ´ Ø§Ø² ÛŒÚ© Ù…Ù‚Ø¯Ø§Ø± Ø³Ù† Ùˆ ÛŒÚ© ÙˆØ§Ø±ÛŒØªÙ‡ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡).")
                             st.dataframe(df_area_selected) # Show table if not enough data for 3D


                        # Histogram of Area per Variety (Melted DataFrame)
                        # Ensure the DataFrame is reset_index correctly for melting if multi-indexed
                        if 'Ø³Ù†' in df_area_selected.index.names:
                             df_area_melt = df_area_selected.reset_index().melt(id_vars='Ø³Ù†', var_name='ÙˆØ§Ø±ÛŒØªÙ‡', value_name='Ù…Ø³Ø§Ø­Øª')
                        else:
                             # If index is not 'Ø³Ù†', assume it's a simple index or column that needs to be an ID var
                             # This case might need adjustment based on actual data structure if multi-index fails
                              st.warning("âš ï¸ Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ø¯Ù‡ Ù…Ø³Ø§Ø­Øª Ø¨Ø±Ø§ÛŒ Ù‡ÛŒØ³ØªÙˆÚ¯Ø±Ø§Ù… ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡ Ø§Ø³Øª.")
                              df_area_melt = pd.DataFrame() # Empty DataFrame to prevent error


                        df_area_melt = df_area_melt.dropna(subset=['Ù…Ø³Ø§Ø­Øª', 'ÙˆØ§Ø±ÛŒØªÙ‡', 'Ø³Ù†']) # Drop NA values for plotting
                        # Filter out potential non-numeric 'Ø³Ù†' if needed, though preprocess should handle this
                        df_area_melt = df_area_melt[pd.to_numeric(df_area_melt['Ø³Ù†'], errors='coerce').notna()]


                        if not df_area_melt.empty:
                            try:
                                fig_hist_area = px.histogram(df_area_melt, x='ÙˆØ§Ø±ÛŒØªÙ‡', y='Ù…Ø³Ø§Ø­Øª', color='Ø³Ù†',
                                                           title=f'Ù‡ÛŒØ³ØªÙˆÚ¯Ø±Ø§Ù… Ù…Ø³Ø§Ø­Øª Ø¨Ø± Ø§Ø³Ø§Ø³ ÙˆØ§Ø±ÛŒØªÙ‡ - Ø§Ø¯Ø§Ø±Ù‡ {selected_edareh}',
                                                           labels={'Ù…Ø³Ø§Ø­Øª':'Ù…Ø¬Ù…ÙˆØ¹ Ù…Ø³Ø§Ø­Øª (Ù‡Ú©ØªØ§Ø±)'})
                                st.plotly_chart(fig_hist_area, use_container_width=True)
                                st.caption("ØªÙˆØ²ÛŒØ¹ Ù…Ø³Ø§Ø­Øª Ù‡Ø± ÙˆØ§Ø±ÛŒØªÙ‡ Ø¨Ù‡ ØªÙÚ©ÛŒÚ© Ø³Ù†.")
                            except Exception as e:
                                 st.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆØ¯Ø§Ø± Ù‡ÛŒØ³ØªÙˆÚ¯Ø±Ø§Ù… Ù…Ø³Ø§Ø­Øª: {e}")
                                 st.dataframe(df_area_melt) # Show data as fallback
                        else:
                            st.info(f"âš ï¸ Ø¯Ø§Ø¯Ù‡ Ù…Ø¹ØªØ¨Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡ÛŒØ³ØªÙˆÚ¯Ø±Ø§Ù… Ù…Ø³Ø§Ø­Øª Ø¯Ø± Ø§Ø¯Ø§Ø±Ù‡ {selected_edareh} ÛŒØ§ÙØª Ù†Ø´Ø¯.")

                    except KeyError:
                        st.error(f"âŒ Ø®Ø·Ø§ÛŒ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ø¯Ø§Ø¯Ù‡ Ø§Ø¯Ø§Ø±Ù‡ '{selected_edareh}' Ø¯Ø± Ø¯Ø§Ø¯Ù‡ Ù…Ø³Ø§Ø­Øª.")
                    except Exception as e:
                         st.error(f"âŒ Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡ Ù…Ø³Ø§Ø­Øª Ø¨Ø±Ø§ÛŒ Ø§Ø¯Ø§Ø±Ù‡ '{selected_edareh}': {e}")
                         st.error(traceback.format_exc())

                else:
                    st.info(f"âš ï¸ Ø¯Ø§Ø¯Ù‡ Ù…Ø³Ø§Ø­Øª Ø¨Ø±Ø§ÛŒ Ø§Ø¯Ø§Ø±Ù‡ {selected_edareh} ÛŒØ§ÙØª Ù†Ø´Ø¯ ÛŒØ§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª.")

            # --- Production Data Visualization ---
            with col2:
                st.markdown("#### ØªÙˆÙ„ÛŒØ¯ (ØªÙ†)")
                # Check if analysis_prod_df exists and the selected_edareh is in its index
                if analysis_prod_df is not None and selected_edareh in analysis_prod_df.index.get_level_values('Ø§Ø¯Ø§Ø±Ù‡').unique():
                    try:
                        df_prod_selected = analysis_prod_df.loc[selected_edareh].copy()

                        # Prepare data for 3D surface plot (similar logic to area)
                        ages_prod = df_prod_selected.index.tolist()
                        varieties_prod = df_prod_selected.columns.tolist()
                        z_data_prod = df_prod_selected.values

                        if len(ages_prod) > 1 and len(varieties_prod) > 1 and z_data_prod.shape[0] > 1 and z_data_prod.shape[1] > 1:
                             try:
                                 fig_3d_prod = go.Figure(data=[go.Surface(z=z_data_prod, x=ages_prod, y=varieties_prod, colorscale='Plasma')])
                                 fig_3d_prod.update_layout(
                                     title=f'Ù†Ù…ÙˆØ¯Ø§Ø± Ø³Ø·Ø­ ØªÙˆÙ„ÛŒØ¯ - Ø§Ø¯Ø§Ø±Ù‡ {selected_edareh}',
                                     scene=dict(
                                         xaxis_title='Ø³Ù†',
                                         yaxis_title='ÙˆØ§Ø±ÛŒØªÙ‡',
                                         zaxis_title='ØªÙˆÙ„ÛŒØ¯ (ØªÙ†)'),
                                     autosize=True, height=500)
                                 st.plotly_chart(fig_3d_prod, use_container_width=True)
                                 st.caption("Ù†Ù…Ø§ÛŒØ´ ØªÙˆØ²ÛŒØ¹ ØªÙˆÙ„ÛŒØ¯ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø³Ù† Ùˆ ÙˆØ§Ø±ÛŒØªÙ‡ Ø¯Ø± ÛŒÚ© Ø³Ø·Ø­ Ø³Ù‡ Ø¨Ø¹Ø¯ÛŒ.")
                             except Exception as e:
                                  st.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆØ¯Ø§Ø± Surface Plot ØªÙˆÙ„ÛŒØ¯: {e}")
                                  st.dataframe(df_prod_selected) # Show table as fallback
                        else:
                             st.info("âš ï¸ Ø¯Ø§Ø¯Ù‡ Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ø±Ø³Ù… Ù†Ù…ÙˆØ¯Ø§Ø± Surface Plot ØªÙˆÙ„ÛŒØ¯ ÙˆØ¬ÙˆØ¯ Ù†Ø¯Ø§Ø±Ø¯ (Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨ÛŒØ´ Ø§Ø² ÛŒÚ© Ù…Ù‚Ø¯Ø§Ø± Ø³Ù† Ùˆ ÛŒÚ© ÙˆØ§Ø±ÛŒØªÙ‡ Ø¨Ø§ Ø¯Ø§Ø¯Ù‡).")
                             st.dataframe(df_prod_selected) # Show table if not enough data for 3D


                        # Histogram of Production per Variety (Melted DataFrame)
                        if 'Ø³Ù†' in df_prod_selected.index.names:
                            df_prod_melt = df_prod_selected.reset_index().melt(id_vars='Ø³Ù†', var_name='ÙˆØ§Ø±ÛŒØªÙ‡', value_name='ØªÙˆÙ„ÛŒØ¯')
                        else:
                             st.warning("âš ï¸ Ø³Ø§Ø®ØªØ§Ø± Ø¯Ø§Ø¯Ù‡ ØªÙˆÙ„ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ù‡ÛŒØ³ØªÙˆÚ¯Ø±Ø§Ù… ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡ Ø§Ø³Øª.")
                             df_prod_melt = pd.DataFrame() # Empty DataFrame

                        df_prod_melt = df_prod_melt.dropna(subset=['ØªÙˆÙ„ÛŒØ¯', 'ÙˆØ§Ø±ÛŒØªÙ‡', 'Ø³Ù†']) # Drop NA values for plotting
                        # Filter out potential non-numeric 'Ø³Ù†'
                        df_prod_melt = df_prod_melt[pd.to_numeric(df_prod_melt['Ø³Ù†'], errors='coerce').notna()]


                        if not df_prod_melt.empty:
                            try:
                                fig_hist_prod = px.histogram(df_prod_melt, x='ÙˆØ§Ø±ÛŒØªÙ‡', y='ØªÙˆÙ„ÛŒØ¯', color='Ø³Ù†',
                                                           title=f'Ù‡ÛŒØ³ØªÙˆÚ¯Ø±Ø§Ù… ØªÙˆÙ„ÛŒØ¯ Ø¨Ø± Ø§Ø³Ø§Ø³ ÙˆØ§Ø±ÛŒØªÙ‡ - Ø§Ø¯Ø§Ø±Ù‡ {selected_edareh}',
                                                           labels={'ØªÙˆÙ„ÛŒØ¯':'Ù…Ø¬Ù…ÙˆØ¹ ØªÙˆÙ„ÛŒØ¯ (ØªÙ†)'})
                                st.plotly_chart(fig_hist_prod, use_container_width=True)
                                st.caption("ØªÙˆØ²ÛŒØ¹ ØªÙˆÙ„ÛŒØ¯ Ù‡Ø± ÙˆØ§Ø±ÛŒØªÙ‡ Ø¨Ù‡ ØªÙÚ©ÛŒÚ© Ø³Ù†.")
                            except Exception as e:
                                 st.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø§ÛŒØ¬Ø§Ø¯ Ù†Ù…ÙˆØ¯Ø§Ø± Ù‡ÛŒØ³ØªÙˆÚ¯Ø±Ø§Ù… ØªÙˆÙ„ÛŒØ¯: {e}")
                                 st.dataframe(df_prod_melt) # Show data as fallback
                        else:
                             st.info(f"âš ï¸ Ø¯Ø§Ø¯Ù‡ Ù…Ø¹ØªØ¨Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ù‡ÛŒØ³ØªÙˆÚ¯Ø±Ø§Ù… ØªÙˆÙ„ÛŒØ¯ Ø¯Ø± Ø§Ø¯Ø§Ø±Ù‡ {selected_edareh} ÛŒØ§ÙØª Ù†Ø´Ø¯.")

                    except KeyError:
                         st.error(f"âŒ Ø®Ø·Ø§ÛŒ Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ø¯Ø§Ø¯Ù‡ Ø§Ø¯Ø§Ø±Ù‡ '{selected_edareh}' Ø¯Ø± Ø¯Ø§Ø¯Ù‡ ØªÙˆÙ„ÛŒØ¯.")
                    except Exception as e:
                         st.error(f"âŒ Ø®Ø·Ø§ÛŒ ØºÛŒØ±Ù…Ù†ØªØ¸Ø±Ù‡ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡ ØªÙˆÙ„ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ø§Ø¯Ø§Ø±Ù‡ '{selected_edareh}': {e}")
                         st.error(traceback.format_exc())
                else:
                    st.info(f"âš ï¸ Ø¯Ø§Ø¯Ù‡ ØªÙˆÙ„ÛŒØ¯ Ø¨Ø±Ø§ÛŒ Ø§Ø¯Ø§Ø±Ù‡ {selected_edareh} ÛŒØ§ÙØª Ù†Ø´Ø¯ ÛŒØ§ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†Ø´Ø¯Ù‡ Ø§Ø³Øª.")

    else:
        st.error("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÛŒØ§ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø§Ø² ÙØ§ÛŒÙ„ 'Ù…Ø­Ø§Ø³Ø¨Ø§Øª 2.csv'. Ù„Ø·ÙØ§Ù‹ ÙØ§ÛŒÙ„ Ø±Ø§ Ø¨Ø±Ø±Ø³ÛŒ Ú©Ù†ÛŒØ¯.")


    st.markdown("---")
    st.sidebar.markdown("---")
    st.sidebar.markdown("Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Streamlit, Google Earth Engine, Ùˆ geemap")


# --- Tab 3: Needs Analysis ---
with tab3:
    st.header("ğŸ’§ ØªØ­Ù„ÛŒÙ„ Ù†ÛŒØ§Ø² Ø¢Ø¨ÛŒØ§Ø±ÛŒ Ùˆ Ú©ÙˆØ¯Ø¯Ù‡ÛŒ")

    if selected_farm_name == "Ù‡Ù…Ù‡ Ù…Ø²Ø§Ø±Ø¹":
        st.info("âš ï¸ Ù„Ø·ÙØ§Ù‹ ÛŒÚ© Ù…Ø²Ø±Ø¹Ù‡ Ø®Ø§Øµ Ø±Ø§ Ø§Ø² Ù¾Ù†Ù„ Ú©Ù†Ø§Ø±ÛŒ (Ø³Ù…Øª Ú†Ù¾) Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯ ØªØ§ ØªØ­Ù„ÛŒÙ„ Ù†ÛŒØ§Ø²Ù‡Ø§ÛŒ Ø¢Ù† Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆØ¯.")
    elif selected_farm_geom and selected_farm_details is not None:
        # Check if it's a point geometry by checking the stored property
        is_point = selected_farm_details.get('geometry_type') == 'Point'

        if not is_point:
            st.warning("âš ï¸ ØªØ­Ù„ÛŒÙ„ Ù†ÛŒØ§Ø²Ù‡Ø§ ÙÙ‚Ø· Ø¨Ø±Ø§ÛŒ Ù…Ø²Ø§Ø±Ø¹ Ø¨Ø§ Ù…Ø®ØªØµØ§Øª Ù†Ù‚Ø·Ù‡â€ŒØ§ÛŒ (ØªÚ© Ù…Ø²Ø±Ø¹Ù‡) Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ø§Ø³Øª.")
        elif not start_date_current_str or not end_date_current_str or not start_date_previous_str or not end_date_previous_str:
             st.warning("âš ï¸ Ø¨Ø§Ø²Ù‡â€ŒÙ‡Ø§ÛŒ Ø²Ù…Ø§Ù†ÛŒ Ù…Ø¹ØªØ¨Ø± Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ù†ÛŒØ§Ø²Ù‡Ø§ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª. Ù„Ø·ÙØ§Ù‹ Ø±ÙˆØ² Ù‡ÙØªÙ‡ Ø±Ø§ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯.")
        else:
            st.subheader(f"ØªØ­Ù„ÛŒÙ„ Ø¨Ø±Ø§ÛŒ Ù…Ø²Ø±Ø¹Ù‡: {selected_farm_name}")

            # Define thresholds (allow user adjustment)
            st.markdown("---")
            st.markdown("#### ØªÙ†Ø¸ÛŒÙ… Ø¢Ø³ØªØ§Ù†Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„")
            st.write("Ø§ÛŒÙ† Ø¢Ø³ØªØ§Ù†Ù‡â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ø§Ø±Ø§Ø¦Ù‡ ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ù‚ÙˆØ§Ù†ÛŒÙ† Ø³Ø§Ø¯Ù‡ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒâ€ŒØ´ÙˆÙ†Ø¯.")

            # Use Streamlit number_input for potentially more precise control than slider
            ndmi_threshold = st.number_input(
                 "Ø¢Ø³ØªØ§Ù†Ù‡ Ù¾Ø§ÛŒÛŒÙ† NDMI Ø¨Ø±Ø§ÛŒ Ù‡Ø´Ø¯Ø§Ø± Ù†ÛŒØ§Ø² Ø¢Ø¨ÛŒØ§Ø±ÛŒ:",
                 min_value=-1.0, max_value=1.0, value=0.25, step=0.01,
                 format="%.2f",
                 help="Ø§Ú¯Ø± Ù…Ù‚Ø¯Ø§Ø± NDMI Ø¯Ø± Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ Ú©Ù…ØªØ± ÛŒØ§ Ù…Ø³Ø§ÙˆÛŒ Ø§ÛŒÙ† Ù…Ù‚Ø¯Ø§Ø± Ø¨Ø§Ø´Ø¯ØŒ Ù‡Ø´Ø¯Ø§Ø± Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¢Ø¨ÛŒØ§Ø±ÛŒ ØµØ§Ø¯Ø± Ù…ÛŒâ€ŒØ´ÙˆØ¯. (Ù…Ø­Ø¯ÙˆØ¯Ù‡: [-1, 1])"
             )

            ndvi_drop_threshold_percent = st.number_input(
                 "Ø¢Ø³ØªØ§Ù†Ù‡ Ø§ÙØª NDVI Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ù†ÛŒØ§Ø² Ú©ÙˆØ¯Ø¯Ù‡ÛŒ (%):",
                 min_value=0.0, max_value=100.0, value=5.0, step=0.5,
                 format="%.1f",
                 help="Ø§Ú¯Ø± Ù…Ù‚Ø¯Ø§Ø± NDVI Ø¯Ø± Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„ Ø¨ÛŒØ´ Ø§Ø² Ø§ÛŒÙ† Ø¯Ø±ØµØ¯ Ú©Ø§Ù‡Ø´ ÛŒØ§Ø¨Ø¯ØŒ Ù‡Ø´Ø¯Ø§Ø± Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ø±Ø±Ø³ÛŒ Ú©ÙˆØ¯Ø¯Ù‡ÛŒ ØµØ§Ø¯Ø± Ù…ÛŒâ€ŒØ´ÙˆØ¯."
             )


            # Get the required index data for the selected farm
            # Reuse get_farm_needs_data which fetches NDVI, NDMI, EVI, SAVI for two periods
            farm_needs_data = get_farm_needs_data(
                selected_farm_geom,
                start_date_current_str, end_date_current_str,
                start_date_previous_str, end_date_previous_str
            )

            st.markdown("---")
            st.markdown("#### Ù†ØªØ§ÛŒØ¬ Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§")
            if farm_needs_data.get('error'): # Check if error key exists and is not None/empty
                st.error(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø±ÛŒØ§ÙØª Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ø§Ø®Øµ Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ù†ÛŒØ§Ø²Ù‡Ø§: {farm_needs_data['error']}")
            elif pd.isna(farm_needs_data.get('NDMI_curr')) and pd.isna(farm_needs_data.get('NDVI_curr')):
                st.warning("âš ï¸ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø´Ø§Ø®Øµ Ù„Ø§Ø²Ù… (NDMI Ùˆ NDVI) Ø¨Ø±Ø§ÛŒ ØªØ­Ù„ÛŒÙ„ Ø¯Ø± Ø¯ÙˆØ±Ù‡ ÙØ¹Ù„ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯. (Ù…Ù…Ú©Ù† Ø§Ø³Øª Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ù¾ÙˆØ´Ø´ Ø§Ø¨Ø±ÛŒ Ø¨Ø§Ø´Ø¯).")
            else:
                # --- Display Current Indices ---
                st.markdown("**Ù…Ù‚Ø§Ø¯ÛŒØ± Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ (Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ):**")
                idx_cols = st.columns(4)
                with idx_cols[0]:
                    display_val = f"{farm_needs_data['NDVI_curr']:.3f}" if pd.notna(farm_needs_data.get('NDVI_curr')) else "N/A"
                    st.metric("NDVI", display_val)
                with idx_cols[1]:
                    display_val = f"{farm_needs_data['NDMI_curr']:.3f}" if pd.notna(farm_needs_data.get('NDMI_curr')) else "N/A"
                    st.metric("NDMI", display_val)
                with idx_cols[2]:
                    display_val = f"{farm_needs_data.get('EVI_curr', 'N/A'):.3f}" if pd.notna(farm_needs_data.get('EVI_curr')) else "N/A"
                    st.metric("EVI", display_val)
                with idx_cols[3]:
                    display_val = f"{farm_needs_data.get('SAVI_curr', 'N/A'):.3f}" if pd.notna(farm_needs_data.get('SAVI_curr')) else "N/A"
                    st.metric("SAVI", display_val)


                st.markdown("**Ù…Ù‚Ø§Ø¯ÛŒØ± Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ (Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„):**")
                idx_prev_cols = st.columns(4)
                with idx_prev_cols[0]:
                     display_val_prev = f"{farm_needs_data['NDVI_prev']:.3f}" if pd.notna(farm_needs_data.get('NDVI_prev')) else "N/A"
                     st.metric("NDVI (Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„)", display_val_prev)
                with idx_prev_cols[1]:
                     display_val_prev = f"{farm_needs_data['NDMI_prev']:.3f}" if pd.notna(farm_needs_data.get('NDMI_prev')) else "N/A"
                     st.metric("NDMI (Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„)", display_val_prev)
                with idx_prev_cols[2]:
                     display_val_prev = f"{farm_needs_data.get('EVI_prev', 'N/A'):.3f}" if pd.notna(farm_needs_data.get('EVI_prev')) else "N/A"
                     st.metric("EVI (Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„)", display_val_prev)
                with idx_prev_cols[3]:
                     display_val_prev = f"{farm_needs_data.get('SAVI_prev', 'N/A'):.3f}" if pd.notna(farm_needs_data.get('SAVI_prev')) else "N/A"
                     st.metric("SAVI (Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„)", display_val_prev)


                # --- Generate Recommendations (Rule-Based) ---
                st.markdown("---")
                st.markdown("#### ØªÙˆØµÛŒÙ‡â€ŒÙ‡Ø§ÛŒ Ø§ÙˆÙ„ÛŒÙ‡ (Ø¨Ø± Ø§Ø³Ø§Ø³ Ø¢Ø³ØªØ§Ù†Ù‡â€ŒÙ‡Ø§)")
                recommendations = []

                # 1. Irrigation Check (based on current NDMI)
                if pd.notna(farm_needs_data.get('NDMI_curr')) and farm_needs_data['NDMI_curr'] <= ndmi_threshold:
                    recommendations.append(f"ğŸ’§ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¢Ø¨ÛŒØ§Ø±ÛŒ (NDMI = {farm_needs_data['NDMI_curr']:.3f} <= Ø¢Ø³ØªØ§Ù†Ù‡ {ndmi_threshold:.2f})")

                # 2. Fertilization Check (based on NDVI drop)
                # Ensure both current and previous NDVI are available and valid
                if pd.notna(farm_needs_data.get('NDVI_curr')) and pd.notna(farm_needs_data.get('NDVI_prev')):
                     # Avoid division by zero if previous NDVI is zero or close to zero
                     if farm_needs_data['NDVI_prev'] is not None and farm_needs_data['NDVI_prev'] > 0.01: # Check for small positive value
                          ndvi_change = farm_needs_data['NDVI_curr'] - farm_needs_data['NDVI_prev']
                          ndvi_change_percent = (ndvi_change / farm_needs_data['NDVI_prev']) * 100 # Calculate percentage change relative to previous

                          if ndvi_change < 0 and abs(ndvi_change_percent) > ndvi_drop_threshold_percent:
                              recommendations.append(f"âš ï¸ Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ø±Ø±Ø³ÛŒ Ú©ÙˆØ¯Ø¯Ù‡ÛŒ (Ø§ÙØª NDVI: {ndvi_change:.3f}, Ù…Ø¹Ø§Ø¯Ù„ {abs(ndvi_change_percent):.1f}% Ø§ÙØª Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„)")
                          elif ndvi_change > 0:
                             st.caption(f"âœ… NDVI Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„ Ø§ÙØ²Ø§ÛŒØ´ ÛŒØ§ÙØªÙ‡ Ø§Ø³Øª (+{ndvi_change_percent:.1f}%).")
                          else: # Change is 0 or very small
                             st.caption("â„¹ï¸ NDVI Ù†Ø³Ø¨Øª Ø¨Ù‡ Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„ ØªØºÛŒÛŒØ± Ù‚Ø§Ø¨Ù„ ØªÙˆØ¬Ù‡ÛŒ Ù†Ø¯Ø§Ø´ØªÙ‡ Ø§Ø³Øª.")

                     elif farm_needs_data['NDVI_prev'] is not None and farm_needs_data['NDVI_prev'] <= 0.01:
                         # Handle case where previous NDVI is very low (close to zero)
                         st.caption("â„¹ï¸ Ù…Ù‚Ø¯Ø§Ø± NDVI Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„ Ø¨Ø³ÛŒØ§Ø± Ù¾Ø§ÛŒÛŒÙ† Ø§Ø³Øª. Ù…Ø­Ø§Ø³Ø¨Ù‡ Ø¯Ø±ØµØ¯ ØªØºÛŒÛŒØ± Ù…Ù…Ú©Ù† Ù†ÛŒØ³Øª.")

                elif pd.isna(farm_needs_data.get('NDVI_prev')):
                     st.caption("â„¹ï¸ Ø¯Ø§Ø¯Ù‡ NDVI Ù‡ÙØªÙ‡ Ù‚Ø¨Ù„ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø§ÙØª Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª.")
                elif pd.isna(farm_needs_data.get('NDVI_curr')):
                     st.caption("â„¹ï¸ Ø¯Ø§Ø¯Ù‡ NDVI Ù‡ÙØªÙ‡ Ø¬Ø§Ø±ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ø§ÙØª Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª.")


                # 3. Default if no issues detected by rules
                if not recommendations:
                    recommendations.append("âœ… Ø¨Ø± Ø§Ø³Ø§Ø³ Ø´Ø§Ø®Øµâ€ŒÙ‡Ø§ÛŒ ÙØ¹Ù„ÛŒ Ùˆ Ø¢Ø³ØªØ§Ù†Ù‡â€ŒÙ‡Ø§ØŒ ÙˆØ¶Ø¹ÛŒØª Ù…Ø²Ø±Ø¹Ù‡ Ù…Ø·Ù„ÙˆØ¨ Ø¨Ù‡ Ù†Ø¸Ø± Ù…ÛŒâ€ŒØ±Ø³Ø¯.")

                # Display Recommendations
                for rec in recommendations:
                    if "Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¢Ø¨ÛŒØ§Ø±ÛŒ" in rec: st.error(rec)
                    elif "Ù†ÛŒØ§Ø² Ø¨Ù‡ Ø¨Ø±Ø±Ø³ÛŒ Ú©ÙˆØ¯Ø¯Ù‡ÛŒ" in rec: st.warning(rec)
                    else: st.success(rec)

                # --- Get and Display AI Analysis ---
                st.markdown("---")
                st.markdown("#### ØªØ­Ù„ÛŒÙ„ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ")
                if gemini_model:
                    ai_explanation = get_ai_needs_analysis(gemini_model, selected_farm_name, farm_needs_data, recommendations)
                    st.markdown(ai_explanation)
                else:
                    st.info("âš ï¸ Ø³Ø±ÙˆÛŒØ³ ØªØ­Ù„ÛŒÙ„ Ù‡ÙˆØ´ Ù…ØµÙ†ÙˆØ¹ÛŒ Ù¾ÛŒÚ©Ø±Ø¨Ù†Ø¯ÛŒ Ù†Ø´Ø¯Ù‡ ÛŒØ§ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª.")

    else:
         st.info("âš ï¸ Ù„Ø·ÙØ§Ù‹ ÛŒÚ© Ù…Ø²Ø±Ø¹Ù‡ Ù…Ø¹ØªØ¨Ø± Ø¨Ø§ Ù…Ø®ØªØµØ§Øª Ù†Ù‚Ø·Ù‡â€ŒØ§ÛŒ Ùˆ Ø±ÙˆØ² Ù‡ÙØªÙ‡ Ø±Ø§ Ø§Ø² Ù¾Ù†Ù„ Ú©Ù†Ø§Ø±ÛŒ Ø§Ù†ØªØ®Ø§Ø¨ Ú©Ù†ÛŒØ¯ ØªØ§ ØªØ­Ù„ÛŒÙ„ Ù†ÛŒØ§Ø²Ù‡Ø§ÛŒ Ø¢Ù† Ù†Ù…Ø§ÛŒØ´ Ø¯Ø§Ø¯Ù‡ Ø´ÙˆØ¯.")


st.markdown("---")
st.sidebar.markdown("---")
st.sidebar.markdown("Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯Ù‡ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Streamlit, Google Earth Engine, Ùˆ geemap")